{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vsMiQeN08VUE",
    "outputId": "6a3afc4d-8c12-478f-d48a-fab32ee0edcb",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional, NamedTuple\n",
    "\n",
    "import common\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan:\n",
    "\n",
    "1. try to use the same data as before - to bredict buy / sell - so I will have classification problem instead of regression problem. \n",
    "2. try LSTM + CNN\n",
    "3. try to add data and see what happens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V Done\n",
      "AAPL Done\n",
      "GOOGL Done\n",
      "JNJ Done\n",
      "AMZN Done\n",
      "XOM Done\n",
      "JPM Done\n",
      "KO Done\n",
      "SPY Done\n",
      "MSFT Done\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "def get_nearest_stocks(ticker):\n",
    "    with open('most_correlated_stocks.json') as f:\n",
    "        stocks_map = json.load(f)\n",
    "    return stocks_map[ticker].keys()\n",
    "\n",
    "wanted_stocks = ['V','AAPL', 'GOOGL', 'JNJ', 'AMZN', 'XOM', 'JPM', 'KO', 'SPY', 'MSFT']\n",
    "for stock in wanted_stocks:\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    for near_stock in [stock] + list(get_nearest_stocks(stock)):\n",
    "        data = common.StockData(near_stock)\n",
    "\n",
    "        relevant_df = data.training[['datetime', 'close']]\n",
    "        relevant_df = relevant_df.set_index('datetime', drop=True)\n",
    "        relevant_df.columns = [near_stock]\n",
    "        train_data.append(relevant_df)\n",
    "\n",
    "        relevant_df = data.validation[['datetime', 'close']]\n",
    "        relevant_df = relevant_df.set_index('datetime', drop=True)\n",
    "        relevant_df.columns = [near_stock]\n",
    "        val_data.append(relevant_df)\n",
    "        \n",
    "        relevant_df = data.test[['datetime', 'close']]\n",
    "        relevant_df = relevant_df.set_index('datetime', drop=True)\n",
    "        relevant_df.columns = [near_stock]\n",
    "        test_data.append(relevant_df)\n",
    "\n",
    "    train_data = reduce(lambda  left,right: pd.merge(left,right, how='outer', left_index=True, right_index=True), train_data)\n",
    "    val_data = reduce(lambda  left,right: pd.merge(left,right, how='outer', left_index=True, right_index=True), val_data)\n",
    "    test_data = reduce(lambda  left,right: pd.merge(left,right, how='outer', left_index=True, right_index=True), test_data)\n",
    "    train_data.to_csv(f'most_common_stocks/training/{stock}.csv')\n",
    "    val_data.to_csv(f'most_common_stocks/validation/{stock}.csv')   \n",
    "    test_data.to_csv(f'most_common_stocks/test/{stock}.csv')   \n",
    "    print(f\"{stock} Done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-26 16:30:00</td>\n",
       "      <td>125.570</td>\n",
       "      <td>125.690</td>\n",
       "      <td>125.570</td>\n",
       "      <td>125.660</td>\n",
       "      <td>0.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-26 16:31:00</td>\n",
       "      <td>125.670</td>\n",
       "      <td>125.670</td>\n",
       "      <td>125.610</td>\n",
       "      <td>125.630</td>\n",
       "      <td>0.2796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-26 16:32:00</td>\n",
       "      <td>125.630</td>\n",
       "      <td>125.690</td>\n",
       "      <td>125.600</td>\n",
       "      <td>125.690</td>\n",
       "      <td>0.2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-26 16:33:00</td>\n",
       "      <td>125.700</td>\n",
       "      <td>125.790</td>\n",
       "      <td>125.700</td>\n",
       "      <td>125.780</td>\n",
       "      <td>0.3564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-26 16:34:00</td>\n",
       "      <td>125.790</td>\n",
       "      <td>125.810</td>\n",
       "      <td>125.750</td>\n",
       "      <td>125.770</td>\n",
       "      <td>0.3098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203938</th>\n",
       "      <td>2019-02-28 22:55:00</td>\n",
       "      <td>173.118</td>\n",
       "      <td>173.118</td>\n",
       "      <td>172.938</td>\n",
       "      <td>172.988</td>\n",
       "      <td>1.8648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203939</th>\n",
       "      <td>2019-02-28 22:56:00</td>\n",
       "      <td>172.988</td>\n",
       "      <td>173.048</td>\n",
       "      <td>172.907</td>\n",
       "      <td>172.988</td>\n",
       "      <td>1.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203940</th>\n",
       "      <td>2019-02-28 22:57:00</td>\n",
       "      <td>172.988</td>\n",
       "      <td>172.998</td>\n",
       "      <td>172.888</td>\n",
       "      <td>172.918</td>\n",
       "      <td>1.9429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203941</th>\n",
       "      <td>2019-02-28 22:58:00</td>\n",
       "      <td>172.917</td>\n",
       "      <td>173.058</td>\n",
       "      <td>172.907</td>\n",
       "      <td>173.038</td>\n",
       "      <td>1.9219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203942</th>\n",
       "      <td>2019-02-28 22:59:00</td>\n",
       "      <td>173.048</td>\n",
       "      <td>173.168</td>\n",
       "      <td>173.017</td>\n",
       "      <td>173.157</td>\n",
       "      <td>2.4608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203943 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime     open     high      low    close  volume\n",
       "0      2017-01-26 16:30:00  125.570  125.690  125.570  125.660  0.3360\n",
       "1      2017-01-26 16:31:00  125.670  125.670  125.610  125.630  0.2796\n",
       "2      2017-01-26 16:32:00  125.630  125.690  125.600  125.690  0.2770\n",
       "3      2017-01-26 16:33:00  125.700  125.790  125.700  125.780  0.3564\n",
       "4      2017-01-26 16:34:00  125.790  125.810  125.750  125.770  0.3098\n",
       "...                    ...      ...      ...      ...      ...     ...\n",
       "203938 2019-02-28 22:55:00  173.118  173.118  172.938  172.988  1.8648\n",
       "203939 2019-02-28 22:56:00  172.988  173.048  172.907  172.988  1.8967\n",
       "203940 2019-02-28 22:57:00  172.988  172.998  172.888  172.918  1.9429\n",
       "203941 2019-02-28 22:58:00  172.917  173.058  172.907  173.038  1.9219\n",
       "203942 2019-02-28 22:59:00  173.048  173.168  173.017  173.157  2.4608\n",
       "\n",
       "[203943 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Local time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2019 00:00:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2019 00:01:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2019 00:02:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.01.2019 00:03:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2019 00:04:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736755</th>\n",
       "      <td>31.12.2018 23:55:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736756</th>\n",
       "      <td>31.12.2018 23:56:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736757</th>\n",
       "      <td>31.12.2018 23:57:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736758</th>\n",
       "      <td>31.12.2018 23:58:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736759</th>\n",
       "      <td>31.12.2018 23:59:00.000 GMT+0200</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>157.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1736760 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Local time     Open     High      Low    Close  \\\n",
       "0        01.01.2019 00:00:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "1        01.01.2019 00:01:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "2        01.01.2019 00:02:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "3        01.01.2019 00:03:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "4        01.01.2019 00:04:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "...                                   ...      ...      ...      ...      ...   \n",
       "1736755  31.12.2018 23:55:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "1736756  31.12.2018 23:56:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "1736757  31.12.2018 23:57:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "1736758  31.12.2018 23:58:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "1736759  31.12.2018 23:59:00.000 GMT+0200  157.718  157.718  157.718  157.718   \n",
       "\n",
       "         Volume  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "1736755     0.0  \n",
       "1736756     0.0  \n",
       "1736757     0.0  \n",
       "1736758     0.0  \n",
       "1736759     0.0  \n",
       "\n",
       "[1736760 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_data = common.get_dukas_data(\"AAPL\")\n",
    "apple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-26 16:30:00</td>\n",
       "      <td>121.680</td>\n",
       "      <td>121.894</td>\n",
       "      <td>121.660</td>\n",
       "      <td>121.710</td>\n",
       "      <td>0.4459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-26 16:31:00</td>\n",
       "      <td>121.720</td>\n",
       "      <td>121.894</td>\n",
       "      <td>121.702</td>\n",
       "      <td>121.891</td>\n",
       "      <td>0.3697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-26 16:32:00</td>\n",
       "      <td>121.851</td>\n",
       "      <td>122.090</td>\n",
       "      <td>121.840</td>\n",
       "      <td>122.060</td>\n",
       "      <td>0.4814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-26 16:33:00</td>\n",
       "      <td>122.070</td>\n",
       "      <td>122.240</td>\n",
       "      <td>122.053</td>\n",
       "      <td>122.240</td>\n",
       "      <td>0.6113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-26 16:34:00</td>\n",
       "      <td>122.240</td>\n",
       "      <td>122.430</td>\n",
       "      <td>122.150</td>\n",
       "      <td>122.213</td>\n",
       "      <td>0.6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337484</th>\n",
       "      <td>2020-05-22 22:56:00</td>\n",
       "      <td>318.317</td>\n",
       "      <td>318.538</td>\n",
       "      <td>318.238</td>\n",
       "      <td>318.438</td>\n",
       "      <td>3.4204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337485</th>\n",
       "      <td>2020-05-22 22:57:00</td>\n",
       "      <td>318.448</td>\n",
       "      <td>318.678</td>\n",
       "      <td>318.388</td>\n",
       "      <td>318.677</td>\n",
       "      <td>3.6803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337486</th>\n",
       "      <td>2020-05-22 22:58:00</td>\n",
       "      <td>318.678</td>\n",
       "      <td>318.918</td>\n",
       "      <td>318.638</td>\n",
       "      <td>318.818</td>\n",
       "      <td>3.8403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337487</th>\n",
       "      <td>2020-05-22 22:59:00</td>\n",
       "      <td>318.818</td>\n",
       "      <td>319.168</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>3.5204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337488</th>\n",
       "      <td>2020-05-22 23:00:00</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337489 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime     open     high      low    close  volume\n",
       "0      2017-01-26 16:30:00  121.680  121.894  121.660  121.710  0.4459\n",
       "1      2017-01-26 16:31:00  121.720  121.894  121.702  121.891  0.3697\n",
       "2      2017-01-26 16:32:00  121.851  122.090  121.840  122.060  0.4814\n",
       "3      2017-01-26 16:33:00  122.070  122.240  122.053  122.240  0.6113\n",
       "4      2017-01-26 16:34:00  122.240  122.430  122.150  122.213  0.6310\n",
       "...                    ...      ...      ...      ...      ...     ...\n",
       "337484 2020-05-22 22:56:00  318.317  318.538  318.238  318.438  3.4204\n",
       "337485 2020-05-22 22:57:00  318.448  318.678  318.388  318.677  3.6803\n",
       "337486 2020-05-22 22:58:00  318.678  318.918  318.638  318.818  3.8403\n",
       "337487 2020-05-22 22:59:00  318.818  319.168  318.688  318.688  3.5204\n",
       "337488 2020-05-22 23:00:00  318.688  318.688  318.688  318.688  0.0000\n",
       "\n",
       "[337489 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data_from_dukas(df):\n",
    "    \n",
    "    # rename columns\n",
    "    new_cols_map = {col: col.lower() for col in df}\n",
    "    new_cols_map['Local time'] = \"datetime\"\n",
    "    df = df.rename(columns=new_cols_map)\n",
    "\n",
    "    # convert datetime type (string to datetime)\n",
    "    df['datetime'] = df['datetime'].str.replace(r\"\\:00\\.000 GMT\\+0\\d00\",\"\")\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d.%m.%Y %H:%M')\n",
    "    \n",
    "    df = df.set_index(['datetime']).between_time(\"16:30\", \"23:00\").reset_index()\n",
    "    df = df.sort_values(by=['datetime'])\n",
    "    \n",
    "    # drop inactive days\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    mask = (df['day_of_week'] != 5) & (df['day_of_week'] != 6)\n",
    "    df = df[mask]\n",
    "    del df['day_of_week']\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "apple_data = preprocess_data_from_dukas(apple_data)\n",
    "apple_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long / Short precentage: 0.5024341534094444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-26 16:30:00</td>\n",
       "      <td>121.680</td>\n",
       "      <td>121.894</td>\n",
       "      <td>121.660</td>\n",
       "      <td>121.710</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-26 16:31:00</td>\n",
       "      <td>121.720</td>\n",
       "      <td>121.894</td>\n",
       "      <td>121.702</td>\n",
       "      <td>121.891</td>\n",
       "      <td>0.3697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-26 16:32:00</td>\n",
       "      <td>121.851</td>\n",
       "      <td>122.090</td>\n",
       "      <td>121.840</td>\n",
       "      <td>122.060</td>\n",
       "      <td>0.4814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-26 16:33:00</td>\n",
       "      <td>122.070</td>\n",
       "      <td>122.240</td>\n",
       "      <td>122.053</td>\n",
       "      <td>122.240</td>\n",
       "      <td>0.6113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-26 16:34:00</td>\n",
       "      <td>122.240</td>\n",
       "      <td>122.430</td>\n",
       "      <td>122.150</td>\n",
       "      <td>122.213</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337484</th>\n",
       "      <td>2020-05-22 22:56:00</td>\n",
       "      <td>318.317</td>\n",
       "      <td>318.538</td>\n",
       "      <td>318.238</td>\n",
       "      <td>318.438</td>\n",
       "      <td>3.4204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337485</th>\n",
       "      <td>2020-05-22 22:57:00</td>\n",
       "      <td>318.448</td>\n",
       "      <td>318.678</td>\n",
       "      <td>318.388</td>\n",
       "      <td>318.677</td>\n",
       "      <td>3.6803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337486</th>\n",
       "      <td>2020-05-22 22:58:00</td>\n",
       "      <td>318.678</td>\n",
       "      <td>318.918</td>\n",
       "      <td>318.638</td>\n",
       "      <td>318.818</td>\n",
       "      <td>3.8403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337487</th>\n",
       "      <td>2020-05-22 22:59:00</td>\n",
       "      <td>318.818</td>\n",
       "      <td>319.168</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>3.5204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337488</th>\n",
       "      <td>2020-05-22 23:00:00</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>318.688</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337489 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime     open     high      low    close  volume  target\n",
       "0      2017-01-26 16:30:00  121.680  121.894  121.660  121.710  0.4459       1\n",
       "1      2017-01-26 16:31:00  121.720  121.894  121.702  121.891  0.3697       0\n",
       "2      2017-01-26 16:32:00  121.851  122.090  121.840  122.060  0.4814       0\n",
       "3      2017-01-26 16:33:00  122.070  122.240  122.053  122.240  0.6113       0\n",
       "4      2017-01-26 16:34:00  122.240  122.430  122.150  122.213  0.6310       0\n",
       "...                    ...      ...      ...      ...      ...     ...     ...\n",
       "337484 2020-05-22 22:56:00  318.317  318.538  318.238  318.438  3.4204       0\n",
       "337485 2020-05-22 22:57:00  318.448  318.678  318.388  318.677  3.6803       0\n",
       "337486 2020-05-22 22:58:00  318.678  318.918  318.638  318.818  3.8403       0\n",
       "337487 2020-05-22 22:59:00  318.818  319.168  318.688  318.688  3.5204       0\n",
       "337488 2020-05-22 23:00:00  318.688  318.688  318.688  318.688  0.0000       0\n",
       "\n",
       "[337489 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_data['target'] = np.where(apple_data['close'] < apple_data['close'].shift(-50), 1, 0)\n",
    "print(\"Long / Short precentage: \" + str(apple_data['target'].sum() / len(apple_data)))\n",
    "apple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i will start using only close & volume prices\n",
    "apple_close_price = apple_data[['close', 'volume']].values\n",
    "apple_targets = apple_data['target'].values.reshape(-1, 1)\n",
    "from keras.utils import to_categorical\n",
    "apple_targets = to_categorical(apple_targets)\n",
    "apple_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SingleDataSet:\n",
    "    X: np.array\n",
    "    y: np.array\n",
    "    \n",
    "    def generator(self, shuffle=False):\n",
    "        shifted_y = np.insert(self.y, 0, 0, axis=0)[:-1]  # TimeseriesGenerator uses stride=1, but we want stride=0\n",
    "        return TimeseriesGenerator(self.X, shifted_y, length=500, sampling_rate=1, stride=1, \n",
    "                                   batch_size=128, shuffle=shuffle)\n",
    "\n",
    "\n",
    "class FullDataSet(NamedTuple):\n",
    "    training: SingleDataSet\n",
    "    validation: SingleDataSet\n",
    "    test: SingleDataSet\n",
    "\n",
    "    x_scaler: Optional[MinMaxScaler] = None\n",
    "    y_scaler: Optional[MinMaxScaler] = None\n",
    "\n",
    "    def normelize(self):\n",
    "        x_scaler = MinMaxScaler()\n",
    "        y_scaler = MinMaxScaler()\n",
    "        x_scaler.fit(self.training.X)\n",
    "        y_scaler.fit(self.training.y)\n",
    "        datasets = []\n",
    "        for dataset in [self.training, self.validation, self.test]:\n",
    "            new_X = x_scaler.transform(dataset.X)\n",
    "            new_y = y_scaler.transform(dataset.y)\n",
    "            datasets.append(SingleDataSet(new_X, new_y))\n",
    "        return FullDataSet(*datasets, x_scaler, y_scaler)\n",
    "\n",
    "\n",
    "def train_val_test_split(samples, features) -> FullDataSet:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        samples, features, test_size=0.2, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.25, shuffle=False)\n",
    "    train = SingleDataSet(X_train, y_train)\n",
    "    validation = SingleDataSet(X_val, y_val)\n",
    "    test = SingleDataSet(X_test, y_test)\n",
    "    return FullDataSet(train, validation, test)\n",
    "\n",
    "\n",
    "dataset = train_val_test_split(apple_close_price, apple_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "normelize_dataset = dataset.normelize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_callbacks(model_name: str):\n",
    "    # define the checkpoint\n",
    "    filepath=\"weights_improvement_\" + model_name + \"_{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # tensorboard\n",
    "    log_dir = f'logs/fit/{model_name}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "\n",
    "    return [checkpoint, tensorboard_callback, es]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl_using_price(curr_price, predicted_price, true_price):    \n",
    "    predicted_roc = predicted_price / curr_price - 1\n",
    "    true_roc = true_price / curr_price - 1\n",
    "    return rate_of_change_PnL(predicted_roc, true_roc)\n",
    "\n",
    "\n",
    "# from common.py - will return a vector of profits/losses (percentage wise) for every trade\n",
    "def rate_of_change_PnL(pred_roc, actual_roc):\n",
    "    \"\"\" calculate the profit/loss of every prediction \"\"\"\n",
    "    return (pred_roc > 0) * actual_roc - (pred_roc < 0) * actual_roc\n",
    "\n",
    "\n",
    "def mean(l: list) -> float:\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "\n",
    "def calculate_model_pnl(model, dataset: FullDataSet, data_to_use: str) -> float:\n",
    "    data_generator = getattr(dataset, data_to_use).generator()\n",
    "    # checking how well the model did:\n",
    "    pnls = []\n",
    "    for batch in data_generator:\n",
    "        samples, targets = batch\n",
    "        predictions = model.predict(samples)\n",
    "        for sample, target, prediction in zip(samples, targets, predictions):\n",
    "            original_last_price_point = dataset.x_scaler.inverse_transform(sample)[:, 0][-1]\n",
    "            original_prediction = dataset.y_scaler.inverse_transform(prediction.reshape(1, 1))[0][0]\n",
    "            original_target = dataset.y_scaler.inverse_transform(target.reshape(1, 1))[0][0]\n",
    "            pnls.append(pnl_using_price(original_last_price_point, original_prediction, original_target))\n",
    "        \n",
    "    return mean(pnls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = normelize_dataset.training.generator(shuffle=True)\n",
    "n_timesteps = train_generator[0][0][0].shape[0]\n",
    "n_features = train_generator[0][0][0].shape[1]\n",
    "n_outputs = train_generator[0][1][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 498, 64)           448       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 496, 64)           12352     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 496, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 248, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               1587300   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,600,302\n",
      "Trainable params: 1,600,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6951 - accuracy: 0.5095\n",
      "Epoch 00001: loss improved from inf to 0.69508, saving model to weights_improvement_CNN_categorical_200_epochs_01-0.6951.hdf5\n",
      "200/200 [==============================] - 55s 273ms/step - loss: 0.6951 - accuracy: 0.5097 - val_loss: 0.6912 - val_accuracy: 0.4984\n",
      "Epoch 2/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5145\n",
      "Epoch 00002: loss improved from 0.69508 to 0.69279, saving model to weights_improvement_CNN_categorical_200_epochs_02-0.6928.hdf5\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 0.6928 - accuracy: 0.5145 - val_loss: 0.6931 - val_accuracy: 0.4933\n",
      "Epoch 3/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5244\n",
      "Epoch 00003: loss improved from 0.69279 to 0.69106, saving model to weights_improvement_CNN_categorical_200_epochs_03-0.6911.hdf5\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.6911 - accuracy: 0.5244 - val_loss: 0.6932 - val_accuracy: 0.5027\n",
      "Epoch 4/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6841 - accuracy: 0.5356\n",
      "Epoch 00004: loss improved from 0.69106 to 0.68406, saving model to weights_improvement_CNN_categorical_200_epochs_04-0.6841.hdf5\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 0.6841 - accuracy: 0.5355 - val_loss: 0.6945 - val_accuracy: 0.5191\n",
      "Epoch 5/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6778 - accuracy: 0.5296\n",
      "Epoch 00005: loss improved from 0.68406 to 0.67773, saving model to weights_improvement_CNN_categorical_200_epochs_05-0.6777.hdf5\n",
      "200/200 [==============================] - 59s 293ms/step - loss: 0.6777 - accuracy: 0.5297 - val_loss: 0.6941 - val_accuracy: 0.4667\n",
      "Epoch 6/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6745 - accuracy: 0.5411\n",
      "Epoch 00006: loss improved from 0.67773 to 0.67452, saving model to weights_improvement_CNN_categorical_200_epochs_06-0.6745.hdf5\n",
      "200/200 [==============================] - 57s 287ms/step - loss: 0.6745 - accuracy: 0.5411 - val_loss: 0.6938 - val_accuracy: 0.5431\n",
      "Epoch 7/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6734 - accuracy: 0.5413\n",
      "Epoch 00007: loss improved from 0.67452 to 0.67342, saving model to weights_improvement_CNN_categorical_200_epochs_07-0.6734.hdf5\n",
      "200/200 [==============================] - 57s 287ms/step - loss: 0.6734 - accuracy: 0.5413 - val_loss: 0.6942 - val_accuracy: 0.5116\n",
      "Epoch 8/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6702 - accuracy: 0.5479\n",
      "Epoch 00008: loss improved from 0.67342 to 0.67029, saving model to weights_improvement_CNN_categorical_200_epochs_08-0.6703.hdf5\n",
      "200/200 [==============================] - 57s 286ms/step - loss: 0.6703 - accuracy: 0.5475 - val_loss: 0.6961 - val_accuracy: 0.5294\n",
      "Epoch 9/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6687 - accuracy: 0.5532\n",
      "Epoch 00009: loss improved from 0.67029 to 0.66872, saving model to weights_improvement_CNN_categorical_200_epochs_09-0.6687.hdf5\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.6687 - accuracy: 0.5533 - val_loss: 0.6971 - val_accuracy: 0.5136\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6690 - accuracy: 0.5467\n",
      "Epoch 00010: loss did not improve from 0.66872\n",
      "200/200 [==============================] - 58s 289ms/step - loss: 0.6689 - accuracy: 0.5466 - val_loss: 0.6961 - val_accuracy: 0.5023\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6644 - accuracy: 0.5508\n",
      "Epoch 00011: loss improved from 0.66872 to 0.66422, saving model to weights_improvement_CNN_categorical_200_epochs_11-0.6642.hdf5\n",
      "200/200 [==============================] - 58s 292ms/step - loss: 0.6642 - accuracy: 0.5507 - val_loss: 0.6959 - val_accuracy: 0.5119\n",
      "Epoch 12/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6633 - accuracy: 0.5543\n",
      "Epoch 00012: loss improved from 0.66422 to 0.66328, saving model to weights_improvement_CNN_categorical_200_epochs_12-0.6633.hdf5\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.6633 - accuracy: 0.5544 - val_loss: 0.6966 - val_accuracy: 0.4750\n",
      "Epoch 13/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6601 - accuracy: 0.5540\n",
      "Epoch 00013: loss improved from 0.66328 to 0.66023, saving model to weights_improvement_CNN_categorical_200_epochs_13-0.6602.hdf5\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.6602 - accuracy: 0.5537 - val_loss: 0.6959 - val_accuracy: 0.4831\n",
      "Epoch 14/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6590 - accuracy: 0.5635\n",
      "Epoch 00014: loss improved from 0.66023 to 0.65910, saving model to weights_improvement_CNN_categorical_200_epochs_14-0.6591.hdf5\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 0.6591 - accuracy: 0.5631 - val_loss: 0.6998 - val_accuracy: 0.5153\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6571 - accuracy: 0.5702\n",
      "Epoch 00015: loss improved from 0.65910 to 0.65707, saving model to weights_improvement_CNN_categorical_200_epochs_15-0.6571.hdf5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 0.6571 - accuracy: 0.5700 - val_loss: 0.7000 - val_accuracy: 0.5073\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6543 - accuracy: 0.5773\n",
      "Epoch 00016: loss improved from 0.65707 to 0.65435, saving model to weights_improvement_CNN_categorical_200_epochs_16-0.6544.hdf5\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.6544 - accuracy: 0.5773 - val_loss: 0.6966 - val_accuracy: 0.5288\n",
      "Epoch 17/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.5771\n",
      "Epoch 00017: loss improved from 0.65435 to 0.65176, saving model to weights_improvement_CNN_categorical_200_epochs_17-0.6518.hdf5\n",
      "200/200 [==============================] - 55s 276ms/step - loss: 0.6518 - accuracy: 0.5772 - val_loss: 0.6982 - val_accuracy: 0.5303\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.5777\n",
      "Epoch 00018: loss improved from 0.65176 to 0.65130, saving model to weights_improvement_CNN_categorical_200_epochs_18-0.6513.hdf5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 0.6513 - accuracy: 0.5779 - val_loss: 0.6956 - val_accuracy: 0.5133\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6477 - accuracy: 0.5789\n",
      "Epoch 00019: loss improved from 0.65130 to 0.64777, saving model to weights_improvement_CNN_categorical_200_epochs_19-0.6478.hdf5\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 0.6478 - accuracy: 0.5784 - val_loss: 0.6972 - val_accuracy: 0.4998\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6447 - accuracy: 0.5843\n",
      "Epoch 00020: loss improved from 0.64777 to 0.64465, saving model to weights_improvement_CNN_categorical_200_epochs_20-0.6446.hdf5\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 0.6446 - accuracy: 0.5846 - val_loss: 0.7014 - val_accuracy: 0.4812\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6449 - accuracy: 0.5867\n",
      "Epoch 00021: loss did not improve from 0.64465\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 0.6450 - accuracy: 0.5866 - val_loss: 0.6997 - val_accuracy: 0.4925\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6423 - accuracy: 0.5857\n",
      "Epoch 00022: loss improved from 0.64465 to 0.64215, saving model to weights_improvement_CNN_categorical_200_epochs_22-0.6422.hdf5\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 0.6422 - accuracy: 0.5857 - val_loss: 0.7061 - val_accuracy: 0.4769\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6360 - accuracy: 0.5932\n",
      "Epoch 00023: loss improved from 0.64215 to 0.63604, saving model to weights_improvement_CNN_categorical_200_epochs_23-0.6360.hdf5\n",
      "200/200 [==============================] - 70s 350ms/step - loss: 0.6360 - accuracy: 0.5934 - val_loss: 0.7008 - val_accuracy: 0.4778\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.5977\n",
      "Epoch 00024: loss did not improve from 0.63604\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.6368 - accuracy: 0.5978 - val_loss: 0.7111 - val_accuracy: 0.4869\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6346 - accuracy: 0.5938\n",
      "Epoch 00025: loss improved from 0.63604 to 0.63445, saving model to weights_improvement_CNN_categorical_200_epochs_25-0.6344.hdf5\n",
      "200/200 [==============================] - 70s 348ms/step - loss: 0.6344 - accuracy: 0.5938 - val_loss: 0.7001 - val_accuracy: 0.5103\n",
      "Epoch 26/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.6049\n",
      "Epoch 00026: loss improved from 0.63445 to 0.63169, saving model to weights_improvement_CNN_categorical_200_epochs_26-0.6317.hdf5\n",
      "200/200 [==============================] - 67s 337ms/step - loss: 0.6317 - accuracy: 0.6051 - val_loss: 0.7068 - val_accuracy: 0.5059\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6303 - accuracy: 0.6041\n",
      "Epoch 00027: loss improved from 0.63169 to 0.63051, saving model to weights_improvement_CNN_categorical_200_epochs_27-0.6305.hdf5\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 0.6305 - accuracy: 0.6037 - val_loss: 0.7084 - val_accuracy: 0.4834\n",
      "Epoch 28/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6203 - accuracy: 0.6164\n",
      "Epoch 00028: loss improved from 0.63051 to 0.62034, saving model to weights_improvement_CNN_categorical_200_epochs_28-0.6203.hdf5\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 0.6203 - accuracy: 0.6165 - val_loss: 0.7134 - val_accuracy: 0.5125\n",
      "Epoch 29/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6207 - accuracy: 0.6177\n",
      "Epoch 00029: loss did not improve from 0.62034\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6206 - accuracy: 0.6179 - val_loss: 0.7222 - val_accuracy: 0.5167\n",
      "Epoch 30/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6178 - accuracy: 0.6154\n",
      "Epoch 00030: loss improved from 0.62034 to 0.61794, saving model to weights_improvement_CNN_categorical_200_epochs_30-0.6179.hdf5\n",
      "200/200 [==============================] - 61s 303ms/step - loss: 0.6179 - accuracy: 0.6152 - val_loss: 0.7343 - val_accuracy: 0.4888\n",
      "Epoch 31/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6146 - accuracy: 0.6277\n",
      "Epoch 00031: loss improved from 0.61794 to 0.61455, saving model to weights_improvement_CNN_categorical_200_epochs_31-0.6145.hdf5\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.6145 - accuracy: 0.6277 - val_loss: 0.7157 - val_accuracy: 0.5102\n",
      "Epoch 32/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6078 - accuracy: 0.6303\n",
      "Epoch 00032: loss improved from 0.61455 to 0.60762, saving model to weights_improvement_CNN_categorical_200_epochs_32-0.6076.hdf5\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.6076 - accuracy: 0.6307 - val_loss: 0.7189 - val_accuracy: 0.5106\n",
      "Epoch 33/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6040 - accuracy: 0.6459\n",
      "Epoch 00033: loss improved from 0.60762 to 0.60413, saving model to weights_improvement_CNN_categorical_200_epochs_33-0.6041.hdf5\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.6041 - accuracy: 0.6458 - val_loss: 0.7099 - val_accuracy: 0.5113\n",
      "Epoch 34/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.6014 - accuracy: 0.6437\n",
      "Epoch 00034: loss improved from 0.60413 to 0.60129, saving model to weights_improvement_CNN_categorical_200_epochs_34-0.6013.hdf5\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6013 - accuracy: 0.6437 - val_loss: 0.7152 - val_accuracy: 0.5116\n",
      "Epoch 35/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5946 - accuracy: 0.6562\n",
      "Epoch 00035: loss improved from 0.60129 to 0.59489, saving model to weights_improvement_CNN_categorical_200_epochs_35-0.5949.hdf5\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 0.5949 - accuracy: 0.6556 - val_loss: 0.7211 - val_accuracy: 0.4995\n",
      "Epoch 36/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5963 - accuracy: 0.6496\n",
      "Epoch 00036: loss did not improve from 0.59489\n",
      "200/200 [==============================] - 59s 293ms/step - loss: 0.5964 - accuracy: 0.6496 - val_loss: 0.7148 - val_accuracy: 0.5008\n",
      "Epoch 37/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5923 - accuracy: 0.6530\n",
      "Epoch 00037: loss improved from 0.59489 to 0.59213, saving model to weights_improvement_CNN_categorical_200_epochs_37-0.5921.hdf5\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 0.5921 - accuracy: 0.6529 - val_loss: 0.7127 - val_accuracy: 0.4978\n",
      "Epoch 38/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5866 - accuracy: 0.6580\n",
      "Epoch 00038: loss improved from 0.59213 to 0.58642, saving model to weights_improvement_CNN_categorical_200_epochs_38-0.5864.hdf5\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.5864 - accuracy: 0.6583 - val_loss: 0.7265 - val_accuracy: 0.5139\n",
      "Epoch 39/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.6648\n",
      "Epoch 00039: loss improved from 0.58642 to 0.57921, saving model to weights_improvement_CNN_categorical_200_epochs_39-0.5792.hdf5\n",
      "200/200 [==============================] - 61s 304ms/step - loss: 0.5792 - accuracy: 0.6645 - val_loss: 0.7431 - val_accuracy: 0.5075\n",
      "Epoch 40/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5786 - accuracy: 0.6625\n",
      "Epoch 00040: loss improved from 0.57921 to 0.57874, saving model to weights_improvement_CNN_categorical_200_epochs_40-0.5787.hdf5\n",
      "200/200 [==============================] - 58s 292ms/step - loss: 0.5787 - accuracy: 0.6623 - val_loss: 0.7264 - val_accuracy: 0.5131\n",
      "Epoch 41/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.6703\n",
      "Epoch 00041: loss improved from 0.57874 to 0.57048, saving model to weights_improvement_CNN_categorical_200_epochs_41-0.5705.hdf5\n",
      "200/200 [==============================] - 57s 286ms/step - loss: 0.5705 - accuracy: 0.6702 - val_loss: 0.7185 - val_accuracy: 0.5219\n",
      "Epoch 42/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.6730\n",
      "Epoch 00042: loss improved from 0.57048 to 0.56789, saving model to weights_improvement_CNN_categorical_200_epochs_42-0.5679.hdf5\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 0.5679 - accuracy: 0.6731 - val_loss: 0.7361 - val_accuracy: 0.5141\n",
      "Epoch 43/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.6645\n",
      "Epoch 00043: loss did not improve from 0.56789\n",
      "200/200 [==============================] - 59s 296ms/step - loss: 0.5741 - accuracy: 0.6646 - val_loss: 0.7290 - val_accuracy: 0.5134\n",
      "Epoch 44/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.6740\n",
      "Epoch 00044: loss did not improve from 0.56789\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5697 - accuracy: 0.6740 - val_loss: 0.7230 - val_accuracy: 0.5095\n",
      "Epoch 45/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.6751\n",
      "Epoch 00045: loss improved from 0.56789 to 0.56593, saving model to weights_improvement_CNN_categorical_200_epochs_45-0.5659.hdf5\n",
      "200/200 [==============================] - 58s 289ms/step - loss: 0.5659 - accuracy: 0.6753 - val_loss: 0.7249 - val_accuracy: 0.5116\n",
      "Epoch 46/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.6839\n",
      "Epoch 00046: loss improved from 0.56593 to 0.55733, saving model to weights_improvement_CNN_categorical_200_epochs_46-0.5573.hdf5\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 0.5573 - accuracy: 0.6839 - val_loss: 0.7431 - val_accuracy: 0.5145\n",
      "Epoch 47/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.6824\n",
      "Epoch 00047: loss did not improve from 0.55733\n",
      "200/200 [==============================] - 61s 304ms/step - loss: 0.5612 - accuracy: 0.6823 - val_loss: 0.7362 - val_accuracy: 0.5234\n",
      "Epoch 48/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5531 - accuracy: 0.6879\n",
      "Epoch 00048: loss improved from 0.55733 to 0.55333, saving model to weights_improvement_CNN_categorical_200_epochs_48-0.5533.hdf5\n",
      "200/200 [==============================] - 56s 278ms/step - loss: 0.5533 - accuracy: 0.6875 - val_loss: 0.7368 - val_accuracy: 0.5078\n",
      "Epoch 49/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5426 - accuracy: 0.6940\n",
      "Epoch 00049: loss improved from 0.55333 to 0.54281, saving model to weights_improvement_CNN_categorical_200_epochs_49-0.5428.hdf5\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.5428 - accuracy: 0.6938 - val_loss: 0.7465 - val_accuracy: 0.5106\n",
      "Epoch 50/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5424 - accuracy: 0.6987\n",
      "Epoch 00050: loss improved from 0.54281 to 0.54236, saving model to weights_improvement_CNN_categorical_200_epochs_50-0.5424.hdf5\n",
      "200/200 [==============================] - 58s 291ms/step - loss: 0.5424 - accuracy: 0.6987 - val_loss: 0.7475 - val_accuracy: 0.5402\n",
      "Epoch 51/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5399 - accuracy: 0.6984\n",
      "Epoch 00051: loss improved from 0.54236 to 0.53995, saving model to weights_improvement_CNN_categorical_200_epochs_51-0.5400.hdf5\n",
      "200/200 [==============================] - 58s 292ms/step - loss: 0.5400 - accuracy: 0.6983 - val_loss: 0.7392 - val_accuracy: 0.5139\n",
      "Epoch 52/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7029\n",
      "Epoch 00052: loss improved from 0.53995 to 0.53922, saving model to weights_improvement_CNN_categorical_200_epochs_52-0.5392.hdf5\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.5392 - accuracy: 0.7030 - val_loss: 0.7391 - val_accuracy: 0.5159\n",
      "Epoch 53/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.7124\n",
      "Epoch 00053: loss improved from 0.53922 to 0.52737, saving model to weights_improvement_CNN_categorical_200_epochs_53-0.5274.hdf5\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 0.5274 - accuracy: 0.7122 - val_loss: 0.7462 - val_accuracy: 0.4906\n",
      "Epoch 54/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5322 - accuracy: 0.7096\n",
      "Epoch 00054: loss did not improve from 0.52737\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.5321 - accuracy: 0.7095 - val_loss: 0.7673 - val_accuracy: 0.5153\n",
      "Epoch 55/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7092\n",
      "Epoch 00055: loss did not improve from 0.52737\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 0.5293 - accuracy: 0.7092 - val_loss: 0.7460 - val_accuracy: 0.5116\n",
      "Epoch 56/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7154\n",
      "Epoch 00056: loss improved from 0.52737 to 0.52232, saving model to weights_improvement_CNN_categorical_200_epochs_56-0.5223.hdf5\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.5223 - accuracy: 0.7151 - val_loss: 0.7715 - val_accuracy: 0.5050\n",
      "Epoch 57/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7212\n",
      "Epoch 00057: loss improved from 0.52232 to 0.51329, saving model to weights_improvement_CNN_categorical_200_epochs_57-0.5133.hdf5\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.5133 - accuracy: 0.7212 - val_loss: 0.7457 - val_accuracy: 0.5155\n",
      "Epoch 58/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5202 - accuracy: 0.7200\n",
      "Epoch 00058: loss did not improve from 0.51329\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 0.5202 - accuracy: 0.7198 - val_loss: 0.7742 - val_accuracy: 0.5150\n",
      "Epoch 59/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5177 - accuracy: 0.7160\n",
      "Epoch 00059: loss did not improve from 0.51329\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5176 - accuracy: 0.7162 - val_loss: 0.7693 - val_accuracy: 0.5080\n",
      "Epoch 60/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5149 - accuracy: 0.7182\n",
      "Epoch 00060: loss did not improve from 0.51329\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.5147 - accuracy: 0.7186 - val_loss: 0.7766 - val_accuracy: 0.5055\n",
      "Epoch 61/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5045 - accuracy: 0.7254\n",
      "Epoch 00061: loss improved from 0.51329 to 0.50446, saving model to weights_improvement_CNN_categorical_200_epochs_61-0.5045.hdf5\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 0.5045 - accuracy: 0.7254 - val_loss: 0.7633 - val_accuracy: 0.5047\n",
      "Epoch 62/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7287\n",
      "Epoch 00062: loss did not improve from 0.50446\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 0.5070 - accuracy: 0.7286 - val_loss: 0.7752 - val_accuracy: 0.5058\n",
      "Epoch 63/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4986 - accuracy: 0.7363\n",
      "Epoch 00063: loss improved from 0.50446 to 0.49893, saving model to weights_improvement_CNN_categorical_200_epochs_63-0.4989.hdf5\n",
      "200/200 [==============================] - 59s 294ms/step - loss: 0.4989 - accuracy: 0.7361 - val_loss: 0.7716 - val_accuracy: 0.5155\n",
      "Epoch 64/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4944 - accuracy: 0.7379\n",
      "Epoch 00064: loss improved from 0.49893 to 0.49458, saving model to weights_improvement_CNN_categorical_200_epochs_64-0.4946.hdf5\n",
      "200/200 [==============================] - 57s 287ms/step - loss: 0.4946 - accuracy: 0.7377 - val_loss: 0.8050 - val_accuracy: 0.5141\n",
      "Epoch 65/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4942 - accuracy: 0.7366\n",
      "Epoch 00065: loss improved from 0.49458 to 0.49392, saving model to weights_improvement_CNN_categorical_200_epochs_65-0.4939.hdf5\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.4939 - accuracy: 0.7370 - val_loss: 0.7884 - val_accuracy: 0.5069\n",
      "Epoch 66/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.7385\n",
      "Epoch 00066: loss improved from 0.49392 to 0.48902, saving model to weights_improvement_CNN_categorical_200_epochs_66-0.4890.hdf5\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 0.4890 - accuracy: 0.7384 - val_loss: 0.8086 - val_accuracy: 0.5003\n",
      "Epoch 67/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4882 - accuracy: 0.7392\n",
      "Epoch 00067: loss improved from 0.48902 to 0.48827, saving model to weights_improvement_CNN_categorical_200_epochs_67-0.4883.hdf5\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4883 - accuracy: 0.7391 - val_loss: 0.7871 - val_accuracy: 0.5170\n",
      "Epoch 68/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4896 - accuracy: 0.7399\n",
      "Epoch 00068: loss did not improve from 0.48827\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4894 - accuracy: 0.7401 - val_loss: 0.8017 - val_accuracy: 0.5098\n",
      "Epoch 69/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7487\n",
      "Epoch 00069: loss improved from 0.48827 to 0.47916, saving model to weights_improvement_CNN_categorical_200_epochs_69-0.4792.hdf5\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 0.4792 - accuracy: 0.7489 - val_loss: 0.8040 - val_accuracy: 0.5048\n",
      "Epoch 70/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4814 - accuracy: 0.7467\n",
      "Epoch 00070: loss did not improve from 0.47916\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 0.4815 - accuracy: 0.7466 - val_loss: 0.7852 - val_accuracy: 0.5042\n",
      "Epoch 71/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7508\n",
      "Epoch 00071: loss improved from 0.47916 to 0.47667, saving model to weights_improvement_CNN_categorical_200_epochs_71-0.4767.hdf5\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4767 - accuracy: 0.7511 - val_loss: 0.8127 - val_accuracy: 0.5063\n",
      "Epoch 72/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7483\n",
      "Epoch 00072: loss did not improve from 0.47667\n",
      "200/200 [==============================] - 56s 279ms/step - loss: 0.4771 - accuracy: 0.7480 - val_loss: 0.7889 - val_accuracy: 0.5119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.7557\n",
      "Epoch 00073: loss improved from 0.47667 to 0.47008, saving model to weights_improvement_CNN_categorical_200_epochs_73-0.4701.hdf5\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4701 - accuracy: 0.7555 - val_loss: 0.7907 - val_accuracy: 0.5017\n",
      "Epoch 74/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.7616\n",
      "Epoch 00074: loss improved from 0.47008 to 0.46661, saving model to weights_improvement_CNN_categorical_200_epochs_74-0.4666.hdf5\n",
      "200/200 [==============================] - 59s 296ms/step - loss: 0.4666 - accuracy: 0.7613 - val_loss: 0.8138 - val_accuracy: 0.5039\n",
      "Epoch 75/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.7569\n",
      "Epoch 00075: loss did not improve from 0.46661\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 0.4689 - accuracy: 0.7569 - val_loss: 0.8224 - val_accuracy: 0.5097\n",
      "Epoch 76/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4618 - accuracy: 0.7634\n",
      "Epoch 00076: loss improved from 0.46661 to 0.46152, saving model to weights_improvement_CNN_categorical_200_epochs_76-0.4615.hdf5\n",
      "200/200 [==============================] - 56s 282ms/step - loss: 0.4615 - accuracy: 0.7636 - val_loss: 0.8129 - val_accuracy: 0.5084\n",
      "Epoch 77/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7634\n",
      "Epoch 00077: loss improved from 0.46152 to 0.45931, saving model to weights_improvement_CNN_categorical_200_epochs_77-0.4593.hdf5\n",
      "200/200 [==============================] - 56s 279ms/step - loss: 0.4593 - accuracy: 0.7634 - val_loss: 0.8062 - val_accuracy: 0.5159\n",
      "Epoch 78/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.7639\n",
      "Epoch 00078: loss did not improve from 0.45931\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4598 - accuracy: 0.7642 - val_loss: 0.8455 - val_accuracy: 0.5148\n",
      "Epoch 79/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4558 - accuracy: 0.7703\n",
      "Epoch 00079: loss improved from 0.45931 to 0.45575, saving model to weights_improvement_CNN_categorical_200_epochs_79-0.4558.hdf5\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.4558 - accuracy: 0.7702 - val_loss: 0.8260 - val_accuracy: 0.5067\n",
      "Epoch 80/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7677\n",
      "Epoch 00080: loss improved from 0.45575 to 0.45026, saving model to weights_improvement_CNN_categorical_200_epochs_80-0.4503.hdf5\n",
      "200/200 [==============================] - 60s 301ms/step - loss: 0.4503 - accuracy: 0.7675 - val_loss: 0.8271 - val_accuracy: 0.5042\n",
      "Epoch 81/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.7655\n",
      "Epoch 00081: loss did not improve from 0.45026\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.4566 - accuracy: 0.7654 - val_loss: 0.8446 - val_accuracy: 0.5098\n",
      "Epoch 82/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7688\n",
      "Epoch 00082: loss improved from 0.45026 to 0.44967, saving model to weights_improvement_CNN_categorical_200_epochs_82-0.4497.hdf5\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4497 - accuracy: 0.7688 - val_loss: 0.8677 - val_accuracy: 0.5023\n",
      "Epoch 83/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7730\n",
      "Epoch 00083: loss improved from 0.44967 to 0.44746, saving model to weights_improvement_CNN_categorical_200_epochs_83-0.4475.hdf5\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 0.4475 - accuracy: 0.7730 - val_loss: 0.8899 - val_accuracy: 0.5077\n",
      "Epoch 84/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7690\n",
      "Epoch 00084: loss did not improve from 0.44746\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4488 - accuracy: 0.7690 - val_loss: 0.8100 - val_accuracy: 0.5152\n",
      "Epoch 85/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4456 - accuracy: 0.7715\n",
      "Epoch 00085: loss improved from 0.44746 to 0.44646, saving model to weights_improvement_CNN_categorical_200_epochs_85-0.4465.hdf5\n",
      "200/200 [==============================] - 56s 282ms/step - loss: 0.4465 - accuracy: 0.7711 - val_loss: 0.9159 - val_accuracy: 0.5172\n",
      "Epoch 86/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.7831\n",
      "Epoch 00086: loss improved from 0.44646 to 0.43437, saving model to weights_improvement_CNN_categorical_200_epochs_86-0.4344.hdf5\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 0.4344 - accuracy: 0.7832 - val_loss: 0.8612 - val_accuracy: 0.5033\n",
      "Epoch 87/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.7789\n",
      "Epoch 00087: loss did not improve from 0.43437\n",
      "200/200 [==============================] - 54s 271ms/step - loss: 0.4353 - accuracy: 0.7792 - val_loss: 0.9149 - val_accuracy: 0.5091\n",
      "Epoch 88/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.7820\n",
      "Epoch 00088: loss improved from 0.43437 to 0.43224, saving model to weights_improvement_CNN_categorical_200_epochs_88-0.4322.hdf5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 0.4322 - accuracy: 0.7819 - val_loss: 0.8460 - val_accuracy: 0.5081\n",
      "Epoch 89/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7833\n",
      "Epoch 00089: loss did not improve from 0.43224\n",
      "200/200 [==============================] - 53s 267ms/step - loss: 0.4349 - accuracy: 0.7833 - val_loss: 0.8536 - val_accuracy: 0.5189\n",
      "Epoch 90/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4292 - accuracy: 0.7859\n",
      "Epoch 00090: loss improved from 0.43224 to 0.42896, saving model to weights_improvement_CNN_categorical_200_epochs_90-0.4290.hdf5\n",
      "200/200 [==============================] - 54s 271ms/step - loss: 0.4290 - accuracy: 0.7861 - val_loss: 0.8620 - val_accuracy: 0.5103\n",
      "Epoch 91/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.7863\n",
      "Epoch 00091: loss did not improve from 0.42896\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.4318 - accuracy: 0.7865 - val_loss: 0.9283 - val_accuracy: 0.5092\n",
      "Epoch 92/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.7855\n",
      "Epoch 00092: loss did not improve from 0.42896\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.4342 - accuracy: 0.7855 - val_loss: 0.9243 - val_accuracy: 0.5094\n",
      "Epoch 93/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.7883\n",
      "Epoch 00093: loss improved from 0.42896 to 0.42364, saving model to weights_improvement_CNN_categorical_200_epochs_93-0.4236.hdf5\n",
      "200/200 [==============================] - 54s 272ms/step - loss: 0.4236 - accuracy: 0.7883 - val_loss: 0.8834 - val_accuracy: 0.5067\n",
      "Epoch 94/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.7875\n",
      "Epoch 00094: loss improved from 0.42364 to 0.42267, saving model to weights_improvement_CNN_categorical_200_epochs_94-0.4227.hdf5\n",
      "200/200 [==============================] - 54s 270ms/step - loss: 0.4227 - accuracy: 0.7874 - val_loss: 0.8821 - val_accuracy: 0.5117\n",
      "Epoch 95/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4155 - accuracy: 0.7931\n",
      "Epoch 00095: loss improved from 0.42267 to 0.41559, saving model to weights_improvement_CNN_categorical_200_epochs_95-0.4156.hdf5\n",
      "200/200 [==============================] - 55s 274ms/step - loss: 0.4156 - accuracy: 0.7931 - val_loss: 0.8565 - val_accuracy: 0.5150\n",
      "Epoch 96/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.7936\n",
      "Epoch 00096: loss did not improve from 0.41559\n",
      "200/200 [==============================] - 55s 273ms/step - loss: 0.4160 - accuracy: 0.7937 - val_loss: 0.8760 - val_accuracy: 0.5134\n",
      "Epoch 97/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.7943\n",
      "Epoch 00097: loss improved from 0.41559 to 0.40990, saving model to weights_improvement_CNN_categorical_200_epochs_97-0.4099.hdf5\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 0.4099 - accuracy: 0.7944 - val_loss: 0.8710 - val_accuracy: 0.5131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.7944\n",
      "Epoch 00098: loss did not improve from 0.40990\n",
      "200/200 [==============================] - 54s 270ms/step - loss: 0.4120 - accuracy: 0.7947 - val_loss: 0.9547 - val_accuracy: 0.5156\n",
      "Epoch 99/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8008\n",
      "Epoch 00099: loss improved from 0.40990 to 0.40931, saving model to weights_improvement_CNN_categorical_200_epochs_99-0.4093.hdf5\n",
      "200/200 [==============================] - 54s 272ms/step - loss: 0.4093 - accuracy: 0.8011 - val_loss: 0.8567 - val_accuracy: 0.5188\n",
      "Epoch 100/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.7952\n",
      "Epoch 00100: loss did not improve from 0.40931\n",
      "200/200 [==============================] - 55s 274ms/step - loss: 0.4133 - accuracy: 0.7954 - val_loss: 0.8941 - val_accuracy: 0.5034\n",
      "Epoch 101/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.7962\n",
      "Epoch 00101: loss improved from 0.40931 to 0.40874, saving model to weights_improvement_CNN_categorical_200_epochs_101-0.4087.hdf5\n",
      "200/200 [==============================] - 54s 269ms/step - loss: 0.4087 - accuracy: 0.7963 - val_loss: 0.9405 - val_accuracy: 0.5183\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbde126fc50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = normelize_dataset.training.generator(shuffle=True)\n",
    "\n",
    "model.fit(\n",
    "    train_generator,epochs=200, steps_per_epoch=200,\n",
    "    validation_data=normelize_dataset.validation.generator(), validation_steps=50,\n",
    "    callbacks=get_callbacks(\"CNN_categorical_200_epochs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"weights_improvement_CNN_categorical_200_epochs_101-0.4087.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524/524 [==============================] - 45s 86ms/step - loss: 1.9145 - accuracy: 0.5081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9144682519215672, 0.5080749)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = normelize_dataset.validation.generator()[0]\n",
    "loss, accuracy = model.evaluate(normelize_dataset.validation.generator(), verbose=1)\n",
    "loss, accuracy\n",
    "# normelize_dataset.validation.generator()\n",
    "# model.evaluate(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6551429e-01, 8.3448571e-01],\n",
       "       [3.7632060e-01, 6.2367940e-01],\n",
       "       [3.6860329e-01, 6.3139671e-01],\n",
       "       ...,\n",
       "       [2.1844049e-19, 1.0000000e+00],\n",
       "       [1.5735119e-19, 1.0000000e+00],\n",
       "       [6.8533295e-18, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(normelize_dataset.validation.generator())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_normelized = []\n",
    "targets = []\n",
    "for batch_of_x, batch_of_y in normelize_dataset.validation.generator():\n",
    "    for x, y in zip(batch_of_x, batch_of_y):\n",
    "        last_price = x[-1][0]\n",
    "        prices_normelized.append(last_price)\n",
    "        targets.append(np.argmax(y, axis=-1))\n",
    "        \n",
    "prices_normelized = np.array(prices_normelized)\n",
    "targets = np.array(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prices = dataset.validation.X[:, 0][499:-1]\n",
    "original_target = np.argmax(normelize_dataset.validation.y[499:-1], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([154.118, 153.927, 154.017, ..., 219.328, 219.068, 218.958])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_dummy_volumes = normelize_dataset.x_scaler.inverse_transform(np.column_stack([prices_normelized, np.zeros(prices_normelized.shape[0])]))\n",
    "prices = prices_and_dummy_volumes[:, 0]\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>original_close</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>original_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.118</td>\n",
       "      <td>154.118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.927</td>\n",
       "      <td>153.927</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.017</td>\n",
       "      <td>154.017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153.968</td>\n",
       "      <td>153.968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153.978</td>\n",
       "      <td>153.978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66993</th>\n",
       "      <td>219.478</td>\n",
       "      <td>219.478</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66994</th>\n",
       "      <td>219.498</td>\n",
       "      <td>219.498</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66995</th>\n",
       "      <td>219.328</td>\n",
       "      <td>219.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66996</th>\n",
       "      <td>219.068</td>\n",
       "      <td>219.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66997</th>\n",
       "      <td>218.958</td>\n",
       "      <td>218.958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66998 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         close  original_close  prediction  target  original_target\n",
       "0      154.118         154.118           1       0                0\n",
       "1      153.927         153.927           1       0                0\n",
       "2      154.017         154.017           1       0                0\n",
       "3      153.968         153.968           1       0                0\n",
       "4      153.978         153.978           1       0                0\n",
       "...        ...             ...         ...     ...              ...\n",
       "66993  219.478         219.478           1       0                0\n",
       "66994  219.498         219.498           1       0                0\n",
       "66995  219.328         219.328           1       0                0\n",
       "66996  219.068         219.068           1       0                0\n",
       "66997  218.958         218.958           1       0                0\n",
       "\n",
       "[66998 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_predictions = pd.DataFrame(\n",
    "    {'close': prices,\n",
    "     'original_close': original_prices,\n",
    "     'prediction': np.argmax(predictions, axis=-1),\n",
    "     'target': targets,\n",
    "     'original_target': original_target\n",
    "    })\n",
    "prices_and_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>original_close</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>original_target</th>\n",
       "      <th>target_calculated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.118</td>\n",
       "      <td>154.118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.927</td>\n",
       "      <td>153.927</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.017</td>\n",
       "      <td>154.017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153.968</td>\n",
       "      <td>153.968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153.978</td>\n",
       "      <td>153.978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66993</th>\n",
       "      <td>219.478</td>\n",
       "      <td>219.478</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66994</th>\n",
       "      <td>219.498</td>\n",
       "      <td>219.498</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66995</th>\n",
       "      <td>219.328</td>\n",
       "      <td>219.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66996</th>\n",
       "      <td>219.068</td>\n",
       "      <td>219.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66997</th>\n",
       "      <td>218.958</td>\n",
       "      <td>218.958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66998 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         close  original_close  prediction  target  original_target  \\\n",
       "0      154.118         154.118           1       0                0   \n",
       "1      153.927         153.927           1       0                0   \n",
       "2      154.017         154.017           1       0                0   \n",
       "3      153.968         153.968           1       0                0   \n",
       "4      153.978         153.978           1       0                0   \n",
       "...        ...             ...         ...     ...              ...   \n",
       "66993  219.478         219.478           1       0                0   \n",
       "66994  219.498         219.498           1       0                0   \n",
       "66995  219.328         219.328           1       0                0   \n",
       "66996  219.068         219.068           1       0                0   \n",
       "66997  218.958         218.958           1       0                0   \n",
       "\n",
       "       target_calculated  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "...                  ...  \n",
       "66993                  0  \n",
       "66994                  0  \n",
       "66995                  0  \n",
       "66996                  0  \n",
       "66997                  0  \n",
       "\n",
       "[66998 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_predictions['target_calculated'] = np.where(prices_and_predictions['close'] < prices_and_predictions['close'].shift(-50), 1, 0)\n",
    "prices_and_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>future_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.927</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153.968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153.978</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66993</th>\n",
       "      <td>219.478</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66994</th>\n",
       "      <td>219.498</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66995</th>\n",
       "      <td>219.328</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66996</th>\n",
       "      <td>219.068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66997</th>\n",
       "      <td>218.958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         close  target  prediction  future_price\n",
       "0      154.118       0           1       152.798\n",
       "1      153.927       0           1       152.727\n",
       "2      154.017       0           1       152.738\n",
       "3      153.968       0           1       152.708\n",
       "4      153.978       0           1       152.848\n",
       "...        ...     ...         ...           ...\n",
       "66993  219.478       0           1           NaN\n",
       "66994  219.498       0           1           NaN\n",
       "66995  219.328       0           1           NaN\n",
       "66996  219.068       0           1           NaN\n",
       "66997  218.958       0           1           NaN\n",
       "\n",
       "[66998 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_predictions['future_price'] = prices_and_predictions['close'].shift(-50)\n",
    "prices_and_predictions[['close', 'target', 'prediction', 'future_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alon/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>original_close</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>original_target</th>\n",
       "      <th>target_calculated</th>\n",
       "      <th>future_price</th>\n",
       "      <th>tmp</th>\n",
       "      <th>roc</th>\n",
       "      <th>pnl</th>\n",
       "      <th>true_pnl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.118</td>\n",
       "      <td>154.118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.798</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.008565</td>\n",
       "      <td>-0.008565</td>\n",
       "      <td>0.008565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.927</td>\n",
       "      <td>153.927</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.727</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.007796</td>\n",
       "      <td>-0.007796</td>\n",
       "      <td>0.007796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.017</td>\n",
       "      <td>154.017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.738</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>0.008304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153.968</td>\n",
       "      <td>153.968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.708</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.008184</td>\n",
       "      <td>-0.008184</td>\n",
       "      <td>0.008184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153.978</td>\n",
       "      <td>153.978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.848</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.007339</td>\n",
       "      <td>-0.007339</td>\n",
       "      <td>0.007339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154.018</td>\n",
       "      <td>154.018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.828</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.007726</td>\n",
       "      <td>-0.007726</td>\n",
       "      <td>0.007726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>153.818</td>\n",
       "      <td>153.818</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.878</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.006111</td>\n",
       "      <td>-0.006111</td>\n",
       "      <td>0.006111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153.707</td>\n",
       "      <td>153.707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.857</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005530</td>\n",
       "      <td>-0.005530</td>\n",
       "      <td>0.005530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153.818</td>\n",
       "      <td>153.818</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.847</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.006313</td>\n",
       "      <td>-0.006313</td>\n",
       "      <td>0.006313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>153.867</td>\n",
       "      <td>153.867</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.817</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>153.837</td>\n",
       "      <td>153.837</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.927</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005915</td>\n",
       "      <td>-0.005915</td>\n",
       "      <td>0.005915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>153.788</td>\n",
       "      <td>153.788</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153.088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004552</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.004552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>153.738</td>\n",
       "      <td>153.738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>153.698</td>\n",
       "      <td>153.698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005530</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.005530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>153.597</td>\n",
       "      <td>153.597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.005208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>153.607</td>\n",
       "      <td>153.607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.005267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>153.497</td>\n",
       "      <td>153.497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.003844</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.003844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>153.508</td>\n",
       "      <td>153.508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004625</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.004625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>153.518</td>\n",
       "      <td>153.518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004690</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.004690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>153.498</td>\n",
       "      <td>153.498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.707</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.005153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>153.447</td>\n",
       "      <td>153.447</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004301</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>153.568</td>\n",
       "      <td>153.568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005672</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.005672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>153.637</td>\n",
       "      <td>153.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.006437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>153.607</td>\n",
       "      <td>153.607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007031</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.007031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>153.508</td>\n",
       "      <td>153.508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006065</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.006065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>153.637</td>\n",
       "      <td>153.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006834</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.006834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>153.648</td>\n",
       "      <td>153.648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>153.537</td>\n",
       "      <td>153.537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007620</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.007620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>153.518</td>\n",
       "      <td>153.518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.007042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>153.327</td>\n",
       "      <td>153.327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      close  original_close  prediction  target  original_target  \\\n",
       "0   154.118         154.118           1       0                0   \n",
       "1   153.927         153.927           1       0                0   \n",
       "2   154.017         154.017           1       0                0   \n",
       "3   153.968         153.968           1       0                0   \n",
       "4   153.978         153.978           1       0                0   \n",
       "5   154.018         154.018           1       0                0   \n",
       "6   153.818         153.818           1       0                0   \n",
       "7   153.707         153.707           1       0                0   \n",
       "8   153.818         153.818           1       0                0   \n",
       "9   153.867         153.867           1       0                0   \n",
       "10  153.837         153.837           1       0                0   \n",
       "11  153.788         153.788           0       0                0   \n",
       "12  153.738         153.738           0       0                0   \n",
       "13  153.698         153.698           0       0                0   \n",
       "14  153.597         153.597           0       0                0   \n",
       "15  153.607         153.607           0       0                0   \n",
       "16  153.497         153.497           0       0                0   \n",
       "17  153.508         153.508           0       0                0   \n",
       "18  153.518         153.518           0       0                0   \n",
       "19  153.498         153.498           0       0                0   \n",
       "20  153.447         153.447           0       0                0   \n",
       "21  153.568         153.568           0       0                0   \n",
       "22  153.637         153.637           0       0                0   \n",
       "23  153.607         153.607           0       0                0   \n",
       "24  153.508         153.508           0       0                0   \n",
       "25  153.637         153.637           0       0                0   \n",
       "26  153.648         153.648           0       0                0   \n",
       "27  153.537         153.537           0       0                0   \n",
       "28  153.518         153.518           0       0                0   \n",
       "29  153.327         153.327           0       0                0   \n",
       "\n",
       "    target_calculated  future_price  tmp       roc       pnl  true_pnl  \n",
       "0                   0       152.798 -1.0 -0.008565 -0.008565  0.008565  \n",
       "1                   0       152.727 -1.0 -0.007796 -0.007796  0.007796  \n",
       "2                   0       152.738 -1.0 -0.008304 -0.008304  0.008304  \n",
       "3                   0       152.708 -1.0 -0.008184 -0.008184  0.008184  \n",
       "4                   0       152.848 -1.0 -0.007339 -0.007339  0.007339  \n",
       "5                   0       152.828 -1.0 -0.007726 -0.007726  0.007726  \n",
       "6                   0       152.878 -1.0 -0.006111 -0.006111  0.006111  \n",
       "7                   0       152.857 -1.0 -0.005530 -0.005530  0.005530  \n",
       "8                   0       152.847 -1.0 -0.006313 -0.006313  0.006313  \n",
       "9                   0       152.817 -1.0 -0.006824 -0.006824  0.006824  \n",
       "10                  0       152.927 -1.0 -0.005915 -0.005915  0.005915  \n",
       "11                  0       153.088  1.0 -0.004552  0.004552  0.004552  \n",
       "12                  0       152.977  1.0 -0.004950  0.004950  0.004950  \n",
       "13                  0       152.848  1.0 -0.005530  0.005530  0.005530  \n",
       "14                  0       152.797  1.0 -0.005208  0.005208  0.005208  \n",
       "15                  0       152.798  1.0 -0.005267  0.005267  0.005267  \n",
       "16                  0       152.907  1.0 -0.003844  0.003844  0.003844  \n",
       "17                  0       152.798  1.0 -0.004625  0.004625  0.004625  \n",
       "18                  0       152.798  1.0 -0.004690  0.004690  0.004690  \n",
       "19                  0       152.707  1.0 -0.005153  0.005153  0.005153  \n",
       "20                  0       152.787  1.0 -0.004301  0.004301  0.004301  \n",
       "21                  0       152.697  1.0 -0.005672  0.005672  0.005672  \n",
       "22                  0       152.648  1.0 -0.006437  0.006437  0.006437  \n",
       "23                  0       152.527  1.0 -0.007031  0.007031  0.007031  \n",
       "24                  0       152.577  1.0 -0.006065  0.006065  0.006065  \n",
       "25                  0       152.587  1.0 -0.006834  0.006834  0.006834  \n",
       "26                  0       152.438  1.0 -0.007875  0.007875  0.007875  \n",
       "27                  0       152.367  1.0 -0.007620  0.007620  0.007620  \n",
       "28                  0       152.437  1.0 -0.007042  0.007042  0.007042  \n",
       "29                  0       152.298  1.0 -0.006711  0.006711  0.006711  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_predictions['tmp'] = prices_and_predictions['target'] == prices_and_predictions['prediction']\n",
    "prices_and_predictions['roc'] = prices_and_predictions['future_price'] / prices_and_predictions['close'] - 1\n",
    "prices_and_predictions['tmp'][prices_and_predictions['tmp']==0] = -1\n",
    "\n",
    "prices_and_predictions['pnl'] = prices_and_predictions['tmp'] * prices_and_predictions['roc'].abs()\n",
    "\n",
    "\n",
    "prices_and_predictions['true_pnl'] = prices_and_predictions['pnl'].abs()\n",
    "prices_and_predictions.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pnl</th>\n",
       "      <th>true_pnl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66948.000000</td>\n",
       "      <td>66948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.003401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.005131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.056876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001901</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.003920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.060842</td>\n",
       "      <td>0.060842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pnl      true_pnl\n",
       "count  66948.000000  66948.000000\n",
       "mean       0.000041      0.003401\n",
       "std        0.006155      0.005131\n",
       "min       -0.056876      0.000000\n",
       "25%       -0.001901      0.000795\n",
       "50%        0.000000      0.001947\n",
       "75%        0.001993      0.003920\n",
       "max        0.060842      0.060842"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_and_predictions[['pnl', 'true_pnl']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "alon_second_attempt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
