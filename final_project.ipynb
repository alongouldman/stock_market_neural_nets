{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Stock-Price-Prediction-Based-On-Previous-Prices\" data-toc-modified-id=\"Stock-Price-Prediction-Based-On-Previous-Prices-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Stock Price Prediction Based On Previous Prices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Project-Proposal\" data-toc-modified-id=\"Project-Proposal-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Project Proposal</a></span></li></ul></li><li><span><a href=\"#Goal\" data-toc-modified-id=\"Goal-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Goal</a></span></li><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#IEX-stock-market-data\" data-toc-modified-id=\"IEX-stock-market-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>IEX stock market data</a></span><ul class=\"toc-item\"><li><span><a href=\"#scraping\" data-toc-modified-id=\"scraping-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>scraping</a></span></li></ul></li><li><span><a href=\"#Ducascopy-stock-market-data\" data-toc-modified-id=\"Ducascopy-stock-market-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Ducascopy stock market data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scraping\" data-toc-modified-id=\"Scraping-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Scraping</a></span></li></ul></li></ul></li><li><span><a href=\"#Some-imports....\" data-toc-modified-id=\"Some-imports....-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Some imports....</a></span></li><li><span><a href=\"#Data-preperation\" data-toc-modified-id=\"Data-preperation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data preperation</a></span></li><li><span><a href=\"#Split-the-data\" data-toc-modified-id=\"Split-the-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Split the data</a></span></li><li><span><a href=\"#Evaluating-models\" data-toc-modified-id=\"Evaluating-models-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluating models</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-matrics-we-used\" data-toc-modified-id=\"The-matrics-we-used-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>The matrics we used</a></span><ul class=\"toc-item\"><li><span><a href=\"#MSE\" data-toc-modified-id=\"MSE-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>MSE</a></span></li><li><span><a href=\"#&quot;Pnl&quot;---Profit-n'-Loss\" data-toc-modified-id=\"&quot;Pnl&quot;---Profit-n'-Loss-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>\"Pnl\" - Profit n' Loss</a></span></li><li><span><a href=\"#Maximum-win-&amp;-Maximum-loss\" data-toc-modified-id=\"Maximum-win-&amp;-Maximum-loss-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>Maximum win &amp; Maximum loss</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-8.1.4\"><span class=\"toc-item-num\">8.1.4&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#UP-&amp;-Down-Ratio\" data-toc-modified-id=\"UP-&amp;-Down-Ratio-8.1.5\"><span class=\"toc-item-num\">8.1.5&nbsp;&nbsp;</span>UP &amp; Down Ratio</a></span></li><li><span><a href=\"#True/False-ups-&amp;-True/False-Downs\" data-toc-modified-id=\"True/False-ups-&amp;-True/False-Downs-8.1.6\"><span class=\"toc-item-num\">8.1.6&nbsp;&nbsp;</span>True/False ups &amp; True/False Downs</a></span></li></ul></li></ul></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#1)-The-&quot;classic&quot;-approch---using-Tecnical-Indicators\" data-toc-modified-id=\"1)-The-&quot;classic&quot;-approch---using-Tecnical-Indicators-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>1) The \"classic\" approch - using Tecnical Indicators</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-the-results\" data-toc-modified-id=\"Evaluating-the-results-9.1.1\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Evaluating the results</a></span></li></ul></li><li><span><a href=\"#2)-rolling-windows\" data-toc-modified-id=\"2)-rolling-windows-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>2) rolling windows</a></span><ul class=\"toc-item\"><li><span><a href=\"#read-data-+-preprocessing\" data-toc-modified-id=\"read-data-+-preprocessing-9.2.1\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>read data + preprocessing</a></span></li><li><span><a href=\"#2.1)-&quot;Linear&quot;-rolling-windows\" data-toc-modified-id=\"2.1)-&quot;Linear&quot;-rolling-windows-9.2.2\"><span class=\"toc-item-num\">9.2.2&nbsp;&nbsp;</span>2.1) \"Linear\" rolling windows</a></span></li><li><span><a href=\"#2.2)-&quot;Exponential&quot;-rolling-windows\" data-toc-modified-id=\"2.2)-&quot;Exponential&quot;-rolling-windows-9.2.3\"><span class=\"toc-item-num\">9.2.3&nbsp;&nbsp;</span>2.2) \"Exponential\" rolling windows</a></span></li><li><span><a href=\"#2.3)-&quot;Linear&quot;-rolling-windows---no-open-/-high-/-low-prices\" data-toc-modified-id=\"2.3)-&quot;Linear&quot;-rolling-windows---no-open-/-high-/-low-prices-9.2.4\"><span class=\"toc-item-num\">9.2.4&nbsp;&nbsp;</span>2.3) \"Linear\" rolling windows - no open / high / low prices</a></span></li><li><span><a href=\"#2.4)-&quot;Linear&quot;-rolling-windows---log-returns\" data-toc-modified-id=\"2.4)-&quot;Linear&quot;-rolling-windows---log-returns-9.2.5\"><span class=\"toc-item-num\">9.2.5&nbsp;&nbsp;</span>2.4) \"Linear\" rolling windows - log returns</a></span></li><li><span><a href=\"#2.5)-&quot;Linear&quot;-rolling-windows---time-data\" data-toc-modified-id=\"2.5)-&quot;Linear&quot;-rolling-windows---time-data-9.2.6\"><span class=\"toc-item-num\">9.2.6&nbsp;&nbsp;</span>2.5) \"Linear\" rolling windows - time data</a></span></li><li><span><a href=\"#2.6)-&quot;Linear&quot;-rolling-windows---different-window-size\" data-toc-modified-id=\"2.6)-&quot;Linear&quot;-rolling-windows---different-window-size-9.2.7\"><span class=\"toc-item-num\">9.2.7&nbsp;&nbsp;</span>2.6) \"Linear\" rolling windows - different window size</a></span></li></ul></li><li><span><a href=\"#3)-different-models\" data-toc-modified-id=\"3)-different-models-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>3) different models</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1)-LGBM/GBM\" data-toc-modified-id=\"3.1)-LGBM/GBM-9.3.1\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>3.1) LGBM/GBM</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1.1)-Simple-linear-rolling-windows-of-size-10\" data-toc-modified-id=\"3.1.1)-Simple-linear-rolling-windows-of-size-10-9.3.1.1\"><span class=\"toc-item-num\">9.3.1.1&nbsp;&nbsp;</span>3.1.1) Simple linear rolling windows of size 10</a></span></li><li><span><a href=\"#3.1.2)-Simple-linear-rolling-windows-of-size-100\" data-toc-modified-id=\"3.1.2)-Simple-linear-rolling-windows-of-size-100-9.3.1.2\"><span class=\"toc-item-num\">9.3.1.2&nbsp;&nbsp;</span>3.1.2) Simple linear rolling windows of size 100</a></span></li></ul></li><li><span><a href=\"#3.2)-Multi-Layer-Preceptron\" data-toc-modified-id=\"3.2)-Multi-Layer-Preceptron-9.3.2\"><span class=\"toc-item-num\">9.3.2&nbsp;&nbsp;</span>3.2) Multi Layer Preceptron</a></span></li><li><span><a href=\"#3.3)-LSTM\" data-toc-modified-id=\"3.3)-LSTM-9.3.3\"><span class=\"toc-item-num\">9.3.3&nbsp;&nbsp;</span>3.3) LSTM</a></span></li></ul></li><li><span><a href=\"#4)-different-models---more-stocks\" data-toc-modified-id=\"4)-different-models---more-stocks-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>4) different models - more stocks</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1)-all-stocks-into-one-model\" data-toc-modified-id=\"4.1)-all-stocks-into-one-model-9.4.1\"><span class=\"toc-item-num\">9.4.1&nbsp;&nbsp;</span>4.1) all stocks into one model</a></span></li><li><span><a href=\"#4.2)-model-for-each-stock---LGBM\" data-toc-modified-id=\"4.2)-model-for-each-stock---LGBM-9.4.2\"><span class=\"toc-item-num\">9.4.2&nbsp;&nbsp;</span>4.2) model for each stock - LGBM</a></span></li></ul></li></ul></li><li><span><a href=\"#Comparing-the-results\" data-toc-modified-id=\"Comparing-the-results-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Comparing the results</a></span></li><li><span><a href=\"#Testing-how-our-model-did-on-the-test-set\" data-toc-modified-id=\"Testing-how-our-model-did-on-the-test-set-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Testing how our model did on the test set</a></span><ul class=\"toc-item\"><li><span><a href=\"#rf_liniar_21-model-VS-test-set\" data-toc-modified-id=\"rf_liniar_21-model-VS-test-set-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>rf_liniar_21 model VS test set</a></span></li><li><span><a href=\"#rf_log_returns_13-model-VS-test-set\" data-toc-modified-id=\"rf_log_returns_13-model-VS-test-set-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>rf_log_returns_13 model VS test set</a></span></li><li><span><a href=\"#stocks_lgbm_predict_spy_38-model-VS-test-set\" data-toc-modified-id=\"stocks_lgbm_predict_spy_38-model-VS-test-set-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>stocks_lgbm_predict_spy_38 model VS test set</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Things-we-have-learnt\" data-toc-modified-id=\"Things-we-have-learnt-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Things we have learnt</a></span></li><li><span><a href=\"#Things-we-could-have-done-better:\" data-toc-modified-id=\"Things-we-could-have-done-better:-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Things we could have done better:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5LZAkVye4O96"
   },
   "source": [
    "# Stock Price Prediction Based On Previous Prices\n",
    "## Project Proposal\n",
    "\n",
    "Project by:  \n",
    "\n",
    "Asaf Alina  \n",
    "Alon Gouldman\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rq4eSjFG4O97"
   },
   "source": [
    "# Goal\n",
    "\n",
    "Our goal is to maximize the profits when simulating trading on the Standard & Poor's 500 ETF Trust security (`SPY`).  \n",
    "We chose this security because: \n",
    "1. It is the most popular ETF in the US stock market.\n",
    "2. It represents the biggest 500 stocks in US stock market.\n",
    "\n",
    "On every timestamp, we wanted to predict the return between the currant price - and the price in the next 10 minutes.  \n",
    "We could think of different approaches on predicting future returns, but to keep things simple, we will stick with predicting the future 10 minute return only.  \n",
    "For example, if the price of a stock now is 10 dollar, and the price in 10 minuets will be 12 dollar - the return between the prices is 0.2% - and this is the target we want our model to predict. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h57yQsDq4O98"
   },
   "source": [
    "# Motivation\n",
    "\n",
    "The motivation behind being able to predict the prices of the stock market is clear -  \n",
    "knowing future market returns, would allow us as investors to trade the stocks and earn profits.  \n",
    "\n",
    "This simple goal is, in fact, one of the toughest problems in history.  \n",
    "Great minds and a lot of money has been spilled trying to solve this problem -  \n",
    "since the ability to forecast future market movement means a great financial benefit for the one who acquire it.  \n",
    "\n",
    "Once a good trading strategy is known to the public -  \n",
    "other market participents exploit it, and the edge of the strategy tends to fade away.  \n",
    "Therefore, market participents have no interest in sharing knowledge,  \n",
    "which leads to little valuable research on market forcasting.\n",
    "\n",
    "\n",
    "The dynamic nature of the stock market - the changing correlations and non-linearity -  \n",
    "make some researches believe in the \"random walk theory\" - which state that stock prices move randomly.  \n",
    "Even if the stock market is not totally random, it is affected by many factors, such as global\n",
    "economy, politics, investor expectation, etc -  \n",
    "which make future movements very hard to forecast.\n",
    "\n",
    "\n",
    "That being said, we have tried our luck into solving this problem.  \n",
    "We believe we have a chance to succeed, because of the following factors: \n",
    "1. The data about the stocks is availiable - but unfortunately it is not so easy to put ones hands on good quality data.  \n",
    "We have had a hard time getting large amounts of data which will be elaborated on later,  \n",
    "therefore we think that when raising the entry barrier for those requesting data - we face less competitors. \n",
    "2. We assume that even if the data looks random - it is not random at all.  \n",
    "The price of a stock is an aggregated mix of what every investor in the world is pricing a certain stock at that specific moment.  \n",
    "Humans are not particularly rational - but they do not act randomly.  \n",
    " In that case, it make sense to us that there could be a way to predict the way people could potentially behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrfeKhXJ4O98"
   },
   "source": [
    "# Data\n",
    "A simple Google search could usually find the answer to most problems.  \n",
    "Not in this case though - going through a lot of websites and search results,  \n",
    "we could not find a good source of data to test our models against.  \n",
    "Generally speaking, there are 2 types of stock's historical prices:\n",
    "1. daily data - which samples stock's price by daily basis.\n",
    "2. intraday data - which samples stock's price in smaller than daily interval, usually every hour / minute / second.\n",
    "\n",
    "Clean, ordered daily data is relatively easy to find.  \n",
    "However, good quality intraday data is hard to get, and is almost always behind paywall.\n",
    "\n",
    "After alot of searching, we managed to put our hands on two valuable data sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBb7Ou3uAHVE"
   },
   "source": [
    "## IEX stock market data\n",
    "[IEX website](https://iexcloud.io/) has an almost free API of stocks' historical prices.\n",
    "\n",
    "There is a limit on how far back in history we could get data for,  \n",
    "however, free, minute by minute historical prices data for approximately 8700 different stocks - is very rare to find.\n",
    "\n",
    "### scraping\n",
    "We built a web scraper with [Scrapy](https://scrapy.org/) - a python library for web scraping -  \n",
    "in order to get all stocks' data, and save it into different files, based on stock's ticker.  \n",
    "We managed to get a minute by minute historical price data, from 1/4/19 to 06/05/2020 -  \n",
    "for total of 8,737 different stocks, which accumulates to 35.03 GB.\n",
    "\n",
    "<br/>\n",
    "<img src=\"dir_IEX.jpg\" alt=\"drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVKU_Yxx4O99"
   },
   "source": [
    "Lets take a glimpse at our data,  \n",
    "and look at the data of a single ticker - Apple inc's stock - AAPL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luIkXMUB4O9-"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def minimal_IEX_preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" get raw IEX-OHLC dataframe, make minimal preprocessing on it and remove redundant columns \"\"\"\n",
    "\n",
    "    # make 1 datetime column\n",
    "    df['datetime'] = df['date'] + \",\" + df['minute']\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d,%H:%M')\n",
    "    del df['date']\n",
    "    del df['minute']\n",
    "    del df['label']\n",
    "\n",
    "    # sort data by from old datetime to new datetime\n",
    "    df.sort_values(by=['datetime']).reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_stock_data(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    optional_file_paths = ['data/relevant/snp500_from_iex', 'data/relevant/iex_data']\n",
    "    for path in optional_file_paths:\n",
    "        file_path = Path(path) / f\"{ticker}.csv\"\n",
    "        if file_path.exists():\n",
    "            return pd.read_csv(file_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "0TV6k8jh4O-C",
    "outputId": "25a7564f-8be9-45aa-8fab-529c4642a8ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>average</th>\n",
       "      <th>volume</th>\n",
       "      <th>notional</th>\n",
       "      <th>numberOfTrades</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.385</td>\n",
       "      <td>199.69</td>\n",
       "      <td>200.33</td>\n",
       "      <td>200.320</td>\n",
       "      <td>200.100</td>\n",
       "      <td>20793.0</td>\n",
       "      <td>4160671.710</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2019-04-09 09:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.880</td>\n",
       "      <td>200.33</td>\n",
       "      <td>200.48</td>\n",
       "      <td>200.690</td>\n",
       "      <td>200.643</td>\n",
       "      <td>17520.0</td>\n",
       "      <td>3515273.345</td>\n",
       "      <td>139.0</td>\n",
       "      <td>2019-04-09 09:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201.090</td>\n",
       "      <td>200.59</td>\n",
       "      <td>200.72</td>\n",
       "      <td>200.870</td>\n",
       "      <td>200.794</td>\n",
       "      <td>16127.0</td>\n",
       "      <td>3238210.120</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2019-04-09 09:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201.030</td>\n",
       "      <td>200.70</td>\n",
       "      <td>200.91</td>\n",
       "      <td>200.990</td>\n",
       "      <td>200.846</td>\n",
       "      <td>6030.0</td>\n",
       "      <td>1211102.170</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2019-04-09 09:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201.250</td>\n",
       "      <td>200.97</td>\n",
       "      <td>200.98</td>\n",
       "      <td>201.155</td>\n",
       "      <td>201.095</td>\n",
       "      <td>6060.0</td>\n",
       "      <td>1218639.150</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2019-04-09 09:34:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      high     low    open    close  average   volume     notional  \\\n",
       "0  200.385  199.69  200.33  200.320  200.100  20793.0  4160671.710   \n",
       "1  200.880  200.33  200.48  200.690  200.643  17520.0  3515273.345   \n",
       "2  201.090  200.59  200.72  200.870  200.794  16127.0  3238210.120   \n",
       "3  201.030  200.70  200.91  200.990  200.846   6030.0  1211102.170   \n",
       "4  201.250  200.97  200.98  201.155  201.095   6060.0  1218639.150   \n",
       "\n",
       "   numberOfTrades            datetime  \n",
       "0           131.0 2019-04-09 09:30:00  \n",
       "1           139.0 2019-04-09 09:31:00  \n",
       "2           114.0 2019-04-09 09:32:00  \n",
       "3            50.0 2019-04-09 09:33:00  \n",
       "4            64.0 2019-04-09 09:34:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_data = get_stock_data(\"AAPL\")\n",
    "minimal_IEX_preprocessing(apple_data)\n",
    "apple_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UowF30W54O-G"
   },
   "source": [
    " \n",
    "This is the data we managed to get for free.  \n",
    "The prices (minutely open, high, low & close) were clearly important for us.  \n",
    "But also maybe the the `number of trades` and `volume` could have been useful. \n",
    "\n",
    "This is how the stock prices looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mW5mvDSt4O-H"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_ohlc_graph_from_dataframe(datafram: pd.DataFrame):\n",
    "    fig = go.Figure(data=go.Ohlc(x=datafram['datetime'],\n",
    "                        open=datafram['open'],\n",
    "                        high=datafram['high'],\n",
    "                        low=datafram['low'],\n",
    "                        close=datafram['close']))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# plot_ohlc_graph_from_dataframe(apple_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul_VsU7B4O-K"
   },
   "source": [
    "![alt text](AAPL_candlestick.jpg)\n",
    "\n",
    "\n",
    "(we used image of the OHLC graph, as it is quite heavy, and it slows down the jupyter notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1c8bz8T4O-K"
   },
   "source": [
    "The second  website we found was a bit more challenging - but also more promising. \n",
    "## Ducascopy stock market data\n",
    "[Ducascopy](https://www.dukascopy.com/swiss/english/home/) is a swiss bank for trading all kinds of equities.  \n",
    "It has 3 years of minute by minute bid and ask data, including stocks, bonds, commodities, forex and more.  \n",
    "Downloading their data manually was straight forward, but to make it automatic was more challenging -  \n",
    "they did not have a free API for individuals, and the network calls made when downloading seemed to return an encrypted data.  \n",
    "Not only that, every so often there was a login pop-up which required entering an email and password to continue downloading.\n",
    "\n",
    "### Scraping\n",
    "We used [selenium](https://selenium-python.readthedocs.io/) library to imitate a user browsing into Ducascopy website.  \n",
    "This user imitation had clear disadvantages - it was very slow and prone to unexpected bugs.  \n",
    "We implimented a \"retrieve data\" logic to bypass unexpected bugs during downloading.  \n",
    "To deal with the speed of downloading, we tried to raise some instances of the scraper in parallel.  \n",
    "This simple step failed, as it seemed that Dukascopy website detected such parallel downloads.  \n",
    "We overcame this issue using Opera web browser, which had a built-in VPN that we manually turned on for each scraper instance.  \n",
    "In addition, we used multiple accounts when the login window popped up,  \n",
    "so our downloads were better disguised (though we were never blocked).\n",
    "\n",
    "After a couple of nights of running the scraper - we finally had a large amount of data we could work with.  \n",
    "We managed to get bid data of 654 financial instruments, going from 26/1/2017 to 1/5/2020 - a total of 65 GB.  \n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"dir_dukascopy.jpg\" alt=\"drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kYfn9B44O-L"
   },
   "source": [
    "Lets take at Apple's stock, from this data source: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qg9DOtwK4O-L"
   },
   "outputs": [],
   "source": [
    "def read_dukas(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # rename columns\n",
    "    new_cols_map = {col: col.lower() for col in df}\n",
    "    new_cols_map['Local time'] = \"datetime\"\n",
    "    df.rename(columns=new_cols_map, inplace=True)\n",
    "\n",
    "    # convert datetime type (string to datetime)\n",
    "    df['datetime'] = df['datetime'].str.replace(r\"\\:00\\.000 GMT\\+0\\d00\",\"\")\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d.%m.%Y %H:%M')\n",
    "    \n",
    "    # sort by datetime\n",
    "    df.sort_values(by=['datetime'], inplace=True)\n",
    "    \n",
    "    # reset index, more convenient\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "qmnpTeAi4O-O",
    "outputId": "4cffd0c9-c645-4c55-8ac0-60ddfee9e66f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe2ede2c910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aapl_csv_path = r'data/relevant/dukascopy/BID/AAPL.USUSD.csv'\n",
    "aapl_df = read_dukas(aapl_csv_path)\n",
    "\n",
    "aapl_df['close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bG1lsCZ_vAh"
   },
   "source": [
    "# Some imports...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "50zuXrGf_10N",
    "outputId": "697e5c12-03ab-4fa0-c854-94d749d33db1"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from joblib import load, dump\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FMWhNgjCQIX"
   },
   "outputs": [],
   "source": [
    "# consts\n",
    "STOCK = 'SPY'\n",
    "\n",
    "SPLIT_DATE1 = \"03-01-2019\"\n",
    "SPLIT_DATE2 = \"12-01-2019\"\n",
    "\n",
    "RAW_DATA_LOCATION = 'data/relevant/dukascopy/BID'\n",
    "\n",
    "data_dir = 'data_feather/dukas_feather/'\n",
    "save_dir = \"saved_models/\" \n",
    "\n",
    "MODEL_RESULTS_CSV = \"models_evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lntadGP8__sG"
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def dir_files(dir_path):\n",
    "    return [os.path.join(dir_path,f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "\n",
    "def read_data(data_dir, ticker):\n",
    "    for file in dir_files(data_dir):\n",
    "        file_ticker_name = os.path.basename(file).split('.')[0]\n",
    "        if file_ticker_name == ticker:\n",
    "            return pd.read_feather(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUFd2Iuy6pby"
   },
   "source": [
    "# Data preperation\n",
    "Below are initial preperation / cleaning procedures we did on the original Dukascopy data:\n",
    "1. Timestamps format: to fit into the pandas built-in datetime type.\n",
    "2. Drop duplicate data: caused by summer/winter saving time.\n",
    "3. Drop inactive hours: such as saturdays, sundays, midnight hours, etc...\n",
    "4. Sort by datetime: convinient when dealing with time series data.\n",
    "5. Save in feather format: better memory usage, and faster loading (compare to csv).\n",
    "\n",
    "The data we obtained from Dukascopy has alot of US stocks data,  \n",
    "as well as forex, bonds, and commodities data, and other major foreign markets data.  \n",
    "Its important to note that many of these securities are trading in different times,  \n",
    "and therefore, removing data, say at midnight, for one security,  \n",
    "might be an actual active trading hours at another security.\n",
    "\n",
    "Therefore, to simplify things up,  \n",
    "we discraded any security that was trading (had volume more than 0) during 23:00 - 15:30, Israel time,  \n",
    "which basically leave us with many, but only,  US stocks securities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F8pTVSelfI0"
   },
   "outputs": [],
   "source": [
    "csv_files = dir_files(RAW_DATA_LOCATION)\n",
    "\n",
    "for file in csv_files:\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # rename columns\n",
    "    new_cols_map = {col: col.lower() for col in df}\n",
    "    new_cols_map['Local time'] = \"datetime\"\n",
    "    df.rename(columns=new_cols_map, inplace=True)\n",
    "\n",
    "    # convert to datetime\n",
    "    df['datetime'] = df['datetime'].str.replace(r\"\\:00\\.000 GMT\\+0\\d00\",\"\")\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d.%m.%Y %H:%M')\n",
    "    \n",
    "    # drop duplicates (daylight saving time)\n",
    "    df = df[~df.duplicated()]\n",
    "    \n",
    "    # drop saturdays and sundays\n",
    "    mask = (df['datetime'].dt.dayofweek==5) | (df['datetime'].dt.dayofweek==6)\n",
    "    if (mask & (df['volume']!=0)).any():\n",
    "        continue\n",
    "    df = df[~mask]\n",
    "    \n",
    "    # drop inactive hours\n",
    "    mask = ((df['datetime'].dt.hour*100 + df['datetime'].dt.minute < 1530) | \\\n",
    "           ( df['datetime'].dt.hour*100 + df['datetime'].dt.minute > 2300))\n",
    "    if (mask & df['volume']!=0).any():\n",
    "        continue\n",
    "    mask = ~mask & (df['volume']!=0)\n",
    "    df = df[mask]\n",
    "    \n",
    "    # sort by datetime\n",
    "    df.sort_values(by=['datetime'], inplace=True)\n",
    "    \n",
    "    # reset index, more convenient\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # save data in feather format\n",
    "    file_name = os.path.basename(file).replace(\".csv\", \".feather\")\n",
    "    df.to_feather(os.path.join(save_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZHcRmEJQz0l"
   },
   "source": [
    "# Split the data\n",
    "\n",
    "\n",
    "In order to truly test our model , we split the data to 3 parts: \n",
    "1. Training set. This set will be used to train our models.\n",
    "2. Validatoion set. This is data the model didn't know - and so we can check that our model truly was able to learn from the data (and not just \"remember\" it).\n",
    "3. Test set. This simulets data we didn't even know. Only in the end of the proccess, after we pick our best model - we will tests in this dataset and see how well we did. \n",
    "\n",
    "Below is a graph visualizing the different sets we used:\n",
    "\n",
    "\n",
    "<img src=\"data_split.jpeg\" />\n",
    "\n",
    "\n",
    "We chose to split the data so that the training set would be around 60% of the data,  \n",
    "the validation is around 25% of the data, and the test set would be the remaining data. \n",
    "\n",
    "It is very popular to use K-fold cross validation technic for splitting the data into several training and validation folds,  \n",
    "to better assesing of models performance on test set.  \n",
    "However, we chose not to use this technic in our project, as it segnificantly increases the traning times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5gtm-mPAspu"
   },
   "source": [
    "\n",
    "# Evaluating models\n",
    "\n",
    "We need to have some common metrices so we can compare our models with.  \n",
    "To evaluate models performance with those metrices,  \n",
    "we need the true, actual, future returns, as an input (`real`),  \n",
    "along with the predicted future returns (`prediction`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rgjqAwJQv7a"
   },
   "source": [
    "## The matrics we used\n",
    "\n",
    "In order to evaulate our models, these are the matrics we used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YWsXfZqPSXr"
   },
   "source": [
    "### MSE\n",
    "\n",
    "This is our loss function. The lower the loss - the better our model predicted.  \n",
    "We chose to stick to the common `mean squared error` loss function to compare our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPROEV--PnE9"
   },
   "source": [
    "### \"Pnl\" - Profit n' Loss\n",
    "\n",
    "This is a common matric to evaluate trading strategies.  \n",
    "For each trade, the PnL is positive if that trade was profitable, and negative if it wasn't.  \n",
    "Here we are calculating the `mean` and `median` of the PnL through the entire predection.  \n",
    "We are also adding the `true_pnl` - this would have been our PnL in case we got **all** of our predictions correct.  \n",
    "This is not a real value (and should be the same for all of the models).  \n",
    "But this is a good was of comparing our results to the optimal results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riA0swfxRTfS"
   },
   "source": [
    "### Maximum win & Maximum loss\n",
    "\n",
    "These are the values of the best trade we had, and the worst trade we had."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmLF53mRRmmM"
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "This is a simple metric to understand the precentage that our model was right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTKVBZk0RsGn"
   },
   "source": [
    "### UP & Down Ratio\n",
    "\n",
    "In the entire prediction spcae - what precentage of the time the model predicted up, and what precentage the model predicted down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sz1nmudNRxOA"
   },
   "source": [
    "### True/False ups & True/False Downs\n",
    "\n",
    "These are metrices that help understand the number of times our model was right vs wrong, when looking only on ups/down.\n",
    "\n",
    "Note: `true_up` + `false_up` != 1, as zero values aren't counted on either of these variables.\n",
    "\n",
    "This is the complete code for our evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13HQ5NCfANMa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def mean_pnl(real: np.array, prediction: np.array):\n",
    "    winloss = np.sign(real * prediction)\n",
    "    return np.mean(winloss * np.abs(real))\n",
    "\n",
    "\n",
    "def median_pnl(real: np.array, prediction: np.array):\n",
    "    winloss = np.sign(real * prediction)\n",
    "    return np.median(winloss * np.abs(real))\n",
    "\n",
    "\n",
    "def calc_max_win(real: np.array, prediction: np.array):\n",
    "    winloss = np.sign(real * prediction)\n",
    "    return np.max(winloss * np.abs(real))\n",
    "\n",
    "\n",
    "def calc_max_loss(real: np.array, prediction: np.array):\n",
    "    winloss = np.sign(real * prediction)\n",
    "    return np.min(winloss * np.abs(real))\n",
    "\n",
    "\n",
    "def evaluate_model(real: np.array, prediction: np.array, model_name: str = 'Nan'):\n",
    "    mse = mean_squared_error(real, prediction)\n",
    "    true_pnl = np.mean(np.abs(real))\n",
    "    avg_pnl = mean_pnl(real, prediction)\n",
    "    median_pnl_ = median_pnl(real, prediction)\n",
    "\n",
    "    num_of_ups = sum(prediction > 0)\n",
    "    num_of_downs =  sum(prediction < 0)\n",
    "    \n",
    "    true_up = sum((real>0) * (prediction>0)) / num_of_ups\n",
    "    false_up = sum((real<0) * (prediction>0)) / num_of_ups\n",
    "\n",
    "    true_down = sum((real<0) * (prediction<0)) / num_of_downs\n",
    "    false_down = sum((real>0) * (prediction<0)) / num_of_downs\n",
    "\n",
    "    accuracy = (sum((real>0) * (prediction>0)) + sum((real<0) * (prediction<0))) / len(real)\n",
    "\n",
    "    max_win = calc_max_win(real, prediction)\n",
    "    max_loss = calc_max_loss(real, prediction)\n",
    "\n",
    "    up_ratio = num_of_ups / len(real)\n",
    "    down_ratio = num_of_downs / len(real)\n",
    "\n",
    "    ret = [\n",
    "        (\"model\", model_name),\n",
    "        (\"mse\", mse),\n",
    "        (\"true_pnl\", true_pnl),\n",
    "        (\"avg_pnl\", avg_pnl),\n",
    "        (\"median_pnl\", median_pnl_),\n",
    "        (\"true_up\", true_up),\n",
    "        (\"false_up\", false_up),\n",
    "        (\"true_down\", true_down),\n",
    "        (\"false_down\", false_down),\n",
    "        (\"up_ratio\", up_ratio),\n",
    "        (\"down_ratio\", down_ratio),\n",
    "        (\"accuracy\", accuracy),\n",
    "        (\"max_win\", max_win),\n",
    "        (\"max_loss\", max_loss)\n",
    "    ]\n",
    "    ret = [(name, value[0]) if isinstance(value, np.ndarray) else (name, value) for name, value in ret]\n",
    "\n",
    "    return OrderedDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGPzVa_mANMU"
   },
   "source": [
    "# Models\n",
    "\n",
    "In the following sections (1,2 and 3) we will try to model with only the `SPY` historical data,  \n",
    "and only at the last section (4), we try modeling with data of more stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPeynWdqAyQe"
   },
   "source": [
    "## 1) The \"classic\" approch - using Tecnical Indicators\n",
    "\n",
    "For a benchmark, we wanted to have a simple trading strategy that we can compare our results with.  \n",
    "We decided to feed a Random Forest model with some technical indicators, and see if we can get decent results.\n",
    "\n",
    "Technical indicators are all kind of calculations, that traders use in order to try and time the market.  \n",
    "These indicators can be as simple as moving averages of past prices - but can also go way more complicated then that. \n",
    "\n",
    "We used a library called `pandas_ta`, and we just calculated all the indicators they had.  \n",
    "We also added some more time ranges for some common indicators:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_dir = 'data/indicators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufXgrAvQGfLH"
   },
   "outputs": [],
   "source": [
    "# this is the technical indicators library that we used\n",
    "import pandas_ta as ta\n",
    "\n",
    "\n",
    "print(f\"processing {STOCK}\") \n",
    "df = pd.read_feather(f'data_feather/dukas_feather/{STOCK}.USUSD.feather')\n",
    "df.ta.strategy(name='all', verbose=True, timed=True)\n",
    "for length in [20, 50, 100, 200]:\n",
    "    df.ta.sma(length=length, append=True)  # simple moving average\n",
    "    df.ta.wma(length=length, append=True)  # weighted moving average\n",
    "    df.ta.mom(length=length, append=True)  # momentum\n",
    "    df.ta.roc(length=length, append=True)  # rate of change\n",
    "\n",
    "df.to_csv(f\"{indicators_dir}/{STOCK}_indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kresog3_H8F6"
   },
   "source": [
    "The first model we will try is Random Forest model.  \n",
    "It is a very fast model, so it is good for our perpose of having a decent banckmark to compare other results to. \n",
    "\n",
    "We are doing grid hyper parameters search, to find the best posible parameters for the forest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ox_wu5Nkwvfp"
   },
   "source": [
    "Minimal preprocessing to the data (removing nans and droping 10 last minuits of every day):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eNFeX0zwhwK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ABER_ZG_5_15</th>\n",
       "      <th>ABER_SG_5_15</th>\n",
       "      <th>ABER_XG_5_15</th>\n",
       "      <th>ABER_ATR_5_15</th>\n",
       "      <th>ACCBL_20</th>\n",
       "      <th>...</th>\n",
       "      <th>ROC_50</th>\n",
       "      <th>SMA_100</th>\n",
       "      <th>WMA_100</th>\n",
       "      <th>MOM_100</th>\n",
       "      <th>ROC_100</th>\n",
       "      <th>SMA_200</th>\n",
       "      <th>WMA_200</th>\n",
       "      <th>MOM_200</th>\n",
       "      <th>ROC_200</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-16 20:47:00</th>\n",
       "      <td>234.276</td>\n",
       "      <td>234.276</td>\n",
       "      <td>234.17</td>\n",
       "      <td>234.18</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>234.243400</td>\n",
       "      <td>234.300289</td>\n",
       "      <td>234.186511</td>\n",
       "      <td>0.056889</td>\n",
       "      <td>234.143215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127943</td>\n",
       "      <td>234.38856</td>\n",
       "      <td>234.361571</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>234.38063</td>\n",
       "      <td>234.367078</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.179028</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 20:48:00</th>\n",
       "      <td>234.180</td>\n",
       "      <td>234.240</td>\n",
       "      <td>234.17</td>\n",
       "      <td>234.23</td>\n",
       "      <td>0.3047</td>\n",
       "      <td>234.238733</td>\n",
       "      <td>234.296497</td>\n",
       "      <td>234.180970</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>234.141714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119398</td>\n",
       "      <td>234.38826</td>\n",
       "      <td>234.358431</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>234.37868</td>\n",
       "      <td>234.365580</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.166226</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 20:49:00</th>\n",
       "      <td>234.240</td>\n",
       "      <td>234.290</td>\n",
       "      <td>234.21</td>\n",
       "      <td>234.29</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>234.244733</td>\n",
       "      <td>234.303979</td>\n",
       "      <td>234.185487</td>\n",
       "      <td>0.059246</td>\n",
       "      <td>234.133715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063982</td>\n",
       "      <td>234.38866</td>\n",
       "      <td>234.356486</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>234.37708</td>\n",
       "      <td>234.364697</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.136397</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 20:50:00</th>\n",
       "      <td>234.289</td>\n",
       "      <td>234.310</td>\n",
       "      <td>234.27</td>\n",
       "      <td>234.29</td>\n",
       "      <td>0.2563</td>\n",
       "      <td>234.251733</td>\n",
       "      <td>234.309696</td>\n",
       "      <td>234.193770</td>\n",
       "      <td>0.057963</td>\n",
       "      <td>234.134714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055456</td>\n",
       "      <td>234.38846</td>\n",
       "      <td>234.354532</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.008536</td>\n",
       "      <td>234.37548</td>\n",
       "      <td>234.363831</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.136397</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 20:51:00</th>\n",
       "      <td>234.300</td>\n",
       "      <td>234.300</td>\n",
       "      <td>234.24</td>\n",
       "      <td>234.24</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>234.247067</td>\n",
       "      <td>234.305165</td>\n",
       "      <td>234.188968</td>\n",
       "      <td>0.058099</td>\n",
       "      <td>234.136214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046938</td>\n",
       "      <td>234.38796</td>\n",
       "      <td>234.351592</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.021341</td>\n",
       "      <td>234.37328</td>\n",
       "      <td>234.362483</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.187489</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high     low   close  volume  ABER_ZG_5_15  \\\n",
       "datetime                                                                      \n",
       "2017-02-16 20:47:00  234.276  234.276  234.17  234.18  0.1667    234.243400   \n",
       "2017-02-16 20:48:00  234.180  234.240  234.17  234.23  0.3047    234.238733   \n",
       "2017-02-16 20:49:00  234.240  234.290  234.21  234.29  0.1355    234.244733   \n",
       "2017-02-16 20:50:00  234.289  234.310  234.27  234.29  0.2563    234.251733   \n",
       "2017-02-16 20:51:00  234.300  234.300  234.24  234.24  0.0568    234.247067   \n",
       "\n",
       "                     ABER_SG_5_15  ABER_XG_5_15  ABER_ATR_5_15    ACCBL_20  \\\n",
       "datetime                                                                     \n",
       "2017-02-16 20:47:00    234.300289    234.186511       0.056889  234.143215   \n",
       "2017-02-16 20:48:00    234.296497    234.180970       0.057764  234.141714   \n",
       "2017-02-16 20:49:00    234.303979    234.185487       0.059246  234.133715   \n",
       "2017-02-16 20:50:00    234.309696    234.193770       0.057963  234.134714   \n",
       "2017-02-16 20:51:00    234.305165    234.188968       0.058099  234.136214   \n",
       "\n",
       "                     ...    ROC_50    SMA_100     WMA_100  MOM_100   ROC_100  \\\n",
       "datetime             ...                                                       \n",
       "2017-02-16 20:47:00  ... -0.127943  234.38856  234.361571    -0.14 -0.059747   \n",
       "2017-02-16 20:48:00  ... -0.119398  234.38826  234.358431    -0.03 -0.012806   \n",
       "2017-02-16 20:49:00  ... -0.063982  234.38866  234.356486     0.04  0.017076   \n",
       "2017-02-16 20:50:00  ... -0.055456  234.38846  234.354532    -0.02 -0.008536   \n",
       "2017-02-16 20:51:00  ... -0.046938  234.38796  234.351592    -0.05 -0.021341   \n",
       "\n",
       "                       SMA_200     WMA_200  MOM_200   ROC_200    target  \n",
       "datetime                                                                 \n",
       "2017-02-16 20:47:00  234.38063  234.367078    -0.42 -0.179028  0.000512  \n",
       "2017-02-16 20:48:00  234.37868  234.365580    -0.39 -0.166226  0.000555  \n",
       "2017-02-16 20:49:00  234.37708  234.364697    -0.32 -0.136397  0.000213  \n",
       "2017-02-16 20:50:00  234.37548  234.363831    -0.32 -0.136397  0.000256  \n",
       "2017-02-16 20:51:00  234.37328  234.362483    -0.44 -0.187489  0.000427  \n",
       "\n",
       "[5 rows x 162 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "df = pd.read_csv(Path(indicators_dir) / (STOCK + \"_indicators.csv\"), index_col=0, parse_dates=['datetime'])\n",
    "\n",
    "# drop indicators with null values after the first 1000 rows:\n",
    "row_to_drop = 250\n",
    "small_null_cols = df.iloc[row_to_drop:].isnull().any()\n",
    "small_null_cols = [col  for col, val in small_null_cols.items() if not val]\n",
    "small_null_cols\n",
    "df = df[small_null_cols]\n",
    "\n",
    "# set index\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime_col = pd.Series(df.index)\n",
    "mask = list(datetime_col.dt.day == datetime_col.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3pKvFBNxG3_"
   },
   "source": [
    "Split into training set, validation set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF22U6Bmw1hk"
   },
   "outputs": [],
   "source": [
    "# drop test data\n",
    "df = df[df.index < SPLIT_DATE2]\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "train_x = X[X.index < SPLIT_DATE1]\n",
    "valid_x = X[X.index >= SPLIT_DATE1]\n",
    "train_y = y[y.index < SPLIT_DATE1]\n",
    "valid_y = y[y.index >= SPLIT_DATE1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tuu1R9iUx1tr"
   },
   "source": [
    "Next thing is to run a Random Forest model on our data.  \n",
    "In order to find the best hyper-parameters possible, we run grid search -  \n",
    "which is basically brute-forcing different parameters for the model, until we find the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o582KBfmVGBJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hps = []\n",
    "hp_names = ['max_depth', 'min_samples_leaf', 'max_features', 'bootstrap', 'oob_score', 'n_estimators']\n",
    "hp_values = [[5,10,15], [50], [0.5,0.8], [True, False], [True, False], [30,100]]\n",
    "\n",
    "# create hyperparameters dictionary for each combo\n",
    "combos = list(itertools.product(*hp_values))\n",
    "for combo in combos:\n",
    "    hp = {key:val for key,val in zip(hp_names, combo)}\n",
    "    if not (hp['oob_score']==True and hp['bootstrap']==False):\n",
    "        hp['n_jobs'] = -1\n",
    "        hp['random_state'] = 1\n",
    "        hps.append(hp)\n",
    "\n",
    "len(hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R4iHzdYXBKm"
   },
   "source": [
    "Running the Random Forsts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtytwYZ6xPzS"
   },
   "outputs": [],
   "source": [
    "for i, hp in enumerate(hps):\n",
    "\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "\n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"saved_models_indocators/{STOCK}_all_indicators_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8CnGSbaDj86"
   },
   "source": [
    "### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttLGRlppXuaW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>indicatores_rf_12</td>\n",
       "      <td>1.319979e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.511101</td>\n",
       "      <td>0.483009</td>\n",
       "      <td>0.474257</td>\n",
       "      <td>0.512449</td>\n",
       "      <td>0.030614</td>\n",
       "      <td>0.969386</td>\n",
       "      <td>0.475385</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>indicatores_rf_14</td>\n",
       "      <td>1.319979e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.511101</td>\n",
       "      <td>0.483009</td>\n",
       "      <td>0.474257</td>\n",
       "      <td>0.512449</td>\n",
       "      <td>0.030614</td>\n",
       "      <td>0.969386</td>\n",
       "      <td>0.475385</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>indicatores_rf_6</td>\n",
       "      <td>8.841392e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.508172</td>\n",
       "      <td>0.483782</td>\n",
       "      <td>0.473985</td>\n",
       "      <td>0.512655</td>\n",
       "      <td>0.055166</td>\n",
       "      <td>0.944834</td>\n",
       "      <td>0.475871</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>indicatores_rf_8</td>\n",
       "      <td>8.841392e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.508172</td>\n",
       "      <td>0.483782</td>\n",
       "      <td>0.473985</td>\n",
       "      <td>0.512655</td>\n",
       "      <td>0.055166</td>\n",
       "      <td>0.944834</td>\n",
       "      <td>0.475871</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indicatores_rf_1</td>\n",
       "      <td>8.941484e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>0.512999</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.997198</td>\n",
       "      <td>0.473429</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indicatores_rf_3</td>\n",
       "      <td>8.941484e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>0.512999</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.997198</td>\n",
       "      <td>0.473429</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>indicatores_rf_9</td>\n",
       "      <td>8.673448e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.473907</td>\n",
       "      <td>0.513005</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>indicatores_rf_7</td>\n",
       "      <td>8.673448e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.473907</td>\n",
       "      <td>0.513005</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>indicatores_rf_34</td>\n",
       "      <td>1.437697e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.505165</td>\n",
       "      <td>0.480372</td>\n",
       "      <td>0.474193</td>\n",
       "      <td>0.512819</td>\n",
       "      <td>0.053710</td>\n",
       "      <td>0.946290</td>\n",
       "      <td>0.475857</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indicatores_rf_0</td>\n",
       "      <td>8.850493e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.508238</td>\n",
       "      <td>0.483774</td>\n",
       "      <td>0.473981</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.055569</td>\n",
       "      <td>0.944431</td>\n",
       "      <td>0.475885</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indicatores_rf_2</td>\n",
       "      <td>8.850493e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.508238</td>\n",
       "      <td>0.483774</td>\n",
       "      <td>0.473981</td>\n",
       "      <td>0.512653</td>\n",
       "      <td>0.055569</td>\n",
       "      <td>0.944431</td>\n",
       "      <td>0.475885</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>indicatores_rf_32</td>\n",
       "      <td>1.700870e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.500836</td>\n",
       "      <td>0.493593</td>\n",
       "      <td>0.474038</td>\n",
       "      <td>0.512703</td>\n",
       "      <td>0.024899</td>\n",
       "      <td>0.975101</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>indicatores_rf_30</td>\n",
       "      <td>1.700870e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.500836</td>\n",
       "      <td>0.493593</td>\n",
       "      <td>0.474038</td>\n",
       "      <td>0.512703</td>\n",
       "      <td>0.024899</td>\n",
       "      <td>0.975101</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>indicatores_rf_10</td>\n",
       "      <td>8.511951e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.288557</td>\n",
       "      <td>0.706468</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.513034</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.997212</td>\n",
       "      <td>0.473360</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>indicatores_rf_11</td>\n",
       "      <td>8.506244e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.288557</td>\n",
       "      <td>0.706468</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.513034</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.997212</td>\n",
       "      <td>0.473360</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>indicatores_rf_25</td>\n",
       "      <td>1.791458e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.473976</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.982370</td>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>indicatores_rf_27</td>\n",
       "      <td>1.791458e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.473976</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.982370</td>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>indicatores_rf_5</td>\n",
       "      <td>8.495744e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.314410</td>\n",
       "      <td>0.681223</td>\n",
       "      <td>0.473867</td>\n",
       "      <td>0.513039</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.996823</td>\n",
       "      <td>0.473360</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>indicatores_rf_29</td>\n",
       "      <td>1.255210e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.496564</td>\n",
       "      <td>0.496564</td>\n",
       "      <td>0.474164</td>\n",
       "      <td>0.512668</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>0.983854</td>\n",
       "      <td>0.474525</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>indicatores_rf_16</td>\n",
       "      <td>1.176993e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.504082</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>0.473893</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.047579</td>\n",
       "      <td>0.952421</td>\n",
       "      <td>0.475330</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>indicatores_rf_13</td>\n",
       "      <td>1.439754e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.497334</td>\n",
       "      <td>0.497819</td>\n",
       "      <td>0.473839</td>\n",
       "      <td>0.512852</td>\n",
       "      <td>0.028617</td>\n",
       "      <td>0.971383</td>\n",
       "      <td>0.474511</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>indicatores_rf_15</td>\n",
       "      <td>1.439754e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.497334</td>\n",
       "      <td>0.497819</td>\n",
       "      <td>0.473839</td>\n",
       "      <td>0.512852</td>\n",
       "      <td>0.028617</td>\n",
       "      <td>0.971383</td>\n",
       "      <td>0.474511</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>indicatores_rf_28</td>\n",
       "      <td>1.240446e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.488309</td>\n",
       "      <td>0.505396</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.512785</td>\n",
       "      <td>0.015425</td>\n",
       "      <td>0.984575</td>\n",
       "      <td>0.474262</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indicatores_rf_4</td>\n",
       "      <td>8.535774e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.505525</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.512849</td>\n",
       "      <td>0.060257</td>\n",
       "      <td>0.939743</td>\n",
       "      <td>0.475690</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>indicatores_rf_35</td>\n",
       "      <td>1.458381e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.501517</td>\n",
       "      <td>0.484380</td>\n",
       "      <td>0.473533</td>\n",
       "      <td>0.513504</td>\n",
       "      <td>0.091468</td>\n",
       "      <td>0.908532</td>\n",
       "      <td>0.476093</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>indicatores_rf_21</td>\n",
       "      <td>1.515821e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.503374</td>\n",
       "      <td>0.473920</td>\n",
       "      <td>0.512852</td>\n",
       "      <td>0.020557</td>\n",
       "      <td>0.979443</td>\n",
       "      <td>0.474276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>indicatores_rf_19</td>\n",
       "      <td>1.515821e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.503374</td>\n",
       "      <td>0.473920</td>\n",
       "      <td>0.512852</td>\n",
       "      <td>0.020557</td>\n",
       "      <td>0.979443</td>\n",
       "      <td>0.474276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>indicatores_rf_31</td>\n",
       "      <td>1.740495e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.487069</td>\n",
       "      <td>0.506466</td>\n",
       "      <td>0.473896</td>\n",
       "      <td>0.512907</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.980691</td>\n",
       "      <td>0.474151</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>indicatores_rf_33</td>\n",
       "      <td>1.740495e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.487069</td>\n",
       "      <td>0.506466</td>\n",
       "      <td>0.473896</td>\n",
       "      <td>0.512907</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.980691</td>\n",
       "      <td>0.474151</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>indicatores_rf_22</td>\n",
       "      <td>1.495379e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.494741</td>\n",
       "      <td>0.497809</td>\n",
       "      <td>0.473764</td>\n",
       "      <td>0.512985</td>\n",
       "      <td>0.031654</td>\n",
       "      <td>0.968346</td>\n",
       "      <td>0.474428</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>indicatores_rf_20</td>\n",
       "      <td>1.759869e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.480590</td>\n",
       "      <td>0.513975</td>\n",
       "      <td>0.473808</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>0.982134</td>\n",
       "      <td>0.473929</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>indicatores_rf_18</td>\n",
       "      <td>1.759869e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.480590</td>\n",
       "      <td>0.513975</td>\n",
       "      <td>0.473808</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>0.982134</td>\n",
       "      <td>0.473929</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>indicatores_rf_17</td>\n",
       "      <td>1.127162e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.500714</td>\n",
       "      <td>0.490717</td>\n",
       "      <td>0.473699</td>\n",
       "      <td>0.513005</td>\n",
       "      <td>0.048564</td>\n",
       "      <td>0.951436</td>\n",
       "      <td>0.475011</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>indicatores_rf_23</td>\n",
       "      <td>1.388495e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.496317</td>\n",
       "      <td>0.490251</td>\n",
       "      <td>0.473449</td>\n",
       "      <td>0.513509</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.474914</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>indicatores_rf_24</td>\n",
       "      <td>1.742919e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.484668</td>\n",
       "      <td>0.507666</td>\n",
       "      <td>0.473410</td>\n",
       "      <td>0.513342</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>indicatores_rf_26</td>\n",
       "      <td>1.742919e-06</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.484668</td>\n",
       "      <td>0.507666</td>\n",
       "      <td>0.473410</td>\n",
       "      <td>0.513342</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "12  indicatores_rf_12  1.319979e-06  0.000617 -0.000004   -0.000032  0.511101   \n",
       "14  indicatores_rf_14  1.319979e-06  0.000617 -0.000004   -0.000032  0.511101   \n",
       "6    indicatores_rf_6  8.841392e-07  0.000617 -0.000005   -0.000032  0.508172   \n",
       "8    indicatores_rf_8  8.841392e-07  0.000617 -0.000005   -0.000032  0.508172   \n",
       "1    indicatores_rf_1  8.941484e-07  0.000617 -0.000005   -0.000033  0.301980   \n",
       "3    indicatores_rf_3  8.941484e-07  0.000617 -0.000005   -0.000033  0.301980   \n",
       "9    indicatores_rf_9  8.673448e-07  0.000617 -0.000005   -0.000033  0.292308   \n",
       "7    indicatores_rf_7  8.673448e-07  0.000617 -0.000005   -0.000033  0.292308   \n",
       "34  indicatores_rf_34  1.437697e-06  0.000617 -0.000005   -0.000032  0.505165   \n",
       "0    indicatores_rf_0  8.850493e-07  0.000617 -0.000006   -0.000032  0.508238   \n",
       "2    indicatores_rf_2  8.850493e-07  0.000617 -0.000006   -0.000032  0.508238   \n",
       "32  indicatores_rf_32  1.700870e-06  0.000617 -0.000006   -0.000033  0.500836   \n",
       "30  indicatores_rf_30  1.700870e-06  0.000617 -0.000006   -0.000033  0.500836   \n",
       "10  indicatores_rf_10  8.511951e-07  0.000617 -0.000006   -0.000033  0.288557   \n",
       "11  indicatores_rf_11  8.506244e-07  0.000617 -0.000006   -0.000033  0.288557   \n",
       "25  indicatores_rf_25  1.791458e-06  0.000617 -0.000006   -0.000033  0.489378   \n",
       "27  indicatores_rf_27  1.791458e-06  0.000617 -0.000006   -0.000033  0.489378   \n",
       "5    indicatores_rf_5  8.495744e-07  0.000617 -0.000006   -0.000033  0.314410   \n",
       "29  indicatores_rf_29  1.255210e-06  0.000617 -0.000006   -0.000033  0.496564   \n",
       "16  indicatores_rf_16  1.176993e-06  0.000617 -0.000006   -0.000033  0.504082   \n",
       "13  indicatores_rf_13  1.439754e-06  0.000617 -0.000006   -0.000033  0.497334   \n",
       "15  indicatores_rf_15  1.439754e-06  0.000617 -0.000006   -0.000033  0.497334   \n",
       "28  indicatores_rf_28  1.240446e-06  0.000617 -0.000006   -0.000033  0.488309   \n",
       "4    indicatores_rf_4  8.535774e-07  0.000617 -0.000007   -0.000032  0.505525   \n",
       "35  indicatores_rf_35  1.458381e-06  0.000617 -0.000007   -0.000032  0.501517   \n",
       "21  indicatores_rf_21  1.515821e-06  0.000617 -0.000007   -0.000033  0.491228   \n",
       "19  indicatores_rf_19  1.515821e-06  0.000617 -0.000007   -0.000033  0.491228   \n",
       "31  indicatores_rf_31  1.740495e-06  0.000617 -0.000007   -0.000033  0.487069   \n",
       "33  indicatores_rf_33  1.740495e-06  0.000617 -0.000007   -0.000033  0.487069   \n",
       "22  indicatores_rf_22  1.495379e-06  0.000617 -0.000007   -0.000033  0.494741   \n",
       "20  indicatores_rf_20  1.759869e-06  0.000617 -0.000007   -0.000033  0.480590   \n",
       "18  indicatores_rf_18  1.759869e-06  0.000617 -0.000007   -0.000033  0.480590   \n",
       "17  indicatores_rf_17  1.127162e-06  0.000617 -0.000007   -0.000033  0.500714   \n",
       "23  indicatores_rf_23  1.388495e-06  0.000617 -0.000008   -0.000033  0.496317   \n",
       "24  indicatores_rf_24  1.742919e-06  0.000617 -0.000009   -0.000033  0.484668   \n",
       "26  indicatores_rf_26  1.742919e-06  0.000617 -0.000009   -0.000033  0.484668   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "12  0.483009   0.474257    0.512449  0.030614    0.969386  0.475385  0.014526   \n",
       "14  0.483009   0.474257    0.512449  0.030614    0.969386  0.475385  0.014526   \n",
       "6   0.483782   0.473985    0.512655  0.055166    0.944834  0.475871  0.014526   \n",
       "8   0.483782   0.473985    0.512655  0.055166    0.944834  0.475871  0.014526   \n",
       "1   0.693069   0.473911    0.512999  0.002802    0.997198  0.473429  0.014526   \n",
       "3   0.693069   0.473911    0.512999  0.002802    0.997198  0.473429  0.014526   \n",
       "9   0.702564   0.473907    0.513005  0.002705    0.997295  0.473416  0.014526   \n",
       "7   0.702564   0.473907    0.513005  0.002705    0.997295  0.473416  0.014526   \n",
       "34  0.480372   0.474193    0.512819  0.053710    0.946290  0.475857  0.014526   \n",
       "0   0.483774   0.473981    0.512653  0.055569    0.944431  0.475885  0.014526   \n",
       "2   0.483774   0.473981    0.512653  0.055569    0.944431  0.475885  0.014526   \n",
       "32  0.493593   0.474038    0.512703  0.024899    0.975101  0.474706  0.014526   \n",
       "30  0.493593   0.474038    0.512703  0.024899    0.975101  0.474706  0.014526   \n",
       "10  0.706468   0.473877    0.513034  0.002788    0.997212  0.473360  0.014526   \n",
       "11  0.706468   0.473877    0.513034  0.002788    0.997212  0.473360  0.014526   \n",
       "25  0.505114   0.473976    0.512821  0.017630    0.982370  0.474248  0.014526   \n",
       "27  0.505114   0.473976    0.512821  0.017630    0.982370  0.474248  0.014526   \n",
       "5   0.681223   0.473867    0.513039  0.003177    0.996823  0.473360  0.014526   \n",
       "29  0.496564   0.474164    0.512668  0.016146    0.983854  0.474525  0.014526   \n",
       "16  0.487172   0.473893    0.512824  0.047579    0.952421  0.475330  0.014526   \n",
       "13  0.497819   0.473839    0.512852  0.028617    0.971383  0.474511  0.014526   \n",
       "15  0.497819   0.473839    0.512852  0.028617    0.971383  0.474511  0.014526   \n",
       "28  0.505396   0.474042    0.512785  0.015425    0.984575  0.474262  0.014526   \n",
       "4   0.486188   0.473777    0.512849  0.060257    0.939743  0.475690  0.014526   \n",
       "35  0.484380   0.473533    0.513504  0.091468    0.908532  0.476093  0.014526   \n",
       "21  0.503374   0.473920    0.512852  0.020557    0.979443  0.474276  0.014526   \n",
       "19  0.503374   0.473920    0.512852  0.020557    0.979443  0.474276  0.014526   \n",
       "31  0.506466   0.473896    0.512907  0.019309    0.980691  0.474151  0.014526   \n",
       "33  0.506466   0.473896    0.512907  0.019309    0.980691  0.474151  0.014526   \n",
       "22  0.497809   0.473764    0.512985  0.031654    0.968346  0.474428  0.014526   \n",
       "20  0.513975   0.473808    0.512987  0.017866    0.982134  0.473929  0.014526   \n",
       "18  0.513975   0.473808    0.512987  0.017866    0.982134  0.473929  0.014526   \n",
       "17  0.490717   0.473699    0.513005  0.048564    0.951436  0.475011  0.014526   \n",
       "23  0.490251   0.473449    0.513509  0.064030    0.935970  0.474914  0.014526   \n",
       "24  0.507666   0.473410    0.513342  0.032570    0.967430  0.473776  0.014526   \n",
       "26  0.507666   0.473410    0.513342  0.032570    0.967430  0.473776  0.014526   \n",
       "\n",
       "    max_loss  \n",
       "12 -0.013493  \n",
       "14 -0.013493  \n",
       "6  -0.013493  \n",
       "8  -0.013493  \n",
       "1  -0.013493  \n",
       "3  -0.013493  \n",
       "9  -0.013493  \n",
       "7  -0.013493  \n",
       "34 -0.013493  \n",
       "0  -0.013493  \n",
       "2  -0.013493  \n",
       "32 -0.013493  \n",
       "30 -0.013493  \n",
       "10 -0.013493  \n",
       "11 -0.013493  \n",
       "25 -0.013493  \n",
       "27 -0.013493  \n",
       "5  -0.013493  \n",
       "29 -0.013493  \n",
       "16 -0.013493  \n",
       "13 -0.013493  \n",
       "15 -0.013493  \n",
       "28 -0.013493  \n",
       "4  -0.013493  \n",
       "35 -0.013493  \n",
       "21 -0.013493  \n",
       "19 -0.013493  \n",
       "31 -0.013493  \n",
       "33 -0.013493  \n",
       "22 -0.013493  \n",
       "20 -0.013493  \n",
       "18 -0.013493  \n",
       "17 -0.013493  \n",
       "23 -0.013493  \n",
       "24 -0.013493  \n",
       "26 -0.013493  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load rf\n",
    "\n",
    "rfs = []\n",
    "for i in range(36):\n",
    "    rf = load(f\"saved_models_indocators/{STOCK}_all_indicators_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y, prediction, f\"indicatores_rf_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5yt4MoMXwuf"
   },
   "source": [
    "Not amazing results. Lets see what were the best indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "HL5D0H6oXyXQ",
    "outputId": "a09febcd-08c7-4b79-de19-2bccd7c1c5ac"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "importences = collections.defaultdict(list)\n",
    "\n",
    "for forest in rfs:\n",
    "    for col, importence in zip(df.columns, forest.feature_importances_):\n",
    "        importences[col].append(importence)\n",
    "\n",
    "feature_averages = []\n",
    "\n",
    "for feature, importences in importences.items():\n",
    "    feature_averages.append((feature, sum(importences) / len(importences)))\n",
    "    \n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.DataFrame(feature_averages)\n",
    "df.columns = ['feature', 'importence']\n",
    "df = df.sort_values(by='importence')\n",
    "\n",
    "fig = px.bar(df, x='feature', y='importence')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"indicators_full.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"indicators_zoom.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZnmyRWGol3g"
   },
   "source": [
    "We can see that the Rate-Of-change for the past 200 minuets, and the Momentum for the past 200 minuets where the top indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wiosn3DAoavG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>close</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.002458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importence\n",
       "0    open    0.000133\n",
       "1    high    0.000203\n",
       "2     low    0.000344\n",
       "3   close    0.000414\n",
       "4  volume    0.002458"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['feature'].isin(['close', 'volume', 'open', 'high', 'low'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIT6KdbLX3wC"
   },
   "source": [
    "Another thing to notice is that the raw values (close, volume, open, high and low) are much less importnt than most of the calculated indicators.\n",
    "\n",
    "\n",
    "We could go on in digging deeper into the indicators - but instead, we want to use a more \"modern\" approach -  \n",
    "giving the model ***all*** the raw data, and just letting the model do the hard work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vZyv0koDs0W"
   },
   "source": [
    "## 2) rolling windows\n",
    "After trying to build Random Forest model, fed by technical indicators,  \n",
    "we try to build Random Forest model fed by \"rolling windows\".  \n",
    "\n",
    "Here, the data is generated from current and fixed amount of past stock's prices - therefore the \"window\" term -  \n",
    "and its relatively stays in raw-ish, unprocessed form, compare to the technical indicators approach.\n",
    "\n",
    "The next sections follow the scheme below:\n",
    "1. Generate rolling windows.\n",
    "2. Build many Random Forests, with many hyperparameters, using grid search.\n",
    "3. Train and test (validate) those Random Forests with the generated windows.\n",
    "\n",
    "We try several approaches of windows generation,  \n",
    "while keeping the Random Forests grid search relatively the same (same hyperparameters).  \n",
    "This way we can get a sense of which windows genereation technic is better,  \n",
    "while keeping the models at relatively the same \"stregth\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPLT-YkctY5d"
   },
   "source": [
    "### read data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7yk0eV-6YEn"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "save_dir = \"saved_models/\" \n",
    "\n",
    "# read data\n",
    "df = read_data(data_dir, STOCK)\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "# drop Nans + test data\n",
    "df.dropna(inplace=True)\n",
    "df = df[df.index < SPLIT_DATE2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P5gsqoWzudRE",
    "outputId": "543fda0b-d481-4327-8fc8-bd0eaa313933"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "search_hps = {\n",
    "    'n_estimators': [30,100],\n",
    "    'max_depth': [6,14],\n",
    "    'min_samples_leaf': [100],\n",
    "    'max_features': [0.5, 0.8],\n",
    "    'bootstrap': [True, False],\n",
    "    'oob_score': [True, False],\n",
    "    'random_state': [1],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# create many sets of hyperparameters\n",
    "hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in hps]\n",
    "\n",
    "# drop invalid hyperparameters\n",
    "good_hps_idx = []\n",
    "for i, hp in enumerate(hps):\n",
    "    if not (hp['bootstrap'] == False and hp['oob_score'] == True):\n",
    "        good_hps_idx.append(i)\n",
    "hps = [hps[i] for i in good_hps_idx]\n",
    "len(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "v69s25IwBtbJ",
    "outputId": "2c048577-6f70-4fcd-d0ca-6c11e6efd244"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:27:00</th>\n",
       "      <td>234.580</td>\n",
       "      <td>234.62</td>\n",
       "      <td>234.56</td>\n",
       "      <td>234.60</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>0.001066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:28:00</th>\n",
       "      <td>234.609</td>\n",
       "      <td>234.65</td>\n",
       "      <td>234.59</td>\n",
       "      <td>234.62</td>\n",
       "      <td>0.3435</td>\n",
       "      <td>0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:29:00</th>\n",
       "      <td>234.610</td>\n",
       "      <td>234.62</td>\n",
       "      <td>234.59</td>\n",
       "      <td>234.61</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:30:00</th>\n",
       "      <td>234.610</td>\n",
       "      <td>234.63</td>\n",
       "      <td>234.59</td>\n",
       "      <td>234.61</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:31:00</th>\n",
       "      <td>234.608</td>\n",
       "      <td>234.69</td>\n",
       "      <td>234.60</td>\n",
       "      <td>234.68</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    high     low   close  volume    target\n",
       "datetime                                                              \n",
       "2017-02-16 17:27:00  234.580  234.62  234.56  234.60  0.3455  0.001066\n",
       "2017-02-16 17:28:00  234.609  234.65  234.59  234.62  0.3435  0.001151\n",
       "2017-02-16 17:29:00  234.610  234.62  234.59  234.61  0.1581  0.001108\n",
       "2017-02-16 17:30:00  234.610  234.63  234.59  234.61  0.2130  0.001364\n",
       "2017-02-16 17:31:00  234.608  234.69  234.60  234.68  0.1257  0.001108"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O13EpOooJ2Jw"
   },
   "source": [
    "### 2.1) \"Linear\" rolling windows\n",
    "\n",
    "The simplest, most straight-forward approach:  \n",
    "each window will be built out of the current and last 10 open, high, low, close and volume values.  \n",
    "\n",
    "Below is a visual illustration simple \"linear\" window with size 10.\n",
    "\n",
    "<img src=\"linear_wondows.jpeg\" />\n",
    "\n",
    "**Windows generation:**\n",
    "1. Build each window out of last 10 open, high, low, close and volume values.\n",
    "2. Normalize using min-max normalization.\n",
    "3. Split windows to training windows and validation windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_windows(df, window_size):\n",
    "    \"\"\" return all windows in given size for each column in given dataframe \"\"\"\n",
    "    data = df.values.T\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def df_windows(df, window_size):\n",
    "    \"\"\" return all windows in given size given dataframe \"\"\"\n",
    "    windows = column_windows(df, window_size)\n",
    "    if windows.ndim == 2:\n",
    "        return windows\n",
    "    else:\n",
    "        f = lambda x : np.flipud(np.rot90(x))\n",
    "        return np.array([f(x) for x in np.stack(windows, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4P4lWe77Bv1i",
    "outputId": "8c2665e7-54a8-4485-faa7-b6d6a2b45e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 48s, sys: 3.55 s, total: 4min 52s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "UcxV_rK3EVy_",
    "outputId": "855b82f7-ade1-4940-c19c-eb506f31331c"
   },
   "outputs": [],
   "source": [
    "# RFs grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "    \n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"{save_dir}{STOCK}_linear_window_rf_{i}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "13DTeA4TUEG4",
    "outputId": "01233013-a20d-4cc3-b693-e17ec36d2ff0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_liniar_22</td>\n",
       "      <td>8.416157e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520935</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.505489</td>\n",
       "      <td>0.443994</td>\n",
       "      <td>0.556006</td>\n",
       "      <td>0.498918</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_liniar_21</td>\n",
       "      <td>8.416157e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520935</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.505489</td>\n",
       "      <td>0.443994</td>\n",
       "      <td>0.556006</td>\n",
       "      <td>0.498918</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_liniar_16</td>\n",
       "      <td>8.408198e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522480</td>\n",
       "      <td>0.463646</td>\n",
       "      <td>0.481606</td>\n",
       "      <td>0.505843</td>\n",
       "      <td>0.390958</td>\n",
       "      <td>0.609042</td>\n",
       "      <td>0.497586</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_liniar_15</td>\n",
       "      <td>8.408198e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522480</td>\n",
       "      <td>0.463646</td>\n",
       "      <td>0.481606</td>\n",
       "      <td>0.505843</td>\n",
       "      <td>0.390958</td>\n",
       "      <td>0.609042</td>\n",
       "      <td>0.497586</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_liniar_2</td>\n",
       "      <td>8.409627e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524428</td>\n",
       "      <td>0.462949</td>\n",
       "      <td>0.481527</td>\n",
       "      <td>0.505139</td>\n",
       "      <td>0.373686</td>\n",
       "      <td>0.626314</td>\n",
       "      <td>0.497558</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_liniar_14</td>\n",
       "      <td>8.409305e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523568</td>\n",
       "      <td>0.463102</td>\n",
       "      <td>0.481525</td>\n",
       "      <td>0.505565</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>0.623262</td>\n",
       "      <td>0.497364</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_liniar_20</td>\n",
       "      <td>8.417588e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519866</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.480109</td>\n",
       "      <td>0.506759</td>\n",
       "      <td>0.426334</td>\n",
       "      <td>0.573666</td>\n",
       "      <td>0.497059</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_liniar_8</td>\n",
       "      <td>8.425092e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519199</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.480026</td>\n",
       "      <td>0.506882</td>\n",
       "      <td>0.443661</td>\n",
       "      <td>0.556339</td>\n",
       "      <td>0.497406</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_liniar_0</td>\n",
       "      <td>8.410141e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521193</td>\n",
       "      <td>0.465555</td>\n",
       "      <td>0.480628</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.400946</td>\n",
       "      <td>0.599054</td>\n",
       "      <td>0.496892</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_liniar_1</td>\n",
       "      <td>8.410141e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521193</td>\n",
       "      <td>0.465555</td>\n",
       "      <td>0.480628</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.400946</td>\n",
       "      <td>0.599054</td>\n",
       "      <td>0.496892</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_liniar_13</td>\n",
       "      <td>8.407695e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520535</td>\n",
       "      <td>0.465622</td>\n",
       "      <td>0.480408</td>\n",
       "      <td>0.507026</td>\n",
       "      <td>0.393857</td>\n",
       "      <td>0.606143</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_liniar_12</td>\n",
       "      <td>8.407695e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520535</td>\n",
       "      <td>0.465622</td>\n",
       "      <td>0.480408</td>\n",
       "      <td>0.507026</td>\n",
       "      <td>0.393857</td>\n",
       "      <td>0.606143</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_liniar_17</td>\n",
       "      <td>8.417182e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524395</td>\n",
       "      <td>0.463003</td>\n",
       "      <td>0.481351</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.368788</td>\n",
       "      <td>0.631212</td>\n",
       "      <td>0.497225</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_liniar_23</td>\n",
       "      <td>8.449574e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519134</td>\n",
       "      <td>0.468111</td>\n",
       "      <td>0.479227</td>\n",
       "      <td>0.507480</td>\n",
       "      <td>0.417635</td>\n",
       "      <td>0.582365</td>\n",
       "      <td>0.495894</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_liniar_10</td>\n",
       "      <td>8.426128e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517975</td>\n",
       "      <td>0.468937</td>\n",
       "      <td>0.479396</td>\n",
       "      <td>0.507553</td>\n",
       "      <td>0.460004</td>\n",
       "      <td>0.539996</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_liniar_9</td>\n",
       "      <td>8.426128e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517975</td>\n",
       "      <td>0.468937</td>\n",
       "      <td>0.479396</td>\n",
       "      <td>0.507553</td>\n",
       "      <td>0.460004</td>\n",
       "      <td>0.539996</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_liniar_5</td>\n",
       "      <td>8.416984e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524511</td>\n",
       "      <td>0.462797</td>\n",
       "      <td>0.481395</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>0.366194</td>\n",
       "      <td>0.633806</td>\n",
       "      <td>0.497184</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_liniar_4</td>\n",
       "      <td>8.409593e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519941</td>\n",
       "      <td>0.467001</td>\n",
       "      <td>0.479903</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.412211</td>\n",
       "      <td>0.587789</td>\n",
       "      <td>0.496407</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_liniar_3</td>\n",
       "      <td>8.409593e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519941</td>\n",
       "      <td>0.467001</td>\n",
       "      <td>0.479903</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.412211</td>\n",
       "      <td>0.587789</td>\n",
       "      <td>0.496407</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_liniar_7</td>\n",
       "      <td>8.427806e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515861</td>\n",
       "      <td>0.470796</td>\n",
       "      <td>0.477847</td>\n",
       "      <td>0.509321</td>\n",
       "      <td>0.462695</td>\n",
       "      <td>0.537305</td>\n",
       "      <td>0.495436</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_liniar_6</td>\n",
       "      <td>8.427806e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515861</td>\n",
       "      <td>0.470796</td>\n",
       "      <td>0.477847</td>\n",
       "      <td>0.509321</td>\n",
       "      <td>0.462695</td>\n",
       "      <td>0.537305</td>\n",
       "      <td>0.495436</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_liniar_11</td>\n",
       "      <td>8.455592e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517102</td>\n",
       "      <td>0.470084</td>\n",
       "      <td>0.478007</td>\n",
       "      <td>0.508731</td>\n",
       "      <td>0.431966</td>\n",
       "      <td>0.568034</td>\n",
       "      <td>0.494895</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_liniar_18</td>\n",
       "      <td>8.417179e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516582</td>\n",
       "      <td>0.469820</td>\n",
       "      <td>0.478355</td>\n",
       "      <td>0.508996</td>\n",
       "      <td>0.441747</td>\n",
       "      <td>0.558253</td>\n",
       "      <td>0.495242</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_liniar_19</td>\n",
       "      <td>8.417179e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516582</td>\n",
       "      <td>0.469820</td>\n",
       "      <td>0.478355</td>\n",
       "      <td>0.508996</td>\n",
       "      <td>0.441747</td>\n",
       "      <td>0.558253</td>\n",
       "      <td>0.495242</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "22  rf_liniar_22  8.416157e-07  0.000617  0.000017         0.0  0.520935   \n",
       "21  rf_liniar_21  8.416157e-07  0.000617  0.000017         0.0  0.520935   \n",
       "16  rf_liniar_16  8.408198e-07  0.000617  0.000015         0.0  0.522480   \n",
       "15  rf_liniar_15  8.408198e-07  0.000617  0.000015         0.0  0.522480   \n",
       "2    rf_liniar_2  8.409627e-07  0.000617  0.000014         0.0  0.524428   \n",
       "14  rf_liniar_14  8.409305e-07  0.000617  0.000013         0.0  0.523568   \n",
       "20  rf_liniar_20  8.417588e-07  0.000617  0.000013         0.0  0.519866   \n",
       "8    rf_liniar_8  8.425092e-07  0.000617  0.000013         0.0  0.519199   \n",
       "0    rf_liniar_0  8.410141e-07  0.000617  0.000013         0.0  0.521193   \n",
       "1    rf_liniar_1  8.410141e-07  0.000617  0.000013         0.0  0.521193   \n",
       "13  rf_liniar_13  8.407695e-07  0.000617  0.000012         0.0  0.520535   \n",
       "12  rf_liniar_12  8.407695e-07  0.000617  0.000012         0.0  0.520535   \n",
       "17  rf_liniar_17  8.417182e-07  0.000617  0.000012         0.0  0.524395   \n",
       "23  rf_liniar_23  8.449574e-07  0.000617  0.000012         0.0  0.519134   \n",
       "10  rf_liniar_10  8.426128e-07  0.000617  0.000011         0.0  0.517975   \n",
       "9    rf_liniar_9  8.426128e-07  0.000617  0.000011         0.0  0.517975   \n",
       "5    rf_liniar_5  8.416984e-07  0.000617  0.000011         0.0  0.524511   \n",
       "4    rf_liniar_4  8.409593e-07  0.000617  0.000011         0.0  0.519941   \n",
       "3    rf_liniar_3  8.409593e-07  0.000617  0.000011         0.0  0.519941   \n",
       "7    rf_liniar_7  8.427806e-07  0.000617  0.000011         0.0  0.515861   \n",
       "6    rf_liniar_6  8.427806e-07  0.000617  0.000011         0.0  0.515861   \n",
       "11  rf_liniar_11  8.455592e-07  0.000617  0.000009         0.0  0.517102   \n",
       "18  rf_liniar_18  8.417179e-07  0.000617  0.000008         0.0  0.516582   \n",
       "19  rf_liniar_19  8.417179e-07  0.000617  0.000008         0.0  0.516582   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "22  0.466129   0.481336    0.505489  0.443994    0.556006  0.498918  0.014526   \n",
       "21  0.466129   0.481336    0.505489  0.443994    0.556006  0.498918  0.014526   \n",
       "16  0.463646   0.481606    0.505843  0.390958    0.609042  0.497586  0.014526   \n",
       "15  0.463646   0.481606    0.505843  0.390958    0.609042  0.497586  0.014526   \n",
       "2   0.462949   0.481527    0.505139  0.373686    0.626314  0.497558  0.014526   \n",
       "14  0.463102   0.481525    0.505565  0.376738    0.623262  0.497364  0.014526   \n",
       "20  0.467150   0.480109    0.506759  0.426334    0.573666  0.497059  0.014526   \n",
       "8   0.467761   0.480026    0.506882  0.443661    0.556339  0.497406  0.013981   \n",
       "0   0.465555   0.480628    0.506426  0.400946    0.599054  0.496892  0.013981   \n",
       "1   0.465555   0.480628    0.506426  0.400946    0.599054  0.496892  0.013981   \n",
       "13  0.465622   0.480408    0.507026  0.393857    0.606143  0.496213  0.012759   \n",
       "12  0.465622   0.480408    0.507026  0.393857    0.606143  0.496213  0.012759   \n",
       "17  0.463003   0.481351    0.505308  0.368788    0.631212  0.497225  0.014526   \n",
       "23  0.468111   0.479227    0.507480  0.417635    0.582365  0.495894  0.014526   \n",
       "10  0.468937   0.479396    0.507553  0.460004    0.539996  0.497142  0.014526   \n",
       "9   0.468937   0.479396    0.507553  0.460004    0.539996  0.497142  0.014526   \n",
       "5   0.462797   0.481395    0.505319  0.366194    0.633806  0.497184  0.014526   \n",
       "4   0.467001   0.479903    0.507022  0.412211    0.587789  0.496407  0.014526   \n",
       "3   0.467001   0.479903    0.507022  0.412211    0.587789  0.496407  0.014526   \n",
       "7   0.470796   0.477847    0.509321  0.462695    0.537305  0.495436  0.014526   \n",
       "6   0.470796   0.477847    0.509321  0.462695    0.537305  0.495436  0.014526   \n",
       "11  0.470084   0.478007    0.508731  0.431966    0.568034  0.494895  0.014526   \n",
       "18  0.469820   0.478355    0.508996  0.441747    0.558253  0.495242  0.014526   \n",
       "19  0.469820   0.478355    0.508996  0.441747    0.558253  0.495242  0.014526   \n",
       "\n",
       "    max_loss  \n",
       "22 -0.013493  \n",
       "21 -0.013493  \n",
       "16 -0.013981  \n",
       "15 -0.013981  \n",
       "2  -0.013493  \n",
       "14 -0.013981  \n",
       "20 -0.013493  \n",
       "8  -0.014526  \n",
       "0  -0.014526  \n",
       "1  -0.014526  \n",
       "13 -0.014526  \n",
       "12 -0.014526  \n",
       "17 -0.013493  \n",
       "23 -0.013981  \n",
       "10 -0.013493  \n",
       "9  -0.013493  \n",
       "5  -0.013493  \n",
       "4  -0.013981  \n",
       "3  -0.013981  \n",
       "7  -0.013493  \n",
       "6  -0.013493  \n",
       "11 -0.013493  \n",
       "18 -0.013493  \n",
       "19 -0.013493  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "    \n",
    "# load rfs\n",
    "rfs = []\n",
    "for i in range(len(hps)):\n",
    "    rf = load(f\"{save_dir}{STOCK}_linear_window_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_liniar_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0xEZU9P5e9Z"
   },
   "source": [
    "We look on all our metrices, but mainly on the `avg_pnl` metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83BZ0O-pcqRP"
   },
   "source": [
    "### 2.2) \"Exponential\" rolling windows\n",
    "\n",
    "The linear windows contain only the most recent data available at the current timestamp.  \n",
    "Maybe having little \"old\" data, at the price of less recent data,  \n",
    "will become handy, and result overall better performance.\n",
    "\n",
    "Instead of taking the last 10 values (prices and volume), like we did in \"linear\" rolling windows,  \n",
    "this time, we will take current and last value, 2nd last, 4th last, 8th last, ... , and 256th last values.  \n",
    "In total, we will get window with size 10, only with different values, from different rows.\n",
    "\n",
    "\n",
    "Below is a visual comparison between \"linear\" window with size 6,  \n",
    "and its \"corresponding\" exponential window, at size 6.\n",
    "\n",
    "<img src=\"exponential_windows.jpeg\" />\n",
    "\n",
    "**windows generation**\n",
    "1. Build each window out of the \"exponential 10\" open, high, low, close and volume values.\n",
    "2. Normalize using min-max normalization.\n",
    "3. Split windows to training windows and validation windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wx2dOhkncqRf",
    "outputId": "3cbcad8c-6039-4409-c5c5-4e37bf652714"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "idx = [0] + [2**i for i in range(window_size-1)]\n",
    "idx = [max(idx)-i for i in idx]\n",
    "idx.reverse()\n",
    "windows_x = df_windows(df.drop(columns=['target']), max(idx)+1)\n",
    "windows_x = windows_x[:,idx,:]\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, max(idx)+1)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ZFir5jR_cqRl",
    "outputId": "1ed9bf9f-c0fa-4e6f-b724-26afcefc2910"
   },
   "outputs": [],
   "source": [
    "# RFs grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "    \n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"{save_dir}{STOCK}_exp_window_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "z-L5YTYlcqRr",
    "outputId": "023f8623-90c2-4f95-eff6-5c5efe81722f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_exponential_23</td>\n",
       "      <td>8.572388e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517101</td>\n",
       "      <td>0.469380</td>\n",
       "      <td>0.479959</td>\n",
       "      <td>0.507420</td>\n",
       "      <td>0.516907</td>\n",
       "      <td>0.483093</td>\n",
       "      <td>0.499158</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_exponential_6</td>\n",
       "      <td>8.488107e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515576</td>\n",
       "      <td>0.470065</td>\n",
       "      <td>0.479077</td>\n",
       "      <td>0.509157</td>\n",
       "      <td>0.508986</td>\n",
       "      <td>0.491014</td>\n",
       "      <td>0.497654</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_exponential_7</td>\n",
       "      <td>8.488107e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515576</td>\n",
       "      <td>0.470065</td>\n",
       "      <td>0.479077</td>\n",
       "      <td>0.509157</td>\n",
       "      <td>0.508986</td>\n",
       "      <td>0.491014</td>\n",
       "      <td>0.497654</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_exponential_22</td>\n",
       "      <td>8.470389e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515565</td>\n",
       "      <td>0.469991</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>0.509170</td>\n",
       "      <td>0.508888</td>\n",
       "      <td>0.491112</td>\n",
       "      <td>0.497682</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_exponential_21</td>\n",
       "      <td>8.470389e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515565</td>\n",
       "      <td>0.469991</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>0.509170</td>\n",
       "      <td>0.508888</td>\n",
       "      <td>0.491112</td>\n",
       "      <td>0.497682</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_exponential_20</td>\n",
       "      <td>8.509218e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516744</td>\n",
       "      <td>0.469047</td>\n",
       "      <td>0.479990</td>\n",
       "      <td>0.508060</td>\n",
       "      <td>0.502582</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.498462</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_exponential_18</td>\n",
       "      <td>8.469841e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516128</td>\n",
       "      <td>0.469727</td>\n",
       "      <td>0.479424</td>\n",
       "      <td>0.508587</td>\n",
       "      <td>0.508819</td>\n",
       "      <td>0.491181</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_exponential_19</td>\n",
       "      <td>8.469841e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516128</td>\n",
       "      <td>0.469727</td>\n",
       "      <td>0.479424</td>\n",
       "      <td>0.508587</td>\n",
       "      <td>0.508819</td>\n",
       "      <td>0.491181</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_exponential_8</td>\n",
       "      <td>8.521704e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516636</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.479761</td>\n",
       "      <td>0.508149</td>\n",
       "      <td>0.503738</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_exponential_11</td>\n",
       "      <td>8.597312e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517201</td>\n",
       "      <td>0.469285</td>\n",
       "      <td>0.480110</td>\n",
       "      <td>0.507267</td>\n",
       "      <td>0.519176</td>\n",
       "      <td>0.480824</td>\n",
       "      <td>0.499367</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_exponential_3</td>\n",
       "      <td>8.436428e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514848</td>\n",
       "      <td>0.471121</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.509971</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.496485</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_exponential_4</td>\n",
       "      <td>8.436428e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514848</td>\n",
       "      <td>0.471121</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.509971</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.496485</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_exponential_2</td>\n",
       "      <td>8.442598e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515575</td>\n",
       "      <td>0.469904</td>\n",
       "      <td>0.479119</td>\n",
       "      <td>0.509244</td>\n",
       "      <td>0.502318</td>\n",
       "      <td>0.497682</td>\n",
       "      <td>0.497432</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_exponential_12</td>\n",
       "      <td>8.434831e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515536</td>\n",
       "      <td>0.470064</td>\n",
       "      <td>0.478964</td>\n",
       "      <td>0.509279</td>\n",
       "      <td>0.502680</td>\n",
       "      <td>0.497320</td>\n",
       "      <td>0.497348</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_exponential_13</td>\n",
       "      <td>8.434831e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515536</td>\n",
       "      <td>0.470064</td>\n",
       "      <td>0.478964</td>\n",
       "      <td>0.509279</td>\n",
       "      <td>0.502680</td>\n",
       "      <td>0.497320</td>\n",
       "      <td>0.497348</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_exponential_17</td>\n",
       "      <td>8.459680e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515498</td>\n",
       "      <td>0.470657</td>\n",
       "      <td>0.478674</td>\n",
       "      <td>0.509069</td>\n",
       "      <td>0.521863</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_exponential_10</td>\n",
       "      <td>8.495306e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.469306</td>\n",
       "      <td>0.479794</td>\n",
       "      <td>0.507787</td>\n",
       "      <td>0.505687</td>\n",
       "      <td>0.494313</td>\n",
       "      <td>0.498587</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_exponential_9</td>\n",
       "      <td>8.495306e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.469306</td>\n",
       "      <td>0.479794</td>\n",
       "      <td>0.507787</td>\n",
       "      <td>0.505687</td>\n",
       "      <td>0.494313</td>\n",
       "      <td>0.498587</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_exponential_5</td>\n",
       "      <td>8.462648e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514850</td>\n",
       "      <td>0.471237</td>\n",
       "      <td>0.477907</td>\n",
       "      <td>0.509876</td>\n",
       "      <td>0.512299</td>\n",
       "      <td>0.487701</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_exponential_1</td>\n",
       "      <td>8.439253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514148</td>\n",
       "      <td>0.471019</td>\n",
       "      <td>0.477939</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.495984</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_exponential_0</td>\n",
       "      <td>8.439253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514148</td>\n",
       "      <td>0.471019</td>\n",
       "      <td>0.477939</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.495984</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_exponential_15</td>\n",
       "      <td>8.435022e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514175</td>\n",
       "      <td>0.471270</td>\n",
       "      <td>0.477864</td>\n",
       "      <td>0.510590</td>\n",
       "      <td>0.511673</td>\n",
       "      <td>0.488327</td>\n",
       "      <td>0.496443</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_exponential_16</td>\n",
       "      <td>8.435022e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514175</td>\n",
       "      <td>0.471270</td>\n",
       "      <td>0.477864</td>\n",
       "      <td>0.510590</td>\n",
       "      <td>0.511673</td>\n",
       "      <td>0.488327</td>\n",
       "      <td>0.496443</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_exponential_14</td>\n",
       "      <td>8.443962e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514112</td>\n",
       "      <td>0.471524</td>\n",
       "      <td>0.477435</td>\n",
       "      <td>0.510749</td>\n",
       "      <td>0.498142</td>\n",
       "      <td>0.501858</td>\n",
       "      <td>0.495705</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "23  rf_exponential_23  8.572388e-07  0.000617  0.000011         0.0  0.517101   \n",
       "6    rf_exponential_6  8.488107e-07  0.000617  0.000010         0.0  0.515576   \n",
       "7    rf_exponential_7  8.488107e-07  0.000617  0.000010         0.0  0.515576   \n",
       "22  rf_exponential_22  8.470389e-07  0.000617  0.000010         0.0  0.515565   \n",
       "21  rf_exponential_21  8.470389e-07  0.000617  0.000010         0.0  0.515565   \n",
       "20  rf_exponential_20  8.509218e-07  0.000617  0.000009         0.0  0.516744   \n",
       "18  rf_exponential_18  8.469841e-07  0.000617  0.000009         0.0  0.516128   \n",
       "19  rf_exponential_19  8.469841e-07  0.000617  0.000009         0.0  0.516128   \n",
       "8    rf_exponential_8  8.521704e-07  0.000617  0.000008         0.0  0.516636   \n",
       "11  rf_exponential_11  8.597312e-07  0.000617  0.000007         0.0  0.517201   \n",
       "3    rf_exponential_3  8.436428e-07  0.000617  0.000007         0.0  0.514848   \n",
       "4    rf_exponential_4  8.436428e-07  0.000617  0.000007         0.0  0.514848   \n",
       "2    rf_exponential_2  8.442598e-07  0.000617  0.000006         0.0  0.515575   \n",
       "12  rf_exponential_12  8.434831e-07  0.000617  0.000006         0.0  0.515536   \n",
       "13  rf_exponential_13  8.434831e-07  0.000617  0.000006         0.0  0.515536   \n",
       "17  rf_exponential_17  8.459680e-07  0.000617  0.000006         0.0  0.515498   \n",
       "10  rf_exponential_10  8.495306e-07  0.000617  0.000005         0.0  0.516958   \n",
       "9    rf_exponential_9  8.495306e-07  0.000617  0.000005         0.0  0.516958   \n",
       "5    rf_exponential_5  8.462648e-07  0.000617  0.000005         0.0  0.514850   \n",
       "1    rf_exponential_1  8.439253e-07  0.000617  0.000005         0.0  0.514148   \n",
       "0    rf_exponential_0  8.439253e-07  0.000617  0.000005         0.0  0.514148   \n",
       "15  rf_exponential_15  8.435022e-07  0.000617  0.000004         0.0  0.514175   \n",
       "16  rf_exponential_16  8.435022e-07  0.000617  0.000004         0.0  0.514175   \n",
       "14  rf_exponential_14  8.443962e-07  0.000617  0.000003         0.0  0.514112   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "23  0.469380   0.479959    0.507420  0.516907    0.483093  0.499158  0.013493   \n",
       "6   0.470065   0.479077    0.509157  0.508986    0.491014  0.497654  0.011996   \n",
       "7   0.470065   0.479077    0.509157  0.508986    0.491014  0.497654  0.011996   \n",
       "22  0.469991   0.479152    0.509170  0.508888    0.491112  0.497682  0.011996   \n",
       "21  0.469991   0.479152    0.509170  0.508888    0.491112  0.497682  0.011996   \n",
       "20  0.469047   0.479990    0.508060  0.502582    0.497418  0.498462  0.012759   \n",
       "18  0.469727   0.479424    0.508587  0.508819    0.491181  0.498100  0.013493   \n",
       "19  0.469727   0.479424    0.508587  0.508819    0.491181  0.498100  0.013493   \n",
       "8   0.469298   0.479761    0.508149  0.503738    0.496262  0.498336  0.012759   \n",
       "11  0.469285   0.480110    0.507267  0.519176    0.480824  0.499367  0.013493   \n",
       "3   0.471121   0.477900    0.509971  0.503000    0.497000  0.496485  0.012759   \n",
       "4   0.471121   0.477900    0.509971  0.503000    0.497000  0.496485  0.012759   \n",
       "2   0.469904   0.479119    0.509244  0.502318    0.497682  0.497432  0.011996   \n",
       "12  0.470064   0.478964    0.509279  0.502680    0.497320  0.497348  0.011996   \n",
       "13  0.470064   0.478964    0.509279  0.502680    0.497320  0.497348  0.011996   \n",
       "17  0.470657   0.478674    0.509069  0.521863    0.478137  0.497891  0.012759   \n",
       "10  0.469306   0.479794    0.507787  0.505687    0.494313  0.498587  0.011996   \n",
       "9   0.469306   0.479794    0.507787  0.505687    0.494313  0.498587  0.011996   \n",
       "5   0.471237   0.477907    0.509876  0.512299    0.487701  0.496833  0.012759   \n",
       "1   0.471019   0.477939    0.510712  0.498350    0.501650  0.495984  0.012759   \n",
       "0   0.471019   0.477939    0.510712  0.498350    0.501650  0.495984  0.012759   \n",
       "15  0.471270   0.477864    0.510590  0.511673    0.488327  0.496443  0.012759   \n",
       "16  0.471270   0.477864    0.510590  0.511673    0.488327  0.496443  0.012759   \n",
       "14  0.471524   0.477435    0.510749  0.498142    0.501858  0.495705  0.012231   \n",
       "\n",
       "    max_loss  \n",
       "23 -0.014526  \n",
       "6  -0.014526  \n",
       "7  -0.014526  \n",
       "22 -0.014526  \n",
       "21 -0.014526  \n",
       "20 -0.014526  \n",
       "18 -0.014526  \n",
       "19 -0.014526  \n",
       "8  -0.014526  \n",
       "11 -0.014526  \n",
       "3  -0.014526  \n",
       "4  -0.014526  \n",
       "2  -0.014526  \n",
       "12 -0.014526  \n",
       "13 -0.014526  \n",
       "17 -0.014526  \n",
       "10 -0.014526  \n",
       "9  -0.014526  \n",
       "5  -0.014526  \n",
       "1  -0.014526  \n",
       "0  -0.014526  \n",
       "15 -0.014526  \n",
       "16 -0.014526  \n",
       "14 -0.014526  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "# load rfs\n",
    "rfs = []\n",
    "for i in range(len(hps)):\n",
    "    rf = load(f\"{save_dir}{STOCK}_exp_window_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_exponential_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7-tWjQu5AXf"
   },
   "source": [
    "Looking at the `avg_pnl` metric, linear windows are considerably better than exponential windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfDSdApIC3fE"
   },
   "source": [
    "### 2.3) \"Linear\" rolling windows - no open / high / low prices\n",
    "Our data is composed of 4 prices at any given data point - open, high, low and close price.  \n",
    "\n",
    "Theoretically, given enough data and enough computing power,  \n",
    "this additional information on stock's price might add to the overall model's performance.  \n",
    "\n",
    "However, this might not be our case -  \n",
    "we got limited amount of data, memory, and computing power.  \n",
    "Therefore, droping the highly correlated, relatively redundent data we got on stock's price at each timestamp,  \n",
    "might increase our model's overall performance.\n",
    "\n",
    "So, instead of using our original data,  \n",
    "this time, we will only use the close prices and the stock's volume.\n",
    "\n",
    "**Windows generation**\n",
    "1. build each window out of the \"linear 10\" ~~open, high, low,~~ close and volume values. \n",
    "2. normalize using min-max normalization.\n",
    "3. split windows to training windows and validation windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "02fEOiop4XIA",
    "outputId": "3d9fa1b8-a5fb-4dcf-fb86-9d45864fdfff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 2.56 s, total: 1min 56s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "adj_df = df.drop(columns=['open', 'high', 'low'])\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(adj_df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(adj_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(adj_df[adj_df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "kprYG39-4Z6O",
    "outputId": "fb45a4f3-035a-45d3-8f51-30cc0a8b9d73"
   },
   "outputs": [],
   "source": [
    "# RFs grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "    \n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"{save_dir}{STOCK}_linear_close_only_window_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "SA8hGYdn6l4m",
    "outputId": "f1f7a4f4-a802-44be-e081-7367cfd5f337"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_no_OHL_12</td>\n",
       "      <td>8.408084e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524944</td>\n",
       "      <td>0.461295</td>\n",
       "      <td>0.483087</td>\n",
       "      <td>0.504288</td>\n",
       "      <td>0.390153</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.499417</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_no_OHL_13</td>\n",
       "      <td>8.408084e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524944</td>\n",
       "      <td>0.461295</td>\n",
       "      <td>0.483087</td>\n",
       "      <td>0.504288</td>\n",
       "      <td>0.390153</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.499417</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_no_OHL_2</td>\n",
       "      <td>8.410160e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524681</td>\n",
       "      <td>0.461650</td>\n",
       "      <td>0.482635</td>\n",
       "      <td>0.504670</td>\n",
       "      <td>0.383633</td>\n",
       "      <td>0.616367</td>\n",
       "      <td>0.498765</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_no_OHL_16</td>\n",
       "      <td>8.408064e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524147</td>\n",
       "      <td>0.462203</td>\n",
       "      <td>0.482409</td>\n",
       "      <td>0.504890</td>\n",
       "      <td>0.387226</td>\n",
       "      <td>0.612774</td>\n",
       "      <td>0.498571</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_no_OHL_15</td>\n",
       "      <td>8.408064e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524147</td>\n",
       "      <td>0.462203</td>\n",
       "      <td>0.482409</td>\n",
       "      <td>0.504890</td>\n",
       "      <td>0.387226</td>\n",
       "      <td>0.612774</td>\n",
       "      <td>0.498571</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_no_OHL_1</td>\n",
       "      <td>8.410912e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522872</td>\n",
       "      <td>0.463939</td>\n",
       "      <td>0.481673</td>\n",
       "      <td>0.505339</td>\n",
       "      <td>0.399711</td>\n",
       "      <td>0.600289</td>\n",
       "      <td>0.498141</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_no_OHL_0</td>\n",
       "      <td>8.410912e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522872</td>\n",
       "      <td>0.463939</td>\n",
       "      <td>0.481673</td>\n",
       "      <td>0.505339</td>\n",
       "      <td>0.399711</td>\n",
       "      <td>0.600289</td>\n",
       "      <td>0.498141</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_no_OHL_23</td>\n",
       "      <td>8.444171e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519072</td>\n",
       "      <td>0.468664</td>\n",
       "      <td>0.479280</td>\n",
       "      <td>0.507015</td>\n",
       "      <td>0.442274</td>\n",
       "      <td>0.557726</td>\n",
       "      <td>0.496879</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_no_OHL_20</td>\n",
       "      <td>8.419032e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520434</td>\n",
       "      <td>0.466325</td>\n",
       "      <td>0.480966</td>\n",
       "      <td>0.506099</td>\n",
       "      <td>0.435865</td>\n",
       "      <td>0.564135</td>\n",
       "      <td>0.498169</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_no_OHL_22</td>\n",
       "      <td>8.416908e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518914</td>\n",
       "      <td>0.467753</td>\n",
       "      <td>0.479978</td>\n",
       "      <td>0.507162</td>\n",
       "      <td>0.441178</td>\n",
       "      <td>0.558822</td>\n",
       "      <td>0.497156</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_no_OHL_21</td>\n",
       "      <td>8.416908e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518914</td>\n",
       "      <td>0.467753</td>\n",
       "      <td>0.479978</td>\n",
       "      <td>0.507162</td>\n",
       "      <td>0.441178</td>\n",
       "      <td>0.558822</td>\n",
       "      <td>0.497156</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_no_OHL_14</td>\n",
       "      <td>8.409072e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523827</td>\n",
       "      <td>0.462511</td>\n",
       "      <td>0.482073</td>\n",
       "      <td>0.505226</td>\n",
       "      <td>0.382828</td>\n",
       "      <td>0.617172</td>\n",
       "      <td>0.498058</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_no_OHL_17</td>\n",
       "      <td>8.414306e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521114</td>\n",
       "      <td>0.465008</td>\n",
       "      <td>0.480834</td>\n",
       "      <td>0.506626</td>\n",
       "      <td>0.394884</td>\n",
       "      <td>0.605116</td>\n",
       "      <td>0.496740</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_no_OHL_5</td>\n",
       "      <td>8.413644e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521048</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.480473</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.387240</td>\n",
       "      <td>0.612760</td>\n",
       "      <td>0.496185</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_no_OHL_3</td>\n",
       "      <td>8.410523e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521823</td>\n",
       "      <td>0.464614</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.506191</td>\n",
       "      <td>0.393815</td>\n",
       "      <td>0.606185</td>\n",
       "      <td>0.497114</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_no_OHL_4</td>\n",
       "      <td>8.410523e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521823</td>\n",
       "      <td>0.464614</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.506191</td>\n",
       "      <td>0.393815</td>\n",
       "      <td>0.606185</td>\n",
       "      <td>0.497114</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_no_OHL_10</td>\n",
       "      <td>8.429197e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518321</td>\n",
       "      <td>0.468313</td>\n",
       "      <td>0.479878</td>\n",
       "      <td>0.507304</td>\n",
       "      <td>0.457743</td>\n",
       "      <td>0.542257</td>\n",
       "      <td>0.497475</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_no_OHL_9</td>\n",
       "      <td>8.429197e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518321</td>\n",
       "      <td>0.468313</td>\n",
       "      <td>0.479878</td>\n",
       "      <td>0.507304</td>\n",
       "      <td>0.457743</td>\n",
       "      <td>0.542257</td>\n",
       "      <td>0.497475</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_no_OHL_8</td>\n",
       "      <td>8.426087e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518821</td>\n",
       "      <td>0.467687</td>\n",
       "      <td>0.480120</td>\n",
       "      <td>0.507152</td>\n",
       "      <td>0.445215</td>\n",
       "      <td>0.554785</td>\n",
       "      <td>0.497350</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_no_OHL_18</td>\n",
       "      <td>8.415941e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518801</td>\n",
       "      <td>0.467671</td>\n",
       "      <td>0.480061</td>\n",
       "      <td>0.507235</td>\n",
       "      <td>0.441997</td>\n",
       "      <td>0.558003</td>\n",
       "      <td>0.497184</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_no_OHL_19</td>\n",
       "      <td>8.415941e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518801</td>\n",
       "      <td>0.467671</td>\n",
       "      <td>0.480061</td>\n",
       "      <td>0.507235</td>\n",
       "      <td>0.441997</td>\n",
       "      <td>0.558003</td>\n",
       "      <td>0.497184</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_no_OHL_11</td>\n",
       "      <td>8.447146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515914</td>\n",
       "      <td>0.471807</td>\n",
       "      <td>0.476843</td>\n",
       "      <td>0.509446</td>\n",
       "      <td>0.448531</td>\n",
       "      <td>0.551469</td>\n",
       "      <td>0.494368</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_no_OHL_6</td>\n",
       "      <td>8.428963e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.470065</td>\n",
       "      <td>0.478456</td>\n",
       "      <td>0.508667</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>0.538650</td>\n",
       "      <td>0.496074</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_no_OHL_7</td>\n",
       "      <td>8.428963e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.470065</td>\n",
       "      <td>0.478456</td>\n",
       "      <td>0.508667</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>0.538650</td>\n",
       "      <td>0.496074</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "12  rf_no_OHL_12  8.408084e-07  0.000617  0.000015         0.0  0.524944   \n",
       "13  rf_no_OHL_13  8.408084e-07  0.000617  0.000015         0.0  0.524944   \n",
       "2    rf_no_OHL_2  8.410160e-07  0.000617  0.000015         0.0  0.524681   \n",
       "16  rf_no_OHL_16  8.408064e-07  0.000617  0.000013         0.0  0.524147   \n",
       "15  rf_no_OHL_15  8.408064e-07  0.000617  0.000013         0.0  0.524147   \n",
       "1    rf_no_OHL_1  8.410912e-07  0.000617  0.000013         0.0  0.522872   \n",
       "0    rf_no_OHL_0  8.410912e-07  0.000617  0.000013         0.0  0.522872   \n",
       "23  rf_no_OHL_23  8.444171e-07  0.000617  0.000013         0.0  0.519072   \n",
       "20  rf_no_OHL_20  8.419032e-07  0.000617  0.000013         0.0  0.520434   \n",
       "22  rf_no_OHL_22  8.416908e-07  0.000617  0.000012         0.0  0.518914   \n",
       "21  rf_no_OHL_21  8.416908e-07  0.000617  0.000012         0.0  0.518914   \n",
       "14  rf_no_OHL_14  8.409072e-07  0.000617  0.000012         0.0  0.523827   \n",
       "17  rf_no_OHL_17  8.414306e-07  0.000617  0.000012         0.0  0.521114   \n",
       "5    rf_no_OHL_5  8.413644e-07  0.000617  0.000011         0.0  0.521048   \n",
       "3    rf_no_OHL_3  8.410523e-07  0.000617  0.000011         0.0  0.521823   \n",
       "4    rf_no_OHL_4  8.410523e-07  0.000617  0.000011         0.0  0.521823   \n",
       "10  rf_no_OHL_10  8.429197e-07  0.000617  0.000011         0.0  0.518321   \n",
       "9    rf_no_OHL_9  8.429197e-07  0.000617  0.000011         0.0  0.518321   \n",
       "8    rf_no_OHL_8  8.426087e-07  0.000617  0.000010         0.0  0.518821   \n",
       "18  rf_no_OHL_18  8.415941e-07  0.000617  0.000010         0.0  0.518801   \n",
       "19  rf_no_OHL_19  8.415941e-07  0.000617  0.000010         0.0  0.518801   \n",
       "11  rf_no_OHL_11  8.447146e-07  0.000617  0.000009         0.0  0.515914   \n",
       "6    rf_no_OHL_6  8.428963e-07  0.000617  0.000006         0.0  0.516644   \n",
       "7    rf_no_OHL_7  8.428963e-07  0.000617  0.000006         0.0  0.516644   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "12  0.461295   0.483087    0.504288  0.390153    0.609847  0.499417  0.014526   \n",
       "13  0.461295   0.483087    0.504288  0.390153    0.609847  0.499417  0.014526   \n",
       "2   0.461650   0.482635    0.504670  0.383633    0.616367  0.498765  0.014526   \n",
       "16  0.462203   0.482409    0.504890  0.387226    0.612774  0.498571  0.014526   \n",
       "15  0.462203   0.482409    0.504890  0.387226    0.612774  0.498571  0.014526   \n",
       "1   0.463939   0.481673    0.505339  0.399711    0.600289  0.498141  0.014526   \n",
       "0   0.463939   0.481673    0.505339  0.399711    0.600289  0.498141  0.014526   \n",
       "23  0.468664   0.479280    0.507015  0.442274    0.557726  0.496879  0.014526   \n",
       "20  0.466325   0.480966    0.506099  0.435865    0.564135  0.498169  0.014526   \n",
       "22  0.467753   0.479978    0.507162  0.441178    0.558822  0.497156  0.014526   \n",
       "21  0.467753   0.479978    0.507162  0.441178    0.558822  0.497156  0.014526   \n",
       "14  0.462511   0.482073    0.505226  0.382828    0.617172  0.498058  0.014526   \n",
       "17  0.465008   0.480834    0.506626  0.394884    0.605116  0.496740  0.014526   \n",
       "5   0.465267   0.480473    0.506849  0.387240    0.612760  0.496185  0.014526   \n",
       "3   0.464614   0.481062    0.506191  0.393815    0.606185  0.497114  0.013981   \n",
       "4   0.464614   0.481062    0.506191  0.393815    0.606185  0.497114  0.013981   \n",
       "10  0.468313   0.479878    0.507304  0.457743    0.542257  0.497475  0.014526   \n",
       "9   0.468313   0.479878    0.507304  0.457743    0.542257  0.497475  0.014526   \n",
       "8   0.467687   0.480120    0.507152  0.445215    0.554785  0.497350  0.014526   \n",
       "18  0.467671   0.480061    0.507235  0.441997    0.558003  0.497184  0.014526   \n",
       "19  0.467671   0.480061    0.507235  0.441997    0.558003  0.497184  0.014526   \n",
       "11  0.471807   0.476843    0.509446  0.448531    0.551469  0.494368  0.014526   \n",
       "6   0.470065   0.478456    0.508667  0.461350    0.538650  0.496074  0.013981   \n",
       "7   0.470065   0.478456    0.508667  0.461350    0.538650  0.496074  0.013981   \n",
       "\n",
       "    max_loss  \n",
       "12 -0.013981  \n",
       "13 -0.013981  \n",
       "2  -0.013493  \n",
       "16 -0.013981  \n",
       "15 -0.013981  \n",
       "1  -0.013493  \n",
       "0  -0.013493  \n",
       "23 -0.013493  \n",
       "20 -0.013493  \n",
       "22 -0.013493  \n",
       "21 -0.013493  \n",
       "14 -0.013493  \n",
       "17 -0.013493  \n",
       "5  -0.013493  \n",
       "3  -0.014526  \n",
       "4  -0.014526  \n",
       "10 -0.013493  \n",
       "9  -0.013493  \n",
       "8  -0.012759  \n",
       "18 -0.013493  \n",
       "19 -0.013493  \n",
       "11 -0.013981  \n",
       "6  -0.014526  \n",
       "7  -0.014526  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "# load rfs\n",
    "rfs = []\n",
    "for i in range(len(hps)):\n",
    "    rf = load(f\"{save_dir}{STOCK}_linear_close_only_window_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_no_OHL_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgDFfCyr8zl_"
   },
   "source": [
    "We do see a little worse performace in terms of `avg_pnl`,  \n",
    "and yet the results are relatively good, \n",
    "and we might consider using `close` only prices,  \n",
    "in case we encounter with memory issues / too long of convergence time when modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9y5NL1ODMY8"
   },
   "source": [
    "### 2.4) \"Linear\" rolling windows - log returns\n",
    "\n",
    "We keep the same simple linear rolling windows method,  \n",
    "however, this time, we apply log to our target value - the next 10 minutes returns.  \n",
    "\n",
    "As we can see [here](https://investmentcache.com/magic-of-log-returns-concept-part-1/#:~:text=The%20logarithm%20of%20a%20number,P0%20to%20Pt.), it is common practice to use log returns,  \n",
    "instead of the actual returns, duo to the logarithm mathematical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qRKlQhkTCQH4",
    "outputId": "87b432ea-36f2-4ef7-a441-365654ec980d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 2.39 s, total: 1min 54s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# apply log\n",
    "adj_df['target'] = np.log(adj_df['target'] + 1)\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(adj_df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(adj_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(adj_df[adj_df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "# apply exponent to the validation target - bring it back to original targets\n",
    "valid_y = np.exp(valid_y) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "e3HXTOZgCZC3",
    "outputId": "4391ebe1-a6a4-45e5-ba0c-6b18f4f13947"
   },
   "outputs": [],
   "source": [
    "# RFs grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "    \n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"{save_dir}{STOCK}_linear_log_window_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "Fm1u7BHcC1tS",
    "outputId": "0fd73d7f-d4ff-4d38-bc1c-6b387f0aec09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_log_returns_20</td>\n",
       "      <td>8.418825e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522610</td>\n",
       "      <td>0.464370</td>\n",
       "      <td>0.482009</td>\n",
       "      <td>0.504887</td>\n",
       "      <td>0.420910</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.499098</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_log_returns_12</td>\n",
       "      <td>8.407905e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>0.459787</td>\n",
       "      <td>0.483340</td>\n",
       "      <td>0.503897</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_log_returns_13</td>\n",
       "      <td>8.407905e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>0.459787</td>\n",
       "      <td>0.483340</td>\n",
       "      <td>0.503897</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_log_returns_2</td>\n",
       "      <td>8.409924e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525210</td>\n",
       "      <td>0.461243</td>\n",
       "      <td>0.482478</td>\n",
       "      <td>0.504736</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.498363</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_log_returns_16</td>\n",
       "      <td>8.408344e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523649</td>\n",
       "      <td>0.462886</td>\n",
       "      <td>0.481451</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.369870</td>\n",
       "      <td>0.630130</td>\n",
       "      <td>0.497059</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_log_returns_15</td>\n",
       "      <td>8.408344e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523649</td>\n",
       "      <td>0.462886</td>\n",
       "      <td>0.481451</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.369870</td>\n",
       "      <td>0.630130</td>\n",
       "      <td>0.497059</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_log_returns_22</td>\n",
       "      <td>8.416200e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520572</td>\n",
       "      <td>0.466359</td>\n",
       "      <td>0.480787</td>\n",
       "      <td>0.506145</td>\n",
       "      <td>0.429913</td>\n",
       "      <td>0.570087</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_log_returns_21</td>\n",
       "      <td>8.416200e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520572</td>\n",
       "      <td>0.466359</td>\n",
       "      <td>0.480787</td>\n",
       "      <td>0.506145</td>\n",
       "      <td>0.429913</td>\n",
       "      <td>0.570087</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_log_returns_1</td>\n",
       "      <td>8.410978e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522884</td>\n",
       "      <td>0.464118</td>\n",
       "      <td>0.481086</td>\n",
       "      <td>0.505802</td>\n",
       "      <td>0.383147</td>\n",
       "      <td>0.616853</td>\n",
       "      <td>0.497101</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_log_returns_0</td>\n",
       "      <td>8.410978e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522884</td>\n",
       "      <td>0.464118</td>\n",
       "      <td>0.481086</td>\n",
       "      <td>0.505802</td>\n",
       "      <td>0.383147</td>\n",
       "      <td>0.616853</td>\n",
       "      <td>0.497101</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_log_returns_11</td>\n",
       "      <td>8.443098e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519287</td>\n",
       "      <td>0.467844</td>\n",
       "      <td>0.479808</td>\n",
       "      <td>0.506969</td>\n",
       "      <td>0.436614</td>\n",
       "      <td>0.563386</td>\n",
       "      <td>0.497045</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_log_returns_14</td>\n",
       "      <td>8.409108e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522989</td>\n",
       "      <td>0.463363</td>\n",
       "      <td>0.481233</td>\n",
       "      <td>0.506042</td>\n",
       "      <td>0.372035</td>\n",
       "      <td>0.627965</td>\n",
       "      <td>0.496768</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_log_returns_17</td>\n",
       "      <td>8.414069e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521899</td>\n",
       "      <td>0.464431</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.506299</td>\n",
       "      <td>0.387697</td>\n",
       "      <td>0.612303</td>\n",
       "      <td>0.496865</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_log_returns_5</td>\n",
       "      <td>8.413286e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521778</td>\n",
       "      <td>0.464474</td>\n",
       "      <td>0.480873</td>\n",
       "      <td>0.506481</td>\n",
       "      <td>0.383480</td>\n",
       "      <td>0.616520</td>\n",
       "      <td>0.496559</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_log_returns_8</td>\n",
       "      <td>8.425229e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520320</td>\n",
       "      <td>0.466443</td>\n",
       "      <td>0.480824</td>\n",
       "      <td>0.506237</td>\n",
       "      <td>0.433881</td>\n",
       "      <td>0.566119</td>\n",
       "      <td>0.497961</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_log_returns_18</td>\n",
       "      <td>8.416170e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518752</td>\n",
       "      <td>0.468134</td>\n",
       "      <td>0.479462</td>\n",
       "      <td>0.507504</td>\n",
       "      <td>0.430565</td>\n",
       "      <td>0.569435</td>\n",
       "      <td>0.496379</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_log_returns_19</td>\n",
       "      <td>8.416170e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518752</td>\n",
       "      <td>0.468134</td>\n",
       "      <td>0.479462</td>\n",
       "      <td>0.507504</td>\n",
       "      <td>0.430565</td>\n",
       "      <td>0.569435</td>\n",
       "      <td>0.496379</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_log_returns_23</td>\n",
       "      <td>8.446253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519794</td>\n",
       "      <td>0.467042</td>\n",
       "      <td>0.480177</td>\n",
       "      <td>0.506825</td>\n",
       "      <td>0.425779</td>\n",
       "      <td>0.574221</td>\n",
       "      <td>0.497045</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_log_returns_4</td>\n",
       "      <td>8.410760e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522179</td>\n",
       "      <td>0.464499</td>\n",
       "      <td>0.480660</td>\n",
       "      <td>0.506425</td>\n",
       "      <td>0.375933</td>\n",
       "      <td>0.624067</td>\n",
       "      <td>0.496268</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_log_returns_3</td>\n",
       "      <td>8.410760e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522179</td>\n",
       "      <td>0.464499</td>\n",
       "      <td>0.480660</td>\n",
       "      <td>0.506425</td>\n",
       "      <td>0.375933</td>\n",
       "      <td>0.624067</td>\n",
       "      <td>0.496268</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_log_returns_7</td>\n",
       "      <td>8.429588e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517627</td>\n",
       "      <td>0.468978</td>\n",
       "      <td>0.479278</td>\n",
       "      <td>0.507927</td>\n",
       "      <td>0.455703</td>\n",
       "      <td>0.544297</td>\n",
       "      <td>0.496754</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_log_returns_6</td>\n",
       "      <td>8.429588e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517627</td>\n",
       "      <td>0.468978</td>\n",
       "      <td>0.479278</td>\n",
       "      <td>0.507927</td>\n",
       "      <td>0.455703</td>\n",
       "      <td>0.544297</td>\n",
       "      <td>0.496754</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_log_returns_10</td>\n",
       "      <td>8.427810e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516427</td>\n",
       "      <td>0.470733</td>\n",
       "      <td>0.477743</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.549444</td>\n",
       "      <td>0.495172</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_log_returns_9</td>\n",
       "      <td>8.427810e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516427</td>\n",
       "      <td>0.470733</td>\n",
       "      <td>0.477743</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.549444</td>\n",
       "      <td>0.495172</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "20  rf_log_returns_20  8.418825e-07  0.000617  0.000016         0.0  0.522610   \n",
       "12  rf_log_returns_12  8.407905e-07  0.000617  0.000015         0.0  0.526628   \n",
       "13  rf_log_returns_13  8.407905e-07  0.000617  0.000015         0.0  0.526628   \n",
       "2    rf_log_returns_2  8.409924e-07  0.000617  0.000013         0.0  0.525210   \n",
       "16  rf_log_returns_16  8.408344e-07  0.000617  0.000013         0.0  0.523649   \n",
       "15  rf_log_returns_15  8.408344e-07  0.000617  0.000013         0.0  0.523649   \n",
       "22  rf_log_returns_22  8.416200e-07  0.000617  0.000012         0.0  0.520572   \n",
       "21  rf_log_returns_21  8.416200e-07  0.000617  0.000012         0.0  0.520572   \n",
       "1    rf_log_returns_1  8.410978e-07  0.000617  0.000012         0.0  0.522884   \n",
       "0    rf_log_returns_0  8.410978e-07  0.000617  0.000012         0.0  0.522884   \n",
       "11  rf_log_returns_11  8.443098e-07  0.000617  0.000011         0.0  0.519287   \n",
       "14  rf_log_returns_14  8.409108e-07  0.000617  0.000011         0.0  0.522989   \n",
       "17  rf_log_returns_17  8.414069e-07  0.000617  0.000011         0.0  0.521899   \n",
       "5    rf_log_returns_5  8.413286e-07  0.000617  0.000011         0.0  0.521778   \n",
       "8    rf_log_returns_8  8.425229e-07  0.000617  0.000011         0.0  0.520320   \n",
       "18  rf_log_returns_18  8.416170e-07  0.000617  0.000011         0.0  0.518752   \n",
       "19  rf_log_returns_19  8.416170e-07  0.000617  0.000011         0.0  0.518752   \n",
       "23  rf_log_returns_23  8.446253e-07  0.000617  0.000011         0.0  0.519794   \n",
       "4    rf_log_returns_4  8.410760e-07  0.000617  0.000010         0.0  0.522179   \n",
       "3    rf_log_returns_3  8.410760e-07  0.000617  0.000010         0.0  0.522179   \n",
       "7    rf_log_returns_7  8.429588e-07  0.000617  0.000009         0.0  0.517627   \n",
       "6    rf_log_returns_6  8.429588e-07  0.000617  0.000009         0.0  0.517627   \n",
       "10  rf_log_returns_10  8.427810e-07  0.000617  0.000005         0.0  0.516427   \n",
       "9    rf_log_returns_9  8.427810e-07  0.000617  0.000005         0.0  0.516427   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "20  0.464370   0.482009    0.504887  0.420910    0.579090  0.499098  0.014526   \n",
       "12  0.459787   0.483340    0.503897  0.371729    0.628271  0.499431  0.014526   \n",
       "13  0.459787   0.483340    0.503897  0.371729    0.628271  0.499431  0.014526   \n",
       "2   0.461243   0.482478    0.504736  0.371729    0.628271  0.498363  0.014526   \n",
       "16  0.462886   0.481451    0.505713  0.369870    0.630130  0.497059  0.014526   \n",
       "15  0.462886   0.481451    0.505713  0.369870    0.630130  0.497059  0.014526   \n",
       "22  0.466359   0.480787    0.506145  0.429913    0.570087  0.497891  0.014526   \n",
       "21  0.466359   0.480787    0.506145  0.429913    0.570087  0.497891  0.014526   \n",
       "1   0.464118   0.481086    0.505802  0.383147    0.616853  0.497101  0.014526   \n",
       "0   0.464118   0.481086    0.505802  0.383147    0.616853  0.497101  0.014526   \n",
       "11  0.467844   0.479808    0.506969  0.436614    0.563386  0.497045  0.014526   \n",
       "14  0.463363   0.481233    0.506042  0.372035    0.627965  0.496768  0.014526   \n",
       "17  0.464431   0.481013    0.506299  0.387697    0.612303  0.496865  0.014526   \n",
       "5   0.464474   0.480873    0.506481  0.383480    0.616520  0.496559  0.014526   \n",
       "8   0.466443   0.480824    0.506237  0.433881    0.566119  0.497961  0.014526   \n",
       "18  0.468134   0.479462    0.507504  0.430565    0.569435  0.496379  0.014526   \n",
       "19  0.468134   0.479462    0.507504  0.430565    0.569435  0.496379  0.014526   \n",
       "23  0.467042   0.480177    0.506825  0.425779    0.574221  0.497045  0.014526   \n",
       "4   0.464499   0.480660    0.506425  0.375933    0.624067  0.496268  0.013981   \n",
       "3   0.464499   0.480660    0.506425  0.375933    0.624067  0.496268  0.013981   \n",
       "7   0.468978   0.479278    0.507927  0.455703    0.544297  0.496754  0.013981   \n",
       "6   0.468978   0.479278    0.507927  0.455703    0.544297  0.496754  0.013981   \n",
       "10  0.470733   0.477743    0.509001  0.450556    0.549444  0.495172  0.014526   \n",
       "9   0.470733   0.477743    0.509001  0.450556    0.549444  0.495172  0.014526   \n",
       "\n",
       "    max_loss  \n",
       "20 -0.013493  \n",
       "12 -0.013981  \n",
       "13 -0.013981  \n",
       "2  -0.013493  \n",
       "16 -0.013493  \n",
       "15 -0.013493  \n",
       "22 -0.013493  \n",
       "21 -0.013493  \n",
       "1  -0.013493  \n",
       "0  -0.013493  \n",
       "11 -0.013493  \n",
       "14 -0.013493  \n",
       "17 -0.013493  \n",
       "5  -0.013493  \n",
       "8  -0.013981  \n",
       "18 -0.013493  \n",
       "19 -0.013493  \n",
       "23 -0.013493  \n",
       "4  -0.014526  \n",
       "3  -0.014526  \n",
       "7  -0.014526  \n",
       "6  -0.014526  \n",
       "10 -0.012759  \n",
       "9  -0.012759  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "# load rfs\n",
    "rfs = []\n",
    "for i in range(len(hps)):\n",
    "    rf = load(f\"{save_dir}{STOCK}_linear_log_window_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "\n",
    "    # apply exponent to the prediction - bring it back to original targets\n",
    "    prediction = np.exp(prediction) - 1\n",
    "    \n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_log_returns_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4qov618_Nlb"
   },
   "source": [
    "Very similar results to the simple linear windows, without the log returns.  \n",
    "Maybe its because the returns are relatively very small,  \n",
    "so the log operation is neglectable, we are not sure though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30Kc2rMRDSRR"
   },
   "source": [
    "### 2.5) \"Linear\" rolling windows - time data\n",
    "\n",
    "The stock market, like many other time series data, might exhibit properties of seasonality.  \n",
    "Therefore, we will add time data to each window:\n",
    "1. current minute\n",
    "2. current hour\n",
    "3. currrnt day\n",
    "4. current month\n",
    "5. current minute of day\n",
    "6. current day of week\n",
    "\n",
    "**Windows generation:**\n",
    "1. Build each window out of last 10 open, high, low, close and volume values, along with time data.\n",
    "2. Normalize using min-max normalization.\n",
    "3. Split windows to training windows and validation windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZjjV67kFgQY"
   },
   "outputs": [],
   "source": [
    "def add_times(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" add time attributes to dataframe \"\"\"\n",
    "\n",
    "    df['minute'] = df.index.minute\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['minute_of_day'] = df['minute'] + df['hour'] * 60\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    return df\n",
    "\n",
    "# adjust DF\n",
    "adj_df = df.copy()\n",
    "add_times(adj_df)\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "win_cols = ['open','high','low','close','volume']\n",
    "windows_x = df_windows(adj_df[win_cols], window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(adj_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# add times to windows\n",
    "start_i = adj_df.shape[0] - windows_x.shape[0]\n",
    "windows_x = np.concatenate([windows_x,adj_df.drop(columns=['target']).values[start_i:,:]],axis=1)\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(adj_df[adj_df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEeLTspca9k5"
   },
   "outputs": [],
   "source": [
    "# RFs grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i} / {len(hps)}\")\n",
    "    \n",
    "    # create RF\n",
    "    rf = RandomForestRegressor(**hp)\n",
    "    rf.fit(train_x, train_y)\n",
    "\n",
    "    # save data and RFs\n",
    "    dump(rf, f\"{save_dir}{STOCK}_linear_time_window_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "EbJ47QSXbDhm",
    "outputId": "a50204a4-03a1-4849-ad88-241d0705fc66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_time_data_19</td>\n",
       "      <td>8.460822e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.237267e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.515503</td>\n",
       "      <td>0.473091</td>\n",
       "      <td>0.474965</td>\n",
       "      <td>0.511542</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.796870</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_time_data_18</td>\n",
       "      <td>8.460822e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.237267e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.515503</td>\n",
       "      <td>0.473091</td>\n",
       "      <td>0.474965</td>\n",
       "      <td>0.511542</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.796870</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_time_data_16</td>\n",
       "      <td>8.413158e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.033521e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.524755</td>\n",
       "      <td>0.463695</td>\n",
       "      <td>0.477256</td>\n",
       "      <td>0.509303</td>\n",
       "      <td>0.196984</td>\n",
       "      <td>0.803016</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_time_data_15</td>\n",
       "      <td>8.413158e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.033521e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.524755</td>\n",
       "      <td>0.463695</td>\n",
       "      <td>0.477256</td>\n",
       "      <td>0.509303</td>\n",
       "      <td>0.196984</td>\n",
       "      <td>0.803016</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_time_data_6</td>\n",
       "      <td>8.484844e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.638545e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.470250</td>\n",
       "      <td>0.475752</td>\n",
       "      <td>0.510883</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>0.484670</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_time_data_7</td>\n",
       "      <td>8.484844e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.638545e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.470250</td>\n",
       "      <td>0.475752</td>\n",
       "      <td>0.510883</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>0.484670</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_time_data_9</td>\n",
       "      <td>8.466762e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.550365e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.513033</td>\n",
       "      <td>0.475944</td>\n",
       "      <td>0.474183</td>\n",
       "      <td>0.512145</td>\n",
       "      <td>0.227796</td>\n",
       "      <td>0.772204</td>\n",
       "      <td>0.483033</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_time_data_10</td>\n",
       "      <td>8.466762e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.550365e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.513033</td>\n",
       "      <td>0.475944</td>\n",
       "      <td>0.474183</td>\n",
       "      <td>0.512145</td>\n",
       "      <td>0.227796</td>\n",
       "      <td>0.772204</td>\n",
       "      <td>0.483033</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_time_data_22</td>\n",
       "      <td>8.467156e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.768704e-06</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.513060</td>\n",
       "      <td>0.475364</td>\n",
       "      <td>0.474405</td>\n",
       "      <td>0.512183</td>\n",
       "      <td>0.186954</td>\n",
       "      <td>0.813046</td>\n",
       "      <td>0.481632</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_time_data_21</td>\n",
       "      <td>8.467156e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.768704e-06</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.513060</td>\n",
       "      <td>0.475364</td>\n",
       "      <td>0.474405</td>\n",
       "      <td>0.512183</td>\n",
       "      <td>0.186954</td>\n",
       "      <td>0.813046</td>\n",
       "      <td>0.481632</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_time_data_20</td>\n",
       "      <td>8.575922e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.770006e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.511081</td>\n",
       "      <td>0.479319</td>\n",
       "      <td>0.473458</td>\n",
       "      <td>0.512648</td>\n",
       "      <td>0.192184</td>\n",
       "      <td>0.807816</td>\n",
       "      <td>0.480689</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_time_data_3</td>\n",
       "      <td>8.413972e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.645941e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.516735</td>\n",
       "      <td>0.470908</td>\n",
       "      <td>0.475483</td>\n",
       "      <td>0.511274</td>\n",
       "      <td>0.196471</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_time_data_4</td>\n",
       "      <td>8.413972e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.645941e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.516735</td>\n",
       "      <td>0.470908</td>\n",
       "      <td>0.475483</td>\n",
       "      <td>0.511274</td>\n",
       "      <td>0.196471</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_time_data_0</td>\n",
       "      <td>8.413363e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.129605e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.515137</td>\n",
       "      <td>0.472540</td>\n",
       "      <td>0.474926</td>\n",
       "      <td>0.511882</td>\n",
       "      <td>0.142976</td>\n",
       "      <td>0.857024</td>\n",
       "      <td>0.480675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_time_data_1</td>\n",
       "      <td>8.413363e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.129605e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.515137</td>\n",
       "      <td>0.472540</td>\n",
       "      <td>0.474926</td>\n",
       "      <td>0.511882</td>\n",
       "      <td>0.142976</td>\n",
       "      <td>0.857024</td>\n",
       "      <td>0.480675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_time_data_11</td>\n",
       "      <td>8.716846e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.197870e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.515197</td>\n",
       "      <td>0.473102</td>\n",
       "      <td>0.475178</td>\n",
       "      <td>0.511207</td>\n",
       "      <td>0.285730</td>\n",
       "      <td>0.714270</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.010846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_time_data_12</td>\n",
       "      <td>8.414590e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.938564e-07</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.517553</td>\n",
       "      <td>0.470705</td>\n",
       "      <td>0.475075</td>\n",
       "      <td>0.511689</td>\n",
       "      <td>0.112233</td>\n",
       "      <td>0.887767</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_time_data_13</td>\n",
       "      <td>8.414590e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.938564e-07</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.517553</td>\n",
       "      <td>0.470705</td>\n",
       "      <td>0.475075</td>\n",
       "      <td>0.511689</td>\n",
       "      <td>0.112233</td>\n",
       "      <td>0.887767</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_time_data_14</td>\n",
       "      <td>8.419673e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-4.146169e-07</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.509966</td>\n",
       "      <td>0.481207</td>\n",
       "      <td>0.473869</td>\n",
       "      <td>0.512604</td>\n",
       "      <td>0.097445</td>\n",
       "      <td>0.902555</td>\n",
       "      <td>0.477387</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_time_data_17</td>\n",
       "      <td>8.415871e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-2.278825e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.511778</td>\n",
       "      <td>0.478120</td>\n",
       "      <td>0.473310</td>\n",
       "      <td>0.512552</td>\n",
       "      <td>0.265031</td>\n",
       "      <td>0.734969</td>\n",
       "      <td>0.483505</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_time_data_5</td>\n",
       "      <td>8.415486e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-2.744799e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.510467</td>\n",
       "      <td>0.478967</td>\n",
       "      <td>0.472872</td>\n",
       "      <td>0.513082</td>\n",
       "      <td>0.280986</td>\n",
       "      <td>0.719014</td>\n",
       "      <td>0.483436</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_time_data_23</td>\n",
       "      <td>8.718284e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-3.029322e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.510152</td>\n",
       "      <td>0.478426</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.513366</td>\n",
       "      <td>0.317028</td>\n",
       "      <td>0.682972</td>\n",
       "      <td>0.484642</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_time_data_8</td>\n",
       "      <td>8.576708e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-3.533538e-06</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.506533</td>\n",
       "      <td>0.483297</td>\n",
       "      <td>0.472455</td>\n",
       "      <td>0.513768</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.479149</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_time_data_2</td>\n",
       "      <td>8.423697e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-5.772235e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.494768</td>\n",
       "      <td>0.494942</td>\n",
       "      <td>0.472825</td>\n",
       "      <td>0.513866</td>\n",
       "      <td>0.079548</td>\n",
       "      <td>0.920452</td>\n",
       "      <td>0.474571</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model           mse  true_pnl       avg_pnl  median_pnl  \\\n",
       "19  rf_time_data_19  8.460822e-07  0.000617  6.237267e-06   -0.000003   \n",
       "18  rf_time_data_18  8.460822e-07  0.000617  6.237267e-06   -0.000003   \n",
       "16  rf_time_data_16  8.413158e-07  0.000617  6.033521e-06   -0.000003   \n",
       "15  rf_time_data_15  8.413158e-07  0.000617  6.033521e-06   -0.000003   \n",
       "6    rf_time_data_6  8.484844e-07  0.000617  5.638545e-06   -0.000003   \n",
       "7    rf_time_data_7  8.484844e-07  0.000617  5.638545e-06   -0.000003   \n",
       "9    rf_time_data_9  8.466762e-07  0.000617  5.550365e-06   -0.000003   \n",
       "10  rf_time_data_10  8.466762e-07  0.000617  5.550365e-06   -0.000003   \n",
       "22  rf_time_data_22  8.467156e-07  0.000617  3.768704e-06   -0.000018   \n",
       "21  rf_time_data_21  8.467156e-07  0.000617  3.768704e-06   -0.000018   \n",
       "20  rf_time_data_20  8.575922e-07  0.000617  1.770006e-06   -0.000029   \n",
       "3    rf_time_data_3  8.413972e-07  0.000617  1.645941e-06   -0.000003   \n",
       "4    rf_time_data_4  8.413972e-07  0.000617  1.645941e-06   -0.000003   \n",
       "0    rf_time_data_0  8.413363e-07  0.000617  1.129605e-06   -0.000029   \n",
       "1    rf_time_data_1  8.413363e-07  0.000617  1.129605e-06   -0.000029   \n",
       "11  rf_time_data_11  8.716846e-07  0.000617  9.197870e-07   -0.000003   \n",
       "12  rf_time_data_12  8.414590e-07  0.000617  3.938564e-07   -0.000030   \n",
       "13  rf_time_data_13  8.414590e-07  0.000617  3.938564e-07   -0.000030   \n",
       "14  rf_time_data_14  8.419673e-07  0.000617 -4.146169e-07   -0.000032   \n",
       "17  rf_time_data_17  8.415871e-07  0.000617 -2.278825e-06   -0.000003   \n",
       "5    rf_time_data_5  8.415486e-07  0.000617 -2.744799e-06   -0.000003   \n",
       "23  rf_time_data_23  8.718284e-07  0.000617 -3.029322e-06   -0.000003   \n",
       "8    rf_time_data_8  8.576708e-07  0.000617 -3.533538e-06   -0.000030   \n",
       "2    rf_time_data_2  8.423697e-07  0.000617 -5.772235e-06   -0.000033   \n",
       "\n",
       "     true_up  false_up  true_down  false_down  up_ratio  down_ratio  accuracy  \\\n",
       "19  0.515503  0.473091   0.474965    0.511542  0.203130    0.796870  0.483200   \n",
       "18  0.515503  0.473091   0.474965    0.511542  0.203130    0.796870  0.483200   \n",
       "16  0.524755  0.463695   0.477256    0.509303  0.196984    0.803016  0.486612   \n",
       "15  0.524755  0.463695   0.477256    0.509303  0.196984    0.803016  0.486612   \n",
       "6   0.517785  0.470250   0.475752    0.510883  0.212175    0.787825  0.484670   \n",
       "7   0.517785  0.470250   0.475752    0.510883  0.212175    0.787825  0.484670   \n",
       "9   0.513033  0.475944   0.474183    0.512145  0.227796    0.772204  0.483033   \n",
       "10  0.513033  0.475944   0.474183    0.512145  0.227796    0.772204  0.483033   \n",
       "22  0.513060  0.475364   0.474405    0.512183  0.186954    0.813046  0.481632   \n",
       "21  0.513060  0.475364   0.474405    0.512183  0.186954    0.813046  0.481632   \n",
       "20  0.511081  0.479319   0.473458    0.512648  0.192184    0.807816  0.480689   \n",
       "3   0.516735  0.470908   0.475483    0.511274  0.196471    0.803529  0.483588   \n",
       "4   0.516735  0.470908   0.475483    0.511274  0.196471    0.803529  0.483588   \n",
       "0   0.515137  0.472540   0.474926    0.511882  0.142976    0.857024  0.480675   \n",
       "1   0.515137  0.472540   0.474926    0.511882  0.142976    0.857024  0.480675   \n",
       "11  0.515197  0.473102   0.475178    0.511207  0.285730    0.714270  0.486612   \n",
       "12  0.517553  0.470705   0.475075    0.511689  0.112233    0.887767  0.479842   \n",
       "13  0.517553  0.470705   0.475075    0.511689  0.112233    0.887767  0.479842   \n",
       "14  0.509966  0.481207   0.473869    0.512604  0.097445    0.902555  0.477387   \n",
       "17  0.511778  0.478120   0.473310    0.512552  0.265031    0.734969  0.483505   \n",
       "5   0.510467  0.478967   0.472872    0.513082  0.280986    0.719014  0.483436   \n",
       "23  0.510152  0.478426   0.472801    0.513366  0.317028    0.682972  0.484642   \n",
       "8   0.506533  0.483297   0.472455    0.513768  0.196429    0.803571  0.479149   \n",
       "2   0.494768  0.494942   0.472825    0.513866  0.079548    0.920452  0.474571   \n",
       "\n",
       "     max_win  max_loss  \n",
       "19  0.014526 -0.013493  \n",
       "18  0.014526 -0.013493  \n",
       "16  0.014526 -0.013493  \n",
       "15  0.014526 -0.013493  \n",
       "6   0.014526 -0.013493  \n",
       "7   0.014526 -0.013493  \n",
       "9   0.014526 -0.013493  \n",
       "10  0.014526 -0.013493  \n",
       "22  0.014526 -0.013493  \n",
       "21  0.014526 -0.013493  \n",
       "20  0.014526 -0.012231  \n",
       "3   0.014526 -0.013493  \n",
       "4   0.014526 -0.013493  \n",
       "0   0.014526 -0.013493  \n",
       "1   0.014526 -0.013493  \n",
       "11  0.014526 -0.010846  \n",
       "12  0.014526 -0.013493  \n",
       "13  0.014526 -0.013493  \n",
       "14  0.014526 -0.013493  \n",
       "17  0.011996 -0.014526  \n",
       "5   0.011996 -0.014526  \n",
       "23  0.014526 -0.012231  \n",
       "8   0.014526 -0.012231  \n",
       "2   0.014526 -0.013493  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "# load rfs\n",
    "rfs = []\n",
    "for i in range(len(hps)):\n",
    "    rf = load(f\"{save_dir}{STOCK}_linear_time_window_rf_{i}.joblib\")\n",
    "    rfs.append(rf)\n",
    "\n",
    "# evaluate rfs\n",
    "for i, rf in enumerate(rfs):\n",
    "    prediction = rf.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_time_data_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4hPC8sACR3L"
   },
   "source": [
    "Suprisingly, disappointing results, as `avg_pnl` metric dropped quite significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPDryPUzj3v5"
   },
   "source": [
    "### 2.6) \"Linear\" rolling windows - different window size\n",
    "\n",
    "Here we try different window sizes - 20, 50, and 100,  \n",
    "instead of the original size - 10.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "8Ad-W0Xuj2xj",
    "outputId": "4eaaaea3-5308-4413-bbb9-948244e26091"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "rfs = []\n",
    "summaries = []\n",
    "window_sizes = [20,50,100]\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print('-'*10 + str(window_size) + '-'*10)\n",
    "\n",
    "    # windows generation\n",
    "    windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "    windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "    windows_x = np.array([win.flatten() for win in windows_x])\n",
    "    windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "    windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "    # split to train-windows and validation-windows\n",
    "    num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "    train_x = windows_x[:num_train_windows]\n",
    "    train_y = windows_y[:num_train_windows]\n",
    "    valid_x = windows_x[num_train_windows:]\n",
    "    valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "    for i, hp in enumerate(hps):\n",
    "        print(f\"{i} / {len(hps)}\")\n",
    "        \n",
    "        # create RF\n",
    "        rf = RandomForestRegressor(**hp)\n",
    "        rf.fit(train_x, train_y)\n",
    "\n",
    "        # save data and RFs\n",
    "        dump(rf, f\"{save_dir}{STOCK}_linear_window_size_{window_size}_rf_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "wo7YThWvNhc4",
    "outputId": "f695574b-9ca2-402a-b3a0-d2230b78572f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rf_window_100_20</td>\n",
       "      <td>8.479543e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519235</td>\n",
       "      <td>0.466257</td>\n",
       "      <td>0.481444</td>\n",
       "      <td>0.506747</td>\n",
       "      <td>0.472997</td>\n",
       "      <td>0.527003</td>\n",
       "      <td>0.499319</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>rf_window_100_8</td>\n",
       "      <td>8.489170e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517595</td>\n",
       "      <td>0.468163</td>\n",
       "      <td>0.479998</td>\n",
       "      <td>0.508006</td>\n",
       "      <td>0.484721</td>\n",
       "      <td>0.515279</td>\n",
       "      <td>0.498222</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>rf_window_100_19</td>\n",
       "      <td>8.451665e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517070</td>\n",
       "      <td>0.468768</td>\n",
       "      <td>0.479284</td>\n",
       "      <td>0.508616</td>\n",
       "      <td>0.477664</td>\n",
       "      <td>0.522336</td>\n",
       "      <td>0.497333</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>rf_window_100_18</td>\n",
       "      <td>8.451665e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517070</td>\n",
       "      <td>0.468768</td>\n",
       "      <td>0.479284</td>\n",
       "      <td>0.508616</td>\n",
       "      <td>0.477664</td>\n",
       "      <td>0.522336</td>\n",
       "      <td>0.497333</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>rf_window_100_22</td>\n",
       "      <td>8.442231e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517559</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.479788</td>\n",
       "      <td>0.508181</td>\n",
       "      <td>0.477025</td>\n",
       "      <td>0.522975</td>\n",
       "      <td>0.497805</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rf_window_50_4</td>\n",
       "      <td>8.419535e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511302</td>\n",
       "      <td>0.476076</td>\n",
       "      <td>0.472999</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>0.451986</td>\n",
       "      <td>0.548014</td>\n",
       "      <td>0.490311</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_window_50_1</td>\n",
       "      <td>8.418525e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511493</td>\n",
       "      <td>0.476297</td>\n",
       "      <td>0.472972</td>\n",
       "      <td>0.513308</td>\n",
       "      <td>0.426335</td>\n",
       "      <td>0.573665</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rf_window_50_0</td>\n",
       "      <td>8.418525e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511493</td>\n",
       "      <td>0.476297</td>\n",
       "      <td>0.472972</td>\n",
       "      <td>0.513308</td>\n",
       "      <td>0.426335</td>\n",
       "      <td>0.573665</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rf_window_50_17</td>\n",
       "      <td>8.444698e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510642</td>\n",
       "      <td>0.476506</td>\n",
       "      <td>0.472457</td>\n",
       "      <td>0.514263</td>\n",
       "      <td>0.477388</td>\n",
       "      <td>0.522612</td>\n",
       "      <td>0.490686</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rf_window_50_5</td>\n",
       "      <td>8.446301e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509859</td>\n",
       "      <td>0.477158</td>\n",
       "      <td>0.471812</td>\n",
       "      <td>0.515026</td>\n",
       "      <td>0.482205</td>\n",
       "      <td>0.517795</td>\n",
       "      <td>0.490159</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.012231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               model           mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "68  rf_window_100_20  8.479543e-07  0.000617  0.000013         0.0  0.519235   \n",
       "56   rf_window_100_8  8.489170e-07  0.000617  0.000011         0.0  0.517595   \n",
       "67  rf_window_100_19  8.451665e-07  0.000617  0.000010         0.0  0.517070   \n",
       "66  rf_window_100_18  8.451665e-07  0.000617  0.000010         0.0  0.517070   \n",
       "70  rf_window_100_22  8.442231e-07  0.000617  0.000010         0.0  0.517559   \n",
       "..               ...           ...       ...       ...         ...       ...   \n",
       "28    rf_window_50_4  8.419535e-07  0.000617 -0.000003         0.0  0.511302   \n",
       "25    rf_window_50_1  8.418525e-07  0.000617 -0.000004         0.0  0.511493   \n",
       "24    rf_window_50_0  8.418525e-07  0.000617 -0.000004         0.0  0.511493   \n",
       "41   rf_window_50_17  8.444698e-07  0.000617 -0.000005         0.0  0.510642   \n",
       "29    rf_window_50_5  8.446301e-07  0.000617 -0.000006         0.0  0.509859   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "68  0.466257   0.481444    0.506747  0.472997    0.527003  0.499319  0.014526   \n",
       "56  0.468163   0.479998    0.508006  0.484721    0.515279  0.498222  0.014526   \n",
       "67  0.468768   0.479284    0.508616  0.477664    0.522336  0.497333  0.014526   \n",
       "66  0.468768   0.479284    0.508616  0.477664    0.522336  0.497333  0.014526   \n",
       "70  0.468202   0.479788    0.508181  0.477025    0.522975  0.497805  0.014526   \n",
       "..       ...        ...         ...       ...         ...       ...       ...   \n",
       "28  0.476076   0.472999    0.513551  0.451986    0.548014  0.490311  0.011996   \n",
       "25  0.476297   0.472972    0.513308  0.426335    0.573665  0.489395  0.011793   \n",
       "24  0.476297   0.472972    0.513308  0.426335    0.573665  0.489395  0.011793   \n",
       "41  0.476506   0.472457    0.514263  0.477388    0.522612  0.490686  0.014526   \n",
       "29  0.477158   0.471812    0.515026  0.482205    0.517795  0.490159  0.014526   \n",
       "\n",
       "    max_loss  \n",
       "68 -0.012759  \n",
       "56 -0.013981  \n",
       "67 -0.013493  \n",
       "66 -0.013493  \n",
       "70 -0.013493  \n",
       "..       ...  \n",
       "28 -0.014526  \n",
       "25 -0.014526  \n",
       "24 -0.014526  \n",
       "41 -0.013493  \n",
       "29 -0.012231  \n",
       "\n",
       "[72 rows x 14 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "for window_size in [20,50,100]:\n",
    "\n",
    "    # windows generation\n",
    "    windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "    windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "    windows_x = np.array([win.flatten() for win in windows_x])\n",
    "    windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "    windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "    # split to train-windows and validation-windows\n",
    "    num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "    train_x = windows_x[:num_train_windows]\n",
    "    train_y = windows_y[:num_train_windows]\n",
    "    valid_x = windows_x[num_train_windows:]\n",
    "    valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "    for i in range(len(hps)):\n",
    "        \n",
    "        try:\n",
    "            # evaluate rf\n",
    "            rf = load(f\"{save_dir}{STOCK}_linear_window_size_{window_size}_rf_{i}.joblib\")\n",
    "            prediction = rf.predict(valid_x)        \n",
    "            summary = evaluate_model(valid_y.flatten(), prediction, f\"rf_window_{window_size}_{i}\")\n",
    "            summaries.append(summary)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBsMgZvINqUJ"
   },
   "source": [
    "Yet another suprise - none of the windows beat the simple linear size 10 windows, in terms of `avg_pnl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qa8tpaSLhYj"
   },
   "source": [
    "## 3) different models\n",
    "\n",
    "After examining the different types of inputs we can give to a model,  \n",
    "we will stick with our previous best performing type - the simple linear windows of size 10.\n",
    "\n",
    "Only this time, instead of using grid search with random forests,  \n",
    "we going to try to use different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmAUAHUFL4Ln"
   },
   "source": [
    "### 3.1) LGBM/GBM\n",
    "\n",
    "Looking at some of the top winning solutions ([here](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions) and [here](https://www.kaggle.com/bangdasun/winning-solutions)) in several time series competitions in kaggle,  \n",
    "we can see a strong tendency to utilize Gradient Boosting Machines,  \n",
    "specifically speaking, strong tendency to use the [LightGBM](https://lightgbm.readthedocs.io/en/latest/) package.\n",
    "\n",
    "Therefore, we will try using this package,  \n",
    "with similar approach we did previously, with random forests - using grid search.  \n",
    "We use the suggestions from [this site](https://sites.google.com/view/lauraepp/parameters) to select hyperparameters for the grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKCqKbro6JiL"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpWsGSLG6I9j"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "df = read_data(data_dir, STOCK)\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "# drop Nans + test data\n",
    "df.dropna(inplace=True)\n",
    "df = df[df.index < SPLIT_DATE2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "545NQa-feCRv"
   },
   "source": [
    "#### 3.1.1) Simple linear rolling windows of size 10\n",
    "\n",
    "As we did with the regular Random Forest (section 2.1),  \n",
    "Only this time we try to run it with LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DhBwREZ7eILu",
    "outputId": "1568ace4-1af3-4249-adf8-970683a6134b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 2s, sys: 4.13 s, total: 5min 6s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "# create data\n",
    "lgb_train = lgb.Dataset(train_x, train_y.flatten())\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "A9Y0UWz_e9Ol",
    "outputId": "5213bcec-b1bf-48ff-b8a1-a99e28b7337f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create hyperparameters for grid search\n",
    "search_hps = {\n",
    "    'num_iterations': [500],\n",
    "    'early_stopping_round' : [50],\n",
    "    'max_depth': [-1,4,8],\n",
    "    'num_leaves': [15,63,255],\n",
    "    'bagging_fraction': [0.3,0.7],\n",
    "    'feature_fraction': [0.3,0.7],\n",
    "    'learning_rate': [0.001],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['mean_squared_error'],\n",
    "    'seed': [1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "all_search_hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in all_search_hps]\n",
    "len(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "    # train\n",
    "    model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "    dump(model, f\"{save_dir}{STOCK}_lgbm__windowsize10_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YluCSO3tkFur",
    "outputId": "33c3cf40-f6a7-449c-86c3-131c0d59a2e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lgbm_windowsize10_34</td>\n",
       "      <td>8.406519e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524476</td>\n",
       "      <td>0.461324</td>\n",
       "      <td>0.481238</td>\n",
       "      <td>0.506261</td>\n",
       "      <td>0.334133</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.495685</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lgbm_windowsize10_32</td>\n",
       "      <td>8.406519e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524476</td>\n",
       "      <td>0.461324</td>\n",
       "      <td>0.481238</td>\n",
       "      <td>0.506261</td>\n",
       "      <td>0.334133</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.495685</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lgbm_windowsize10_8</td>\n",
       "      <td>8.406745e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524386</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.480818</td>\n",
       "      <td>0.506380</td>\n",
       "      <td>0.331386</td>\n",
       "      <td>0.668614</td>\n",
       "      <td>0.495255</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lgbm_windowsize10_10</td>\n",
       "      <td>8.406745e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524386</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.480818</td>\n",
       "      <td>0.506380</td>\n",
       "      <td>0.331386</td>\n",
       "      <td>0.668614</td>\n",
       "      <td>0.495255</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lgbm_windowsize10_24</td>\n",
       "      <td>8.406982e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528940</td>\n",
       "      <td>0.458062</td>\n",
       "      <td>0.481807</td>\n",
       "      <td>0.505094</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.496143</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lgbm_windowsize10_26</td>\n",
       "      <td>8.406982e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528940</td>\n",
       "      <td>0.458062</td>\n",
       "      <td>0.481807</td>\n",
       "      <td>0.505094</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.496143</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lgbm_windowsize10_31</td>\n",
       "      <td>8.406793e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526288</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.481018</td>\n",
       "      <td>0.505832</td>\n",
       "      <td>0.318485</td>\n",
       "      <td>0.681515</td>\n",
       "      <td>0.495436</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lgbm_windowsize10_29</td>\n",
       "      <td>8.406793e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526288</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.481018</td>\n",
       "      <td>0.505832</td>\n",
       "      <td>0.318485</td>\n",
       "      <td>0.681515</td>\n",
       "      <td>0.495436</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lgbm_windowsize10_35</td>\n",
       "      <td>8.406838e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524737</td>\n",
       "      <td>0.461930</td>\n",
       "      <td>0.480439</td>\n",
       "      <td>0.506615</td>\n",
       "      <td>0.316306</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.494451</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lgbm_windowsize10_33</td>\n",
       "      <td>8.406838e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524737</td>\n",
       "      <td>0.461930</td>\n",
       "      <td>0.480439</td>\n",
       "      <td>0.506615</td>\n",
       "      <td>0.316306</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.494451</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lgbm_windowsize10_9</td>\n",
       "      <td>8.406553e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526747</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.481353</td>\n",
       "      <td>0.505674</td>\n",
       "      <td>0.316653</td>\n",
       "      <td>0.683347</td>\n",
       "      <td>0.495727</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm_windowsize10_11</td>\n",
       "      <td>8.406553e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526747</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.481353</td>\n",
       "      <td>0.505674</td>\n",
       "      <td>0.316653</td>\n",
       "      <td>0.683347</td>\n",
       "      <td>0.495727</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm_windowsize10_0</td>\n",
       "      <td>8.406989e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>0.458809</td>\n",
       "      <td>0.481479</td>\n",
       "      <td>0.505383</td>\n",
       "      <td>0.304126</td>\n",
       "      <td>0.695874</td>\n",
       "      <td>0.495713</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm_windowsize10_2</td>\n",
       "      <td>8.406989e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>0.458809</td>\n",
       "      <td>0.481479</td>\n",
       "      <td>0.505383</td>\n",
       "      <td>0.304126</td>\n",
       "      <td>0.695874</td>\n",
       "      <td>0.495713</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lgbm_windowsize10_30</td>\n",
       "      <td>8.407016e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.459993</td>\n",
       "      <td>0.481377</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.317638</td>\n",
       "      <td>0.682362</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lgbm_windowsize10_28</td>\n",
       "      <td>8.407016e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.459993</td>\n",
       "      <td>0.481377</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.317638</td>\n",
       "      <td>0.682362</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm_windowsize10_12</td>\n",
       "      <td>8.407146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526755</td>\n",
       "      <td>0.459671</td>\n",
       "      <td>0.480959</td>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.299451</td>\n",
       "      <td>0.700549</td>\n",
       "      <td>0.494673</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_windowsize10_14</td>\n",
       "      <td>8.407146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526755</td>\n",
       "      <td>0.459671</td>\n",
       "      <td>0.480959</td>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.299451</td>\n",
       "      <td>0.700549</td>\n",
       "      <td>0.494673</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lgbm_windowsize10_20</td>\n",
       "      <td>8.407142e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526928</td>\n",
       "      <td>0.459585</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.506118</td>\n",
       "      <td>0.299326</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lgbm_windowsize10_22</td>\n",
       "      <td>8.407142e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526928</td>\n",
       "      <td>0.459585</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.506118</td>\n",
       "      <td>0.299326</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lgbm_windowsize10_18</td>\n",
       "      <td>8.407142e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526928</td>\n",
       "      <td>0.459585</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.506118</td>\n",
       "      <td>0.299326</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lgbm_windowsize10_16</td>\n",
       "      <td>8.407142e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526928</td>\n",
       "      <td>0.459585</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.506118</td>\n",
       "      <td>0.299326</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lgbm_windowsize10_6</td>\n",
       "      <td>8.407055e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525344</td>\n",
       "      <td>0.461301</td>\n",
       "      <td>0.480686</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.314753</td>\n",
       "      <td>0.685247</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lgbm_windowsize10_4</td>\n",
       "      <td>8.407055e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525344</td>\n",
       "      <td>0.461301</td>\n",
       "      <td>0.480686</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.314753</td>\n",
       "      <td>0.685247</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lgbm_windowsize10_25</td>\n",
       "      <td>8.407246e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525610</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>0.480411</td>\n",
       "      <td>0.506294</td>\n",
       "      <td>0.313379</td>\n",
       "      <td>0.686621</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lgbm_windowsize10_27</td>\n",
       "      <td>8.407246e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525610</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>0.480411</td>\n",
       "      <td>0.506294</td>\n",
       "      <td>0.313379</td>\n",
       "      <td>0.686621</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lgbm_windowsize10_7</td>\n",
       "      <td>8.407075e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525233</td>\n",
       "      <td>0.461795</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.506496</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.687717</td>\n",
       "      <td>0.494395</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm_windowsize10_5</td>\n",
       "      <td>8.407075e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525233</td>\n",
       "      <td>0.461795</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.506496</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.687717</td>\n",
       "      <td>0.494395</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgbm_windowsize10_1</td>\n",
       "      <td>8.407271e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525629</td>\n",
       "      <td>0.461634</td>\n",
       "      <td>0.480474</td>\n",
       "      <td>0.506307</td>\n",
       "      <td>0.312602</td>\n",
       "      <td>0.687398</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lgbm_windowsize10_3</td>\n",
       "      <td>8.407271e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525629</td>\n",
       "      <td>0.461634</td>\n",
       "      <td>0.480474</td>\n",
       "      <td>0.506307</td>\n",
       "      <td>0.312602</td>\n",
       "      <td>0.687398</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lgbm_windowsize10_21</td>\n",
       "      <td>8.407479e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525808</td>\n",
       "      <td>0.461889</td>\n",
       "      <td>0.479596</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.716975</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lgbm_windowsize10_19</td>\n",
       "      <td>8.407479e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525808</td>\n",
       "      <td>0.461889</td>\n",
       "      <td>0.479596</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.716975</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lgbm_windowsize10_23</td>\n",
       "      <td>8.407479e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525808</td>\n",
       "      <td>0.461889</td>\n",
       "      <td>0.479596</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.716975</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lgbm_windowsize10_17</td>\n",
       "      <td>8.407479e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525808</td>\n",
       "      <td>0.461889</td>\n",
       "      <td>0.479596</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.716975</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lgbm_windowsize10_15</td>\n",
       "      <td>8.407485e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525294</td>\n",
       "      <td>0.462108</td>\n",
       "      <td>0.479509</td>\n",
       "      <td>0.507237</td>\n",
       "      <td>0.283011</td>\n",
       "      <td>0.716989</td>\n",
       "      <td>0.492467</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm_windowsize10_13</td>\n",
       "      <td>8.407485e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525294</td>\n",
       "      <td>0.462108</td>\n",
       "      <td>0.479509</td>\n",
       "      <td>0.507237</td>\n",
       "      <td>0.283011</td>\n",
       "      <td>0.716989</td>\n",
       "      <td>0.492467</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model           mse  true_pnl   avg_pnl  median_pnl  \\\n",
       "34  lgbm_windowsize10_34  8.406519e-07  0.000617  0.000014         0.0   \n",
       "32  lgbm_windowsize10_32  8.406519e-07  0.000617  0.000014         0.0   \n",
       "8    lgbm_windowsize10_8  8.406745e-07  0.000617  0.000013         0.0   \n",
       "10  lgbm_windowsize10_10  8.406745e-07  0.000617  0.000013         0.0   \n",
       "24  lgbm_windowsize10_24  8.406982e-07  0.000617  0.000013         0.0   \n",
       "26  lgbm_windowsize10_26  8.406982e-07  0.000617  0.000013         0.0   \n",
       "31  lgbm_windowsize10_31  8.406793e-07  0.000617  0.000013         0.0   \n",
       "29  lgbm_windowsize10_29  8.406793e-07  0.000617  0.000013         0.0   \n",
       "35  lgbm_windowsize10_35  8.406838e-07  0.000617  0.000013         0.0   \n",
       "33  lgbm_windowsize10_33  8.406838e-07  0.000617  0.000013         0.0   \n",
       "9    lgbm_windowsize10_9  8.406553e-07  0.000617  0.000013         0.0   \n",
       "11  lgbm_windowsize10_11  8.406553e-07  0.000617  0.000013         0.0   \n",
       "0    lgbm_windowsize10_0  8.406989e-07  0.000617  0.000013         0.0   \n",
       "2    lgbm_windowsize10_2  8.406989e-07  0.000617  0.000013         0.0   \n",
       "30  lgbm_windowsize10_30  8.407016e-07  0.000617  0.000013         0.0   \n",
       "28  lgbm_windowsize10_28  8.407016e-07  0.000617  0.000013         0.0   \n",
       "12  lgbm_windowsize10_12  8.407146e-07  0.000617  0.000011         0.0   \n",
       "14  lgbm_windowsize10_14  8.407146e-07  0.000617  0.000011         0.0   \n",
       "20  lgbm_windowsize10_20  8.407142e-07  0.000617  0.000011         0.0   \n",
       "22  lgbm_windowsize10_22  8.407142e-07  0.000617  0.000011         0.0   \n",
       "18  lgbm_windowsize10_18  8.407142e-07  0.000617  0.000011         0.0   \n",
       "16  lgbm_windowsize10_16  8.407142e-07  0.000617  0.000011         0.0   \n",
       "6    lgbm_windowsize10_6  8.407055e-07  0.000617  0.000011         0.0   \n",
       "4    lgbm_windowsize10_4  8.407055e-07  0.000617  0.000011         0.0   \n",
       "25  lgbm_windowsize10_25  8.407246e-07  0.000617  0.000010         0.0   \n",
       "27  lgbm_windowsize10_27  8.407246e-07  0.000617  0.000010         0.0   \n",
       "7    lgbm_windowsize10_7  8.407075e-07  0.000617  0.000010         0.0   \n",
       "5    lgbm_windowsize10_5  8.407075e-07  0.000617  0.000010         0.0   \n",
       "1    lgbm_windowsize10_1  8.407271e-07  0.000617  0.000010         0.0   \n",
       "3    lgbm_windowsize10_3  8.407271e-07  0.000617  0.000010         0.0   \n",
       "21  lgbm_windowsize10_21  8.407479e-07  0.000617  0.000007         0.0   \n",
       "19  lgbm_windowsize10_19  8.407479e-07  0.000617  0.000007         0.0   \n",
       "23  lgbm_windowsize10_23  8.407479e-07  0.000617  0.000007         0.0   \n",
       "17  lgbm_windowsize10_17  8.407479e-07  0.000617  0.000007         0.0   \n",
       "15  lgbm_windowsize10_15  8.407485e-07  0.000617  0.000007         0.0   \n",
       "13  lgbm_windowsize10_13  8.407485e-07  0.000617  0.000007         0.0   \n",
       "\n",
       "     true_up  false_up  true_down  false_down  up_ratio  down_ratio  accuracy  \\\n",
       "34  0.524476  0.461324   0.481238    0.506261  0.334133    0.665867  0.495685   \n",
       "32  0.524476  0.461324   0.481238    0.506261  0.334133    0.665867  0.495685   \n",
       "8   0.524386  0.462009   0.480818    0.506380  0.331386    0.668614  0.495255   \n",
       "10  0.524386  0.462009   0.480818    0.506380  0.331386    0.668614  0.495255   \n",
       "24  0.528940  0.458062   0.481807    0.505094  0.304167    0.695833  0.496143   \n",
       "26  0.528940  0.458062   0.481807    0.505094  0.304167    0.695833  0.496143   \n",
       "31  0.526288  0.460818   0.481018    0.505832  0.318485    0.681515  0.495436   \n",
       "29  0.526288  0.460818   0.481018    0.505832  0.318485    0.681515  0.495436   \n",
       "35  0.524737  0.461930   0.480439    0.506615  0.316306    0.683694  0.494451   \n",
       "33  0.524737  0.461930   0.480439    0.506615  0.316306    0.683694  0.494451   \n",
       "9   0.526747  0.459978   0.481353    0.505674  0.316653    0.683347  0.495727   \n",
       "11  0.526747  0.459978   0.481353    0.505674  0.316653    0.683347  0.495727   \n",
       "0   0.528282  0.458809   0.481479    0.505383  0.304126    0.695874  0.495713   \n",
       "2   0.528282  0.458809   0.481479    0.505383  0.304126    0.695874  0.495713   \n",
       "30  0.526599  0.459993   0.481377    0.505713  0.317638    0.682362  0.495741   \n",
       "28  0.526599  0.459993   0.481377    0.505713  0.317638    0.682362  0.495741   \n",
       "12  0.526755  0.459671   0.480959    0.506188  0.299451    0.700549  0.494673   \n",
       "14  0.526755  0.459671   0.480959    0.506188  0.299451    0.700549  0.494673   \n",
       "20  0.526928  0.459585   0.480992    0.506118  0.299326    0.700674  0.494742   \n",
       "22  0.526928  0.459585   0.480992    0.506118  0.299326    0.700674  0.494742   \n",
       "18  0.526928  0.459585   0.480992    0.506118  0.299326    0.700674  0.494742   \n",
       "16  0.526928  0.459585   0.480992    0.506118  0.299326    0.700674  0.494742   \n",
       "6   0.525344  0.461301   0.480686    0.506377  0.314753    0.685247  0.494742   \n",
       "4   0.525344  0.461301   0.480686    0.506377  0.314753    0.685247  0.494742   \n",
       "25  0.525610  0.461818   0.480411    0.506294  0.313379    0.686621  0.494576   \n",
       "27  0.525610  0.461818   0.480411    0.506294  0.313379    0.686621  0.494576   \n",
       "7   0.525233  0.461795   0.480392    0.506496  0.312283    0.687717  0.494395   \n",
       "5   0.525233  0.461795   0.480392    0.506496  0.312283    0.687717  0.494395   \n",
       "1   0.525629  0.461634   0.480474    0.506307  0.312602    0.687398  0.494589   \n",
       "3   0.525629  0.461634   0.480474    0.506307  0.312602    0.687398  0.494589   \n",
       "21  0.525808  0.461889   0.479596    0.507034  0.283025    0.716975  0.492675   \n",
       "19  0.525808  0.461889   0.479596    0.507034  0.283025    0.716975  0.492675   \n",
       "23  0.525808  0.461889   0.479596    0.507034  0.283025    0.716975  0.492675   \n",
       "17  0.525808  0.461889   0.479596    0.507034  0.283025    0.716975  0.492675   \n",
       "15  0.525294  0.462108   0.479509    0.507237  0.283011    0.716989  0.492467   \n",
       "13  0.525294  0.462108   0.479509    0.507237  0.283011    0.716989  0.492467   \n",
       "\n",
       "     max_win  max_loss  \n",
       "34  0.014526 -0.013493  \n",
       "32  0.014526 -0.013493  \n",
       "8   0.014526 -0.013493  \n",
       "10  0.014526 -0.013493  \n",
       "24  0.014526 -0.013493  \n",
       "26  0.014526 -0.013493  \n",
       "31  0.014526 -0.013493  \n",
       "29  0.014526 -0.013493  \n",
       "35  0.014526 -0.013981  \n",
       "33  0.014526 -0.013981  \n",
       "9   0.014526 -0.013493  \n",
       "11  0.014526 -0.013493  \n",
       "0   0.014526 -0.013493  \n",
       "2   0.014526 -0.013493  \n",
       "30  0.014526 -0.013493  \n",
       "28  0.014526 -0.013493  \n",
       "12  0.014526 -0.013493  \n",
       "14  0.014526 -0.013493  \n",
       "20  0.014526 -0.013493  \n",
       "22  0.014526 -0.013493  \n",
       "18  0.014526 -0.013493  \n",
       "16  0.014526 -0.013493  \n",
       "6   0.014526 -0.013493  \n",
       "4   0.014526 -0.013493  \n",
       "25  0.014526 -0.013981  \n",
       "27  0.014526 -0.013981  \n",
       "7   0.014526 -0.013493  \n",
       "5   0.014526 -0.013493  \n",
       "1   0.014526 -0.013981  \n",
       "3   0.014526 -0.013981  \n",
       "21  0.014526 -0.013981  \n",
       "19  0.014526 -0.013981  \n",
       "23  0.014526 -0.013981  \n",
       "17  0.014526 -0.013981  \n",
       "15  0.014526 -0.013981  \n",
       "13  0.014526 -0.013981  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    # evaluate\n",
    "    model = load(f\"{save_dir}{STOCK}_lgbm__windowsize10_{i}.joblib\")\n",
    "    prediction = model.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"lgbm_windowsize10_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRvJRDhsiuxn"
   },
   "source": [
    "Our best `avg_pnl` so far was `0.000017`, so this is almost as good.  \n",
    "Can we do better though?\n",
    "\n",
    "Lets try using bigger window size: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2) Simple linear rolling windows of size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "K_mNvNkbk1WR",
    "outputId": "055bcbc3-339e-4597-bf8f-d55b003e1a4f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 100\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "# create data\n",
    "lgb_train = lgb.Dataset(train_x, train_y.flatten())\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "It2E8-OKlCe9",
    "outputId": "b769b33e-b541-43b7-99be-808b756d468a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create hyperparameters for grid search\n",
    "search_hps = {\n",
    "    'num_iterations': [500],\n",
    "    'early_stopping_round' : [50],\n",
    "    'max_depth': [-1,4,8],\n",
    "    'num_leaves': [15,63,255],\n",
    "    'bagging_fraction': [0.3,0.7],\n",
    "    'feature_fraction': [0.3,0.7],\n",
    "    'learning_rate': [0.001],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['mean_squared_error'],\n",
    "    'seed': [1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "all_search_hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in all_search_hps]\n",
    "\n",
    "models = []\n",
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "    # train\n",
    "    model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "    dump(model, f\"{save_dir}{STOCK}_lgbm__windowsize100_{i}.joblib\")\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lgbm_windowsize100_10</td>\n",
       "      <td>8.407819e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.476045e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.554015</td>\n",
       "      <td>0.434624</td>\n",
       "      <td>0.477342</td>\n",
       "      <td>0.509439</td>\n",
       "      <td>0.072133</td>\n",
       "      <td>0.927867</td>\n",
       "      <td>0.482873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lgbm_windowsize100_8</td>\n",
       "      <td>8.407819e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.476045e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.554015</td>\n",
       "      <td>0.434624</td>\n",
       "      <td>0.477342</td>\n",
       "      <td>0.509439</td>\n",
       "      <td>0.072133</td>\n",
       "      <td>0.927867</td>\n",
       "      <td>0.482873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lgbm_windowsize100_34</td>\n",
       "      <td>8.407928e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.672794e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.554243</td>\n",
       "      <td>0.435016</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.509779</td>\n",
       "      <td>0.064660</td>\n",
       "      <td>0.935340</td>\n",
       "      <td>0.481970</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lgbm_windowsize100_32</td>\n",
       "      <td>8.407928e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.672794e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.554243</td>\n",
       "      <td>0.435016</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.509779</td>\n",
       "      <td>0.064660</td>\n",
       "      <td>0.935340</td>\n",
       "      <td>0.481970</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lgbm_windowsize100_28</td>\n",
       "      <td>8.408166e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.507538e-06</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.560886</td>\n",
       "      <td>0.426814</td>\n",
       "      <td>0.475925</td>\n",
       "      <td>0.510963</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.966121</td>\n",
       "      <td>0.478803</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lgbm_windowsize100_30</td>\n",
       "      <td>8.408166e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.507538e-06</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.560886</td>\n",
       "      <td>0.426814</td>\n",
       "      <td>0.475925</td>\n",
       "      <td>0.510963</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.966121</td>\n",
       "      <td>0.478803</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lgbm_windowsize100_35</td>\n",
       "      <td>8.408188e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.635211e-07</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.532099</td>\n",
       "      <td>0.456333</td>\n",
       "      <td>0.475653</td>\n",
       "      <td>0.511144</td>\n",
       "      <td>0.072050</td>\n",
       "      <td>0.927950</td>\n",
       "      <td>0.479720</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lgbm_windowsize100_33</td>\n",
       "      <td>8.408188e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.635211e-07</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.532099</td>\n",
       "      <td>0.456333</td>\n",
       "      <td>0.475653</td>\n",
       "      <td>0.511144</td>\n",
       "      <td>0.072050</td>\n",
       "      <td>0.927950</td>\n",
       "      <td>0.479720</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lgbm_windowsize100_31</td>\n",
       "      <td>8.408282e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.484731e-07</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.547677</td>\n",
       "      <td>0.441320</td>\n",
       "      <td>0.474640</td>\n",
       "      <td>0.512252</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.988638</td>\n",
       "      <td>0.475469</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lgbm_windowsize100_29</td>\n",
       "      <td>8.408282e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.484731e-07</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.547677</td>\n",
       "      <td>0.441320</td>\n",
       "      <td>0.474640</td>\n",
       "      <td>0.512252</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.988638</td>\n",
       "      <td>0.475469</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lgbm_windowsize100_21</td>\n",
       "      <td>8.408230e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.426996e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.558386</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.512353</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.475122</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lgbm_windowsize100_17</td>\n",
       "      <td>8.408230e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.426996e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.558386</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.512353</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.475122</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lgbm_windowsize100_23</td>\n",
       "      <td>8.408230e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.426996e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.558386</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.512353</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.475122</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lgbm_windowsize100_19</td>\n",
       "      <td>8.408230e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.426996e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.558386</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.512353</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.475122</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm_windowsize100_13</td>\n",
       "      <td>8.408227e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.491440e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>0.474580</td>\n",
       "      <td>0.512347</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.993416</td>\n",
       "      <td>0.475136</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lgbm_windowsize100_15</td>\n",
       "      <td>8.408227e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-9.491440e-07</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>0.474580</td>\n",
       "      <td>0.512347</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.993416</td>\n",
       "      <td>0.475136</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lgbm_windowsize100_6</td>\n",
       "      <td>8.408270e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.511316e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.561462</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.474453</td>\n",
       "      <td>0.512449</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.474817</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lgbm_windowsize100_4</td>\n",
       "      <td>8.408270e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.511316e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.561462</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.474453</td>\n",
       "      <td>0.512449</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.474817</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lgbm_windowsize100_9</td>\n",
       "      <td>8.408265e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.525022e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.525988</td>\n",
       "      <td>0.465696</td>\n",
       "      <td>0.474319</td>\n",
       "      <td>0.512565</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>0.993319</td>\n",
       "      <td>0.474664</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm_windowsize100_11</td>\n",
       "      <td>8.408265e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.525022e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.525988</td>\n",
       "      <td>0.465696</td>\n",
       "      <td>0.474319</td>\n",
       "      <td>0.512565</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>0.993319</td>\n",
       "      <td>0.474664</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lgbm_windowsize100_22</td>\n",
       "      <td>8.408277e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lgbm_windowsize100_18</td>\n",
       "      <td>8.408277e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lgbm_windowsize100_20</td>\n",
       "      <td>8.408277e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lgbm_windowsize100_16</td>\n",
       "      <td>8.408277e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_windowsize100_14</td>\n",
       "      <td>8.408276e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm_windowsize100_12</td>\n",
       "      <td>8.408276e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.873651e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lgbm_windowsize100_24</td>\n",
       "      <td>8.408275e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.964357e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.512662</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lgbm_windowsize100_26</td>\n",
       "      <td>8.408275e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.964357e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.512662</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm_windowsize100_2</td>\n",
       "      <td>8.408275e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.964357e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.512662</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm_windowsize100_0</td>\n",
       "      <td>8.408275e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.964357e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.512662</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgbm_windowsize100_1</td>\n",
       "      <td>8.408273e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.968159e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.474244</td>\n",
       "      <td>0.512669</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.474233</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lgbm_windowsize100_25</td>\n",
       "      <td>8.408273e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.968159e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.474244</td>\n",
       "      <td>0.512669</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.474233</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lgbm_windowsize100_27</td>\n",
       "      <td>8.408273e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.968159e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.474244</td>\n",
       "      <td>0.512669</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.474233</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lgbm_windowsize100_3</td>\n",
       "      <td>8.408273e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.968159e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.474244</td>\n",
       "      <td>0.512669</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.474233</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lgbm_windowsize100_7</td>\n",
       "      <td>8.408306e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-2.241373e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.474236</td>\n",
       "      <td>0.512677</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.474219</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm_windowsize100_5</td>\n",
       "      <td>8.408306e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-2.241373e-06</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.474236</td>\n",
       "      <td>0.512677</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.474219</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model           mse  true_pnl       avg_pnl  median_pnl  \\\n",
       "10  lgbm_windowsize100_10  8.407819e-07  0.000617  3.476045e-06   -0.000003   \n",
       "8    lgbm_windowsize100_8  8.407819e-07  0.000617  3.476045e-06   -0.000003   \n",
       "34  lgbm_windowsize100_34  8.407928e-07  0.000617  2.672794e-06   -0.000005   \n",
       "32  lgbm_windowsize100_32  8.407928e-07  0.000617  2.672794e-06   -0.000005   \n",
       "28  lgbm_windowsize100_28  8.408166e-07  0.000617  1.507538e-06   -0.000031   \n",
       "30  lgbm_windowsize100_30  8.408166e-07  0.000617  1.507538e-06   -0.000031   \n",
       "35  lgbm_windowsize100_35  8.408188e-07  0.000617  1.635211e-07   -0.000030   \n",
       "33  lgbm_windowsize100_33  8.408188e-07  0.000617  1.635211e-07   -0.000030   \n",
       "31  lgbm_windowsize100_31  8.408282e-07  0.000617  1.484731e-07   -0.000032   \n",
       "29  lgbm_windowsize100_29  8.408282e-07  0.000617  1.484731e-07   -0.000032   \n",
       "21  lgbm_windowsize100_21  8.408230e-07  0.000617 -9.426996e-07   -0.000033   \n",
       "17  lgbm_windowsize100_17  8.408230e-07  0.000617 -9.426996e-07   -0.000033   \n",
       "23  lgbm_windowsize100_23  8.408230e-07  0.000617 -9.426996e-07   -0.000033   \n",
       "19  lgbm_windowsize100_19  8.408230e-07  0.000617 -9.426996e-07   -0.000033   \n",
       "13  lgbm_windowsize100_13  8.408227e-07  0.000617 -9.491440e-07   -0.000033   \n",
       "15  lgbm_windowsize100_15  8.408227e-07  0.000617 -9.491440e-07   -0.000033   \n",
       "6    lgbm_windowsize100_6  8.408270e-07  0.000617 -1.511316e-06   -0.000033   \n",
       "4    lgbm_windowsize100_4  8.408270e-07  0.000617 -1.511316e-06   -0.000033   \n",
       "9    lgbm_windowsize100_9  8.408265e-07  0.000617 -1.525022e-06   -0.000033   \n",
       "11  lgbm_windowsize100_11  8.408265e-07  0.000617 -1.525022e-06   -0.000033   \n",
       "22  lgbm_windowsize100_22  8.408277e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "18  lgbm_windowsize100_18  8.408277e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "20  lgbm_windowsize100_20  8.408277e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "16  lgbm_windowsize100_16  8.408277e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "14  lgbm_windowsize100_14  8.408276e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "12  lgbm_windowsize100_12  8.408276e-07  0.000617 -1.873651e-06   -0.000033   \n",
       "24  lgbm_windowsize100_24  8.408275e-07  0.000617 -1.964357e-06   -0.000033   \n",
       "26  lgbm_windowsize100_26  8.408275e-07  0.000617 -1.964357e-06   -0.000033   \n",
       "2    lgbm_windowsize100_2  8.408275e-07  0.000617 -1.964357e-06   -0.000033   \n",
       "0    lgbm_windowsize100_0  8.408275e-07  0.000617 -1.964357e-06   -0.000033   \n",
       "1    lgbm_windowsize100_1  8.408273e-07  0.000617 -1.968159e-06   -0.000033   \n",
       "25  lgbm_windowsize100_25  8.408273e-07  0.000617 -1.968159e-06   -0.000033   \n",
       "27  lgbm_windowsize100_27  8.408273e-07  0.000617 -1.968159e-06   -0.000033   \n",
       "3    lgbm_windowsize100_3  8.408273e-07  0.000617 -1.968159e-06   -0.000033   \n",
       "7    lgbm_windowsize100_7  8.408306e-07  0.000617 -2.241373e-06   -0.000033   \n",
       "5    lgbm_windowsize100_5  8.408306e-07  0.000617 -2.241373e-06   -0.000033   \n",
       "\n",
       "     true_up  false_up  true_down  false_down  up_ratio  down_ratio  accuracy  \\\n",
       "10  0.554015  0.434624   0.477342    0.509439  0.072133    0.927867  0.482873   \n",
       "8   0.554015  0.434624   0.477342    0.509439  0.072133    0.927867  0.482873   \n",
       "34  0.554243  0.435016   0.476974    0.509779  0.064660    0.935340  0.481970   \n",
       "32  0.554243  0.435016   0.476974    0.509779  0.064660    0.935340  0.481970   \n",
       "28  0.560886  0.426814   0.475925    0.510963  0.033879    0.966121  0.478803   \n",
       "30  0.560886  0.426814   0.475925    0.510963  0.033879    0.966121  0.478803   \n",
       "35  0.532099  0.456333   0.475653    0.511144  0.072050    0.927950  0.479720   \n",
       "33  0.532099  0.456333   0.475653    0.511144  0.072050    0.927950  0.479720   \n",
       "31  0.547677  0.441320   0.474640    0.512252  0.011362    0.988638  0.475469   \n",
       "29  0.547677  0.441320   0.474640    0.512252  0.011362    0.988638  0.475469   \n",
       "21  0.558386  0.426752   0.474574    0.512353  0.006542    0.993458  0.475122   \n",
       "17  0.558386  0.426752   0.474574    0.512353  0.006542    0.993458  0.475122   \n",
       "23  0.558386  0.426752   0.474574    0.512353  0.006542    0.993458  0.475122   \n",
       "19  0.558386  0.426752   0.474574    0.512353  0.006542    0.993458  0.475122   \n",
       "13  0.559072  0.426160   0.474580    0.512347  0.006584    0.993416  0.475136   \n",
       "15  0.559072  0.426160   0.474580    0.512347  0.006584    0.993416  0.475136   \n",
       "6   0.561462  0.428571   0.474453    0.512449  0.004181    0.995819  0.474817   \n",
       "4   0.561462  0.428571   0.474453    0.512449  0.004181    0.995819  0.474817   \n",
       "9   0.525988  0.465696   0.474319    0.512565  0.006681    0.993319  0.474664   \n",
       "11  0.525988  0.465696   0.474319    0.512565  0.006681    0.993319  0.474664   \n",
       "22       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "18       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "20       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "16       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "14       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "12       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "24  0.428571  0.571429   0.474252    0.512662  0.000097    0.999903  0.474247   \n",
       "26  0.428571  0.571429   0.474252    0.512662  0.000097    0.999903  0.474247   \n",
       "2   0.428571  0.571429   0.474252    0.512662  0.000097    0.999903  0.474247   \n",
       "0   0.428571  0.571429   0.474252    0.512662  0.000097    0.999903  0.474247   \n",
       "1   0.375000  0.625000   0.474244    0.512669  0.000111    0.999889  0.474233   \n",
       "25  0.375000  0.625000   0.474244    0.512669  0.000111    0.999889  0.474233   \n",
       "27  0.375000  0.625000   0.474244    0.512669  0.000111    0.999889  0.474233   \n",
       "3   0.375000  0.625000   0.474244    0.512669  0.000111    0.999889  0.474233   \n",
       "7   0.384615  0.615385   0.474236    0.512677  0.000181    0.999819  0.474219   \n",
       "5   0.384615  0.615385   0.474236    0.512677  0.000181    0.999819  0.474219   \n",
       "\n",
       "     max_win  max_loss  \n",
       "10  0.014526 -0.013493  \n",
       "8   0.014526 -0.013493  \n",
       "34  0.014526 -0.013493  \n",
       "32  0.014526 -0.013493  \n",
       "28  0.014526 -0.013493  \n",
       "30  0.014526 -0.013493  \n",
       "35  0.014526 -0.013493  \n",
       "33  0.014526 -0.013493  \n",
       "31  0.014526 -0.013493  \n",
       "29  0.014526 -0.013493  \n",
       "21  0.014526 -0.013493  \n",
       "17  0.014526 -0.013493  \n",
       "23  0.014526 -0.013493  \n",
       "19  0.014526 -0.013493  \n",
       "13  0.014526 -0.013493  \n",
       "15  0.014526 -0.013493  \n",
       "6   0.014526 -0.013493  \n",
       "4   0.014526 -0.013493  \n",
       "9   0.014526 -0.013493  \n",
       "11  0.014526 -0.013493  \n",
       "22  0.014526 -0.013493  \n",
       "18  0.014526 -0.013493  \n",
       "20  0.014526 -0.013493  \n",
       "16  0.014526 -0.013493  \n",
       "14  0.014526 -0.013493  \n",
       "12  0.014526 -0.013493  \n",
       "24  0.014526 -0.013493  \n",
       "26  0.014526 -0.013493  \n",
       "2   0.014526 -0.013493  \n",
       "0   0.014526 -0.013493  \n",
       "1   0.014526 -0.013493  \n",
       "25  0.014526 -0.013493  \n",
       "27  0.014526 -0.013493  \n",
       "3   0.014526 -0.013493  \n",
       "7   0.014526 -0.013493  \n",
       "5   0.014526 -0.013493  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    model = models[i]\n",
    "\n",
    "    # evaluate\n",
    "    prediction = model.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"lgbm_windowsize100_{i}\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBmAJsiNpIoo"
   },
   "source": [
    "Bad results, as there is low, and mostly negative, `avg_pnl`.  \n",
    "But even more importantly, all models have nearly 100% `down_ratio`,  \n",
    "which means they almost always predict that the market will go downward.\n",
    "\n",
    "Lets examine the results on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wzsqmU2Rp3wT",
    "outputId": "7e963692-6f6a-4e9d-cd66-c6ea9e2c424e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.781155</td>\n",
       "      <td>0.210048</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>0.488237</td>\n",
       "      <td>0.071371</td>\n",
       "      <td>0.928629</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.009981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.781155</td>\n",
       "      <td>0.210048</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>0.488237</td>\n",
       "      <td>0.071371</td>\n",
       "      <td>0.928629</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.009981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.744288</td>\n",
       "      <td>0.245896</td>\n",
       "      <td>0.488928</td>\n",
       "      <td>0.493358</td>\n",
       "      <td>0.062907</td>\n",
       "      <td>0.937093</td>\n",
       "      <td>0.504992</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.009981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.744288</td>\n",
       "      <td>0.245896</td>\n",
       "      <td>0.488928</td>\n",
       "      <td>0.493358</td>\n",
       "      <td>0.062907</td>\n",
       "      <td>0.937093</td>\n",
       "      <td>0.504992</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.009981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.710662</td>\n",
       "      <td>0.279117</td>\n",
       "      <td>0.488180</td>\n",
       "      <td>0.494080</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.930451</td>\n",
       "      <td>0.503653</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.010775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.710662</td>\n",
       "      <td>0.279117</td>\n",
       "      <td>0.488180</td>\n",
       "      <td>0.494080</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.930451</td>\n",
       "      <td>0.503653</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.010775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732101</td>\n",
       "      <td>0.259832</td>\n",
       "      <td>0.480451</td>\n",
       "      <td>0.502040</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.969125</td>\n",
       "      <td>0.488221</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732101</td>\n",
       "      <td>0.259832</td>\n",
       "      <td>0.480451</td>\n",
       "      <td>0.502040</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.969125</td>\n",
       "      <td>0.488221</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.901615</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.476326</td>\n",
       "      <td>0.506350</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.010775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.901615</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.476326</td>\n",
       "      <td>0.506350</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.010775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.794454</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>0.476280</td>\n",
       "      <td>0.506394</td>\n",
       "      <td>0.009543</td>\n",
       "      <td>0.990457</td>\n",
       "      <td>0.479316</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.794454</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>0.476280</td>\n",
       "      <td>0.506394</td>\n",
       "      <td>0.009543</td>\n",
       "      <td>0.990457</td>\n",
       "      <td>0.479316</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.150888</td>\n",
       "      <td>0.474776</td>\n",
       "      <td>0.507978</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.996492</td>\n",
       "      <td>0.476058</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.150888</td>\n",
       "      <td>0.474776</td>\n",
       "      <td>0.507978</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.996492</td>\n",
       "      <td>0.476058</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.011633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.677391</td>\n",
       "      <td>0.318261</td>\n",
       "      <td>0.474572</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.994033</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.677391</td>\n",
       "      <td>0.318261</td>\n",
       "      <td>0.474572</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.994033</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.677391</td>\n",
       "      <td>0.318261</td>\n",
       "      <td>0.474572</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.994033</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.677391</td>\n",
       "      <td>0.318261</td>\n",
       "      <td>0.474572</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.994033</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.676215</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.474567</td>\n",
       "      <td>0.508138</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.994022</td>\n",
       "      <td>0.475772</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.676215</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.474567</td>\n",
       "      <td>0.508138</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.994022</td>\n",
       "      <td>0.475772</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.473930</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.473930</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.473748</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nan</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473639</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.020250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model       mse  true_pnl   avg_pnl  median_pnl   true_up  false_up  \\\n",
       "8    Nan  0.000001  0.000663  0.000140    0.000036  0.781155  0.210048   \n",
       "10   Nan  0.000001  0.000663  0.000140    0.000036  0.781155  0.210048   \n",
       "32   Nan  0.000001  0.000663  0.000106    0.000004  0.744288  0.245896   \n",
       "34   Nan  0.000001  0.000663  0.000106    0.000004  0.744288  0.245896   \n",
       "35   Nan  0.000001  0.000663  0.000100    0.000004  0.710662  0.279117   \n",
       "33   Nan  0.000001  0.000663  0.000100    0.000004  0.710662  0.279117   \n",
       "28   Nan  0.000001  0.000663  0.000055    0.000000  0.732101  0.259832   \n",
       "30   Nan  0.000001  0.000663  0.000055    0.000000  0.732101  0.259832   \n",
       "9    Nan  0.000001  0.000663  0.000036   -0.000004  0.901615  0.096182   \n",
       "11   Nan  0.000001  0.000663  0.000036   -0.000004  0.901615  0.096182   \n",
       "31   Nan  0.000001  0.000663  0.000026   -0.000004  0.794454  0.199565   \n",
       "29   Nan  0.000001  0.000663  0.000026   -0.000004  0.794454  0.199565   \n",
       "4    Nan  0.000001  0.000663  0.000018   -0.000031  0.840237  0.150888   \n",
       "6    Nan  0.000001  0.000663  0.000018   -0.000031  0.840237  0.150888   \n",
       "23   Nan  0.000001  0.000663  0.000010   -0.000031  0.677391  0.318261   \n",
       "21   Nan  0.000001  0.000663  0.000010   -0.000031  0.677391  0.318261   \n",
       "17   Nan  0.000001  0.000663  0.000010   -0.000031  0.677391  0.318261   \n",
       "19   Nan  0.000001  0.000663  0.000010   -0.000031  0.677391  0.318261   \n",
       "15   Nan  0.000001  0.000663  0.000010   -0.000031  0.676215  0.319444   \n",
       "13   Nan  0.000001  0.000663  0.000010   -0.000031  0.676215  0.319444   \n",
       "5    Nan  0.000001  0.000663  0.000005   -0.000033  0.951613  0.048387   \n",
       "7    Nan  0.000001  0.000663  0.000005   -0.000033  0.951613  0.048387   \n",
       "25   Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "27   Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "26   Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "0    Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "24   Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "1    Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "3    Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "2    Nan  0.000001  0.000663  0.000004   -0.000033  1.000000  0.000000   \n",
       "22   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "20   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "16   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "14   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "12   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "18   Nan  0.000001  0.000663  0.000003   -0.000034       NaN       NaN   \n",
       "\n",
       "    true_down  false_down  up_ratio  down_ratio  accuracy   max_win  max_loss  \n",
       "8    0.493898    0.488237  0.071371    0.928629  0.514400  0.020250 -0.009981  \n",
       "10   0.493898    0.488237  0.071371    0.928629  0.514400  0.020250 -0.009981  \n",
       "32   0.488928    0.493358  0.062907    0.937093  0.504992  0.020250 -0.009981  \n",
       "34   0.488928    0.493358  0.062907    0.937093  0.504992  0.020250 -0.009981  \n",
       "35   0.488180    0.494080  0.069549    0.930451  0.503653  0.020250 -0.010775  \n",
       "33   0.488180    0.494080  0.069549    0.930451  0.503653  0.020250 -0.010775  \n",
       "28   0.480451    0.502040  0.030875    0.969125  0.488221  0.020250 -0.011318  \n",
       "30   0.480451    0.502040  0.030875    0.969125  0.488221  0.020250 -0.011318  \n",
       "9    0.476326    0.506350  0.007068    0.992932  0.479332  0.020250 -0.010775  \n",
       "11   0.476326    0.506350  0.007068    0.992932  0.479332  0.020250 -0.010775  \n",
       "31   0.476280    0.506394  0.009543    0.990457  0.479316  0.020250 -0.011931  \n",
       "29   0.476280    0.506394  0.009543    0.990457  0.479316  0.020250 -0.011931  \n",
       "4    0.474776    0.507978  0.003508    0.996492  0.476058  0.020250 -0.011633  \n",
       "6    0.474776    0.507978  0.003508    0.996492  0.476058  0.020250 -0.011633  \n",
       "23   0.474572    0.508133  0.005967    0.994033  0.475783  0.020250 -0.014055  \n",
       "21   0.474572    0.508133  0.005967    0.994033  0.475783  0.020250 -0.014055  \n",
       "17   0.474572    0.508133  0.005967    0.994033  0.475783  0.020250 -0.014055  \n",
       "19   0.474572    0.508133  0.005967    0.994033  0.475783  0.020250 -0.014055  \n",
       "15   0.474567    0.508138  0.005978    0.994022  0.475772  0.020250 -0.014055  \n",
       "13   0.474567    0.508138  0.005978    0.994022  0.475772  0.020250 -0.014055  \n",
       "5    0.473776    0.509001  0.000322    0.999678  0.473930  0.020250 -0.014055  \n",
       "7    0.473776    0.509001  0.000322    0.999678  0.473930  0.020250 -0.014055  \n",
       "25   0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "27   0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "26   0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "0    0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "24   0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "1    0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "3    0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "2    0.473691    0.509090  0.000109    0.999891  0.473748  0.016928 -0.020250  \n",
       "22   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  \n",
       "20   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  \n",
       "16   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  \n",
       "14   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  \n",
       "12   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  \n",
       "18   0.473639    0.509143  0.000000    1.000000  0.473639  0.016928 -0.020250  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "for model in models:\n",
    "\n",
    "    # evaluate\n",
    "    prediction = model.predict(train_x)\n",
    "    summary = evaluate_model(train_y.flatten(), prediction)\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PNAoNAJqZor"
   },
   "source": [
    "Relatively good results (on the top models), `avg_pnl` wise, on training set - probably too good to be true, which is probably overfit.  \n",
    "Nevertheless, still quite strange `down_ratio`s, as they are very high.  \n",
    "\n",
    "Lets try using different hyperparameters, ones which should generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "amcJQXvfqnyo",
    "outputId": "4d8dda82-5448-42ce-fe46-2f679f5265e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create hyperparameters for grid search\n",
    "search_hps = {\n",
    "    'num_iterations': [20,50,100],\n",
    "    'early_stopping_round' : [10],\n",
    "    'max_depth': [3,4,5],\n",
    "    'num_leaves': [7,15,31],\n",
    "    'bagging_fraction': [0.3,0.7],\n",
    "    'feature_fraction': [0.3,0.7],\n",
    "    'learning_rate': [0.005],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['mean_squared_error'],\n",
    "    'seed': [1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "all_search_hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in all_search_hps]\n",
    "hps = [hp for hp in hps if 2**hp['max_depth']-1==hp['num_leaves']]\n",
    "\n",
    "models2 = []\n",
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "    # train\n",
    "    model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "    dump(model, f\"{save_dir}{STOCK}_lgbm__windowsize100_params_v2_{i}.joblib\")\n",
    "    models2.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408258e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.510556</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>0.480276</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408191e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.510371</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.964302</td>\n",
       "      <td>0.479873</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408146e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.426511</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.964371</td>\n",
       "      <td>0.479178</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408253e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.542047</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.475567</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408292e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.512608</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.474414</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm_window100_params2</td>\n",
       "      <td>8.408254e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.512654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model           mse  true_pnl   avg_pnl  median_pnl  \\\n",
       "35  lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "33  lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "23  lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "9   lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "21  lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "11  lgbm_window100_params2  8.408258e-07  0.000617  0.000003   -0.000030   \n",
       "13  lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "3   lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "1   lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "15  lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "25  lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "27  lgbm_window100_params2  8.408191e-07  0.000617  0.000002   -0.000030   \n",
       "17  lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "19  lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "29  lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "7   lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "5   lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "31  lgbm_window100_params2  8.408146e-07  0.000617  0.000001   -0.000031   \n",
       "30  lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "28  lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "18  lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "16  lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "6   lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "4   lgbm_window100_params2  8.408253e-07  0.000617 -0.000001   -0.000032   \n",
       "20  lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "22  lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "10  lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "8   lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "32  lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "34  lgbm_window100_params2  8.408292e-07  0.000617 -0.000001   -0.000033   \n",
       "24  lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "14  lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "26  lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "12  lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "2   lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "0   lgbm_window100_params2  8.408254e-07  0.000617 -0.000002   -0.000033   \n",
       "\n",
       "     true_up  false_up  true_down  false_down  up_ratio  down_ratio  accuracy  \\\n",
       "35  0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "33  0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "23  0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "9   0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "21  0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "11  0.546830  0.442843   0.476190    0.510556  0.057840    0.942160  0.480276   \n",
       "13  0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "3   0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "1   0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "15  0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "25  0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "27  0.574319  0.417121   0.476376    0.510371  0.035698    0.964302  0.479873   \n",
       "17  0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "19  0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "29  0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "7   0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "5   0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "31  0.564522  0.426511   0.476025    0.510738  0.035629    0.964371  0.479178   \n",
       "30  0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "28  0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "18  0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "16  0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "6   0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "4   0.542047  0.446809   0.474643    0.512246  0.013710    0.986290  0.475567   \n",
       "20  0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "22  0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "10  0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "8   0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "32  0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "34  0.548387  0.430108   0.474318    0.512608  0.001292    0.998708  0.474414   \n",
       "24       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "14       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "26       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "12       NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "2        NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "0        NaN       NaN   0.474261    0.512654  0.000000    1.000000  0.474261   \n",
       "\n",
       "     max_win  max_loss  \n",
       "35  0.014526 -0.013493  \n",
       "33  0.014526 -0.013493  \n",
       "23  0.014526 -0.013493  \n",
       "9   0.014526 -0.013493  \n",
       "21  0.014526 -0.013493  \n",
       "11  0.014526 -0.013493  \n",
       "13  0.014526 -0.013493  \n",
       "3   0.014526 -0.013493  \n",
       "1   0.014526 -0.013493  \n",
       "15  0.014526 -0.013493  \n",
       "25  0.014526 -0.013493  \n",
       "27  0.014526 -0.013493  \n",
       "17  0.014526 -0.013493  \n",
       "19  0.014526 -0.013493  \n",
       "29  0.014526 -0.013493  \n",
       "7   0.014526 -0.013493  \n",
       "5   0.014526 -0.013493  \n",
       "31  0.014526 -0.013493  \n",
       "30  0.014526 -0.013493  \n",
       "28  0.014526 -0.013493  \n",
       "18  0.014526 -0.013493  \n",
       "16  0.014526 -0.013493  \n",
       "6   0.014526 -0.013493  \n",
       "4   0.014526 -0.013493  \n",
       "20  0.014526 -0.013493  \n",
       "22  0.014526 -0.013493  \n",
       "10  0.014526 -0.013493  \n",
       "8   0.014526 -0.013493  \n",
       "32  0.014526 -0.013493  \n",
       "34  0.014526 -0.013493  \n",
       "24  0.014526 -0.013493  \n",
       "14  0.014526 -0.013493  \n",
       "26  0.014526 -0.013493  \n",
       "12  0.014526 -0.013493  \n",
       "2   0.014526 -0.013493  \n",
       "0   0.014526 -0.013493  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    model = models2[i]\n",
    "\n",
    "    # evaluate\n",
    "    prediction = model.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, \"lgbm_window100_params2\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively bad, and quite wierd results, cause the `down_ratio` is so extreme.  \n",
    "We are not sure why this happened, or how we can solve it and help the model do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssajBMInMzbl"
   },
   "source": [
    "### 3.2) Multi Layer Preceptron\n",
    "\n",
    "Next, we try Neural Networks model - a Multi Layer Preceptron.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:27:00</th>\n",
       "      <td>234.60</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>0.001066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:28:00</th>\n",
       "      <td>234.62</td>\n",
       "      <td>0.3435</td>\n",
       "      <td>0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:29:00</th>\n",
       "      <td>234.61</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:30:00</th>\n",
       "      <td>234.61</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:31:00</th>\n",
       "      <td>234.68</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      close  volume    target\n",
       "datetime                                     \n",
       "2017-02-16 17:27:00  234.60  0.3455  0.001066\n",
       "2017-02-16 17:28:00  234.62  0.3435  0.001151\n",
       "2017-02-16 17:29:00  234.61  0.1581  0.001108\n",
       "2017-02-16 17:30:00  234.61  0.2130  0.001364\n",
       "2017-02-16 17:31:00  234.68  0.1257  0.001108"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = read_data(data_dir, STOCK)\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "# drop open, high and low prices\n",
    "df.drop(columns=['open','high','low'], inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To generate the data to feed into this model,  \n",
    "we used the exponentional windows technic demonstrated in section 2.2.  \n",
    "This helped us feed bigger amount of data in to model.  \n",
    "We also used just the `close` and `volume` data - since it was lighter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 2.2 s, total: 2min 7s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "split_date1 = \"03-01-2019\"\n",
    "split_date2 = \"12-01-2019\"\n",
    "window_size = 8\n",
    "\n",
    "# drop test data\n",
    "df = df[df.index < split_date2]\n",
    "\n",
    "# generate exponential windows\n",
    "idx = [0] + [2**i for i in range(window_size-1)]\n",
    "idx = [max(idx)-i for i in idx]\n",
    "idx.reverse()\n",
    "windows_x = df_windows(df.drop(columns=['target']), max(idx)+1)\n",
    "windows_x = windows_x[:,idx,:]\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = windows_x.reshape(windows_x.shape[0], -1)\n",
    "windows_y = df_windows(df['target'].T, max(idx)+1)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < split_date1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define model checkpoints, that will save the model in the lowest loss possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BR8kJNGTYaMF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def get_callbacks(model_name: str):\n",
    "    # define the checkpoint\n",
    "    filepath=\"weights_improvement_\" + model_name + \"_{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "    return [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0L3Ml787taZK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 11,753\n",
      "Trainable params: 11,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 264739 samples, validate on 72027 samples\n",
      "Epoch 1/10\n",
      "264640/264739 [============================>.] - ETA: 0s - loss: 5.9529e-05\n",
      "Epoch 00001: loss improved from inf to 0.00006, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_01-0.0001.hdf5\n",
      "264739/264739 [==============================] - 37s 138us/sample - loss: 5.9508e-05 - val_loss: 4.3315e-08\n",
      "Epoch 2/10\n",
      "264448/264739 [============================>.] - ETA: 0s - loss: 1.6219e-06\n",
      "Epoch 00002: loss improved from 0.00006 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_02-0.0000.hdf5\n",
      "264739/264739 [==============================] - 36s 135us/sample - loss: 1.6222e-06 - val_loss: 2.2571e-08\n",
      "Epoch 3/10\n",
      "264640/264739 [============================>.] - ETA: 0s - loss: 1.3551e-06\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_03-0.0000.hdf5\n",
      "264739/264739 [==============================] - 34s 129us/sample - loss: 1.3549e-06 - val_loss: 2.9748e-08\n",
      "Epoch 4/10\n",
      "264672/264739 [============================>.] - ETA: 0s - loss: 1.2853e-06\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_04-0.0000.hdf5\n",
      "264739/264739 [==============================] - 34s 128us/sample - loss: 1.2853e-06 - val_loss: 2.1071e-08\n",
      "Epoch 5/10\n",
      "264672/264739 [============================>.] - ETA: 0s - loss: 1.2577e-06\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_05-0.0000.hdf5\n",
      "264739/264739 [==============================] - 34s 129us/sample - loss: 1.2575e-06 - val_loss: 2.2421e-08\n",
      "Epoch 6/10\n",
      "264576/264739 [============================>.] - ETA: 0s - loss: 1.2225e-06\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_06-0.0000.hdf5\n",
      "264739/264739 [==============================] - 34s 128us/sample - loss: 1.2225e-06 - val_loss: 2.1042e-08\n",
      "Epoch 7/10\n",
      "264544/264739 [============================>.] - ETA: 0s - loss: 1.2162e-06\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_07-0.0000.hdf5\n",
      "264739/264739 [==============================] - 34s 130us/sample - loss: 1.2162e-06 - val_loss: 4.1310e-08\n",
      "Epoch 8/10\n",
      "264608/264739 [============================>.] - ETA: 0s - loss: 1.2029e-06\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_08-0.0000.hdf5\n",
      "264739/264739 [==============================] - 36s 136us/sample - loss: 1.2030e-06 - val_loss: 2.1718e-08\n",
      "Epoch 9/10\n",
      "264736/264739 [============================>.] - ETA: 0s - loss: 1.1962e-06\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_09-0.0000.hdf5\n",
      "264739/264739 [==============================] - 33s 123us/sample - loss: 1.1962e-06 - val_loss: 3.0474e-08\n",
      "Epoch 10/10\n",
      "264512/264739 [============================>.] - ETA: 0s - loss: 1.1777e-06\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to weights_improvement_ANN_only_SPY_loss_mse_10_epochs_10-0.0000.hdf5\n",
      "264739/264739 [==============================] - 33s 124us/sample - loss: 1.1777e-06 - val_loss: 2.2688e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea5a50d210>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu', input_shape=train_x.shape[1:]))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(20, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x=windows_x, y=windows_y, epochs=10, validation_steps=50, \n",
    "          callbacks=get_callbacks(\"ANN_only_SPY_loss_mse_10_epochs\"), batch_size=32, validation_data=(valid_x, valid_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model', 'ANN_exponential'),\n",
       "             ('mse', 9.329822218717828e-07),\n",
       "             ('true_pnl', 0.0006169401292710023),\n",
       "             ('avg_pnl', 1.5615233964644904e-06),\n",
       "             ('median_pnl', -3.0796710911218383e-05),\n",
       "             ('true_up', 0.5435134260757036),\n",
       "             ('false_up', 0.4432222581688774),\n",
       "             ('true_down', 0.4757456191249855),\n",
       "             ('false_down', 0.5111842868747825),\n",
       "             ('up_ratio', 0.04291446263206853),\n",
       "             ('down_ratio', 0.9570855373679314),\n",
       "             ('accuracy', 0.47865383814402934),\n",
       "             ('max_win', 0.01452604091900278),\n",
       "             ('max_loss', -0.01349331044650115)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate results\n",
    "prediction = model.predict(valid_x)\n",
    "summary = evaluate_model(valid_y, prediction, \"ANN_exponential\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([summary]).to_csv(MODEL_RESULTS_CSV, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We tried a few different hyper parametes & architectures of the networks -  \n",
    "but the results where the same. You can find the saved models next to this code, in github. \n",
    "\n",
    "The results are not good.  \n",
    "Similar to the LightGBM, the MLP model almost always predicted `down`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-6KBauaO8-i"
   },
   "source": [
    "### 3.3) LSTM\n",
    "\n",
    "The next model we try is called LSTM - which stands for \"Long short-term memory\".  \n",
    "This model is known for fitting well with time series data - so it might yield good results for our problem.  \n",
    "The LSTM model has one big disadvantage - it is VERY slow to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'SPY'\n",
    "data_dir = \"data_feather/dukas_feather\"\n",
    "\n",
    "# read data\n",
    "df = read_data(data_dir, ticker)\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "# drop open, high and low prices\n",
    "df.drop(columns=['open','high','low'], inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 2.88 s, total: 2min 3s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "split_date1 = \"03-01-2019\"\n",
    "split_date2 = \"12-01-2019\"\n",
    "window_size = 8\n",
    "\n",
    "# drop test data\n",
    "df = df[df.index < split_date2]\n",
    "\n",
    "# generate exponential windows\n",
    "idx = [0] + [2**i for i in range(window_size-1)]\n",
    "idx = [max(idx)-i for i in idx]\n",
    "idx.reverse()\n",
    "windows_x = df_windows(df.drop(columns=['target']), max(idx)+1)\n",
    "windows_x = windows_x[:,idx,:]\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "# windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, max(idx)+1)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(df[df.index < split_date1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = windows_x.shape[1]\n",
    "n_features = windows_x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3Wlq0rMuA4N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                520       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 531\n",
      "Trainable params: 531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, activation='relu', input_shape=(n_timesteps, n_features), unroll=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UF1IJln8uEcC"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(x=windows_x, y=windows_y, epochs=5, validation_steps=50, \n",
    "          callbacks=get_callbacks(\"LSTM_only_SPY_loss_mse_5_epochs_batch_32_10_neurons\"), batch_size=32, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5REVut7uNLi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model', 'lstm_exponentials'),\n",
       "             ('mse', 1.1226012406447078e-06),\n",
       "             ('true_pnl', 0.0006169401292710023),\n",
       "             ('avg_pnl', 3.2042750072069603e-06),\n",
       "             ('median_pnl', -2.9214329303917097e-05),\n",
       "             ('true_up', 0.5402843601895735),\n",
       "             ('false_up', 0.44797032763239236),\n",
       "             ('true_down', 0.47625569416738617),\n",
       "             ('false_down', 0.5105695656057403),\n",
       "             ('up_ratio', 0.06737751121107363),\n",
       "             ('down_ratio', 0.9326224887889264),\n",
       "             ('accuracy', 0.4805697863301262),\n",
       "             ('max_win', 0.01452604091900278),\n",
       "             ('max_loss', -0.01349331044650115)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(f\"{save_dir}/weights_improvement_LSTM_only_SPY_loss_mse_5_epochs_batch_32_10_neurons_05-0.0000.hdf5\")\n",
    "prediction = model.predict(valid_x)\n",
    "summary = evaluate_model(valid_y, prediction, \"lstm_exponentials\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([summary]).to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than a normal MLP - but still not as good as the Random Forests.  \n",
    "Something we did notice, is that this model is very slow.  \n",
    "This made it very hard to tune and change the parameters and architecture -  \n",
    "since every iteration takes a few good hours.  \n",
    "Similar to the MLP model - we tried different architectures, with no luck.  \n",
    "All of the different models we tried will be in github, next to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MH2OgKjPLnRJ"
   },
   "source": [
    "## 4) different models - more stocks\n",
    "\n",
    "So far, we only used `SPY` historical data to predict its future returns.  \n",
    "Now, in addition to `SPY`s data, we will also use data of several other stocks.\n",
    "\n",
    "We cant use all our Dukascopy data, cause of memory limitations.  \n",
    "When we did overcome the memory limitations (by just taking subsets of the data),  \n",
    "training was infeasible - as it simply was very slow, unrealistic for hyperparameters tunining.\n",
    "\n",
    "Therefore, we decided to pick 10 stocks, along with `SPY`, to use for predicting future `SPY` returns.\n",
    "\n",
    "To do that, with reduced chance of survivership bias,  \n",
    "we selected stocks within the S&P500 composite, as of 2017.\n",
    "\n",
    "We did that by using the website [archive.org](http://web.archive.org/).  \n",
    "[archive.org](http://web.archive.org/) stores historical snapshots of many web pages,  \n",
    "including snapshots of the wikipedia article \"List of S&P 500 companies\".  \n",
    "We entered a [snapshot taken by the end of 2016](http://web.archive.org/web/20161020172115/https://en.wikipedia.org/wiki/List_of_S%26P_500_companies), and there we picked stocks from different sectors:\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "| ticker | company               | sector                      |\n",
    "|--------|-----------------------|-----------------------------|\n",
    "| AAPL   | AAPL Inc.             | Information Technology      |\n",
    "| BAC    | Bank of America Corp  | Financials                  |\n",
    "| GE     | General Electric      | Industrials                 |\n",
    "| GOOGL  | Alphabet Inc Class A  | Information Technology      |\n",
    "| JNJ    | Johnson & johnson     | Health Care                 |\n",
    "| KO     | The Coca Cola Company | Consumer Staples            |\n",
    "| MSFT   | Microsoft Corp.       | Information Technology      |\n",
    "| PG     | Procter & Gamble      | Consumer Staples            |\n",
    "| T      | AT&T Inc              | Telecommunications Services |\n",
    "| XOM    | Exxon Mobil Corp      | Energy                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDhTDa2DCgn7"
   },
   "outputs": [],
   "source": [
    "stocks = ['AAPL', 'BAC', 'GE', 'GOOGL', 'JNJ', 'KO', 'MSFT', 'PG', 'T', 'XOM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCa1JwzXczHv"
   },
   "source": [
    "### 4.1) all stocks into one model\n",
    "For our first try, we will put all our stocks into one dataframe,  \n",
    "create linear windows of size 10 on it,  \n",
    "and model LGBM based on those windows (using, you guessed it - grid search).\n",
    "\n",
    "Below is a diagram of the model:\n",
    "\n",
    "<img src=\"lgbm_all_stocks1.jpeg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rBWQj_BJSwc"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for stock in [STOCK] + stocks:\n",
    "    df = read_data(data_dir, stock)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df[['close', 'volume']]\n",
    "    df.columns = [stock+\"_\"+col for col in df.columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "HfAnVg-PcMq7",
    "outputId": "e984ce21-f8db-4def-a9bc-507e7f9b94a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>datetime</th>\n",
       "      <th>2017-01-26 16:30:00</th>\n",
       "      <th>2017-01-26 16:31:00</th>\n",
       "      <th>2017-01-26 16:32:00</th>\n",
       "      <th>2017-01-26 16:33:00</th>\n",
       "      <th>2017-01-26 16:34:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY_close</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY_volume</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL_close</th>\n",
       "      <td>121.7100</td>\n",
       "      <td>121.8910</td>\n",
       "      <td>122.0600</td>\n",
       "      <td>122.2400</td>\n",
       "      <td>122.2130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL_volume</th>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.3697</td>\n",
       "      <td>0.4814</td>\n",
       "      <td>0.6113</td>\n",
       "      <td>0.6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC_close</th>\n",
       "      <td>23.3600</td>\n",
       "      <td>23.3900</td>\n",
       "      <td>23.4000</td>\n",
       "      <td>23.3400</td>\n",
       "      <td>23.4030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC_volume</th>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.3877</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.2850</td>\n",
       "      <td>0.4189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE_close</th>\n",
       "      <td>30.3700</td>\n",
       "      <td>30.3700</td>\n",
       "      <td>30.3920</td>\n",
       "      <td>30.3910</td>\n",
       "      <td>30.4210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE_volume</th>\n",
       "      <td>0.4188</td>\n",
       "      <td>0.2287</td>\n",
       "      <td>0.2711</td>\n",
       "      <td>0.2706</td>\n",
       "      <td>0.2324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL_close</th>\n",
       "      <td>859.1000</td>\n",
       "      <td>858.5500</td>\n",
       "      <td>858.3600</td>\n",
       "      <td>858.5500</td>\n",
       "      <td>857.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL_volume</th>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.2608</td>\n",
       "      <td>0.1884</td>\n",
       "      <td>0.2956</td>\n",
       "      <td>0.1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ_close</th>\n",
       "      <td>112.1100</td>\n",
       "      <td>111.8800</td>\n",
       "      <td>112.0800</td>\n",
       "      <td>112.3300</td>\n",
       "      <td>112.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ_volume</th>\n",
       "      <td>0.2358</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.1577</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KO_close</th>\n",
       "      <td>42.0640</td>\n",
       "      <td>42.0730</td>\n",
       "      <td>42.0500</td>\n",
       "      <td>42.0700</td>\n",
       "      <td>42.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KO_volume</th>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>0.3205</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT_close</th>\n",
       "      <td>64.1900</td>\n",
       "      <td>64.1300</td>\n",
       "      <td>64.2200</td>\n",
       "      <td>64.3100</td>\n",
       "      <td>64.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT_volume</th>\n",
       "      <td>0.4280</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG_close</th>\n",
       "      <td>87.1200</td>\n",
       "      <td>86.9700</td>\n",
       "      <td>86.9800</td>\n",
       "      <td>86.9100</td>\n",
       "      <td>86.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG_volume</th>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.2399</td>\n",
       "      <td>0.3484</td>\n",
       "      <td>0.3581</td>\n",
       "      <td>0.2727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T_close</th>\n",
       "      <td>41.2200</td>\n",
       "      <td>41.3000</td>\n",
       "      <td>41.2900</td>\n",
       "      <td>41.3300</td>\n",
       "      <td>41.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T_volume</th>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.3879</td>\n",
       "      <td>0.3623</td>\n",
       "      <td>0.3148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM_close</th>\n",
       "      <td>85.7200</td>\n",
       "      <td>85.5570</td>\n",
       "      <td>85.5800</td>\n",
       "      <td>85.7000</td>\n",
       "      <td>85.7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM_volume</th>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.2153</td>\n",
       "      <td>0.1854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "datetime      2017-01-26 16:30:00  2017-01-26 16:31:00  2017-01-26 16:32:00  \\\n",
       "SPY_close                     NaN                  NaN                  NaN   \n",
       "SPY_volume                    NaN                  NaN                  NaN   \n",
       "AAPL_close               121.7100             121.8910             122.0600   \n",
       "AAPL_volume                0.4459               0.3697               0.4814   \n",
       "BAC_close                 23.3600              23.3900              23.4000   \n",
       "BAC_volume                 0.3169               0.3877               0.3525   \n",
       "GE_close                  30.3700              30.3700              30.3920   \n",
       "GE_volume                  0.4188               0.2287               0.2711   \n",
       "GOOGL_close              859.1000             858.5500             858.3600   \n",
       "GOOGL_volume               0.1957               0.2608               0.1884   \n",
       "JNJ_close                112.1100             111.8800             112.0800   \n",
       "JNJ_volume                 0.2358               0.2503               0.1577   \n",
       "KO_close                  42.0640              42.0730              42.0500   \n",
       "KO_volume                  0.1761               0.2109               0.3205   \n",
       "MSFT_close                64.1900              64.1300              64.2200   \n",
       "MSFT_volume                0.4280               0.3106               0.3001   \n",
       "PG_close                  87.1200              86.9700              86.9800   \n",
       "PG_volume                  0.0096               0.2399               0.3484   \n",
       "T_close                   41.2200              41.3000              41.2900   \n",
       "T_volume                   0.0184               0.0160               0.3879   \n",
       "XOM_close                 85.7200              85.5570              85.5800   \n",
       "XOM_volume                 0.0224               0.2079               0.1278   \n",
       "\n",
       "datetime      2017-01-26 16:33:00  2017-01-26 16:34:00  \n",
       "SPY_close                     NaN                  NaN  \n",
       "SPY_volume                    NaN                  NaN  \n",
       "AAPL_close               122.2400             122.2130  \n",
       "AAPL_volume                0.6113               0.6310  \n",
       "BAC_close                 23.3400              23.4030  \n",
       "BAC_volume                 0.2850               0.4189  \n",
       "GE_close                  30.3910              30.4210  \n",
       "GE_volume                  0.2706               0.2324  \n",
       "GOOGL_close              858.5500             857.6100  \n",
       "GOOGL_volume               0.2956               0.1961  \n",
       "JNJ_close                112.3300             112.0600  \n",
       "JNJ_volume                 0.1506               0.1505  \n",
       "KO_close                  42.0700              42.0030  \n",
       "KO_volume                  0.2957               0.3105  \n",
       "MSFT_close                64.3100              64.3100  \n",
       "MSFT_volume                0.1875               0.1950  \n",
       "PG_close                  86.9100              86.8500  \n",
       "PG_volume                  0.3581               0.2727  \n",
       "T_close                   41.3300              41.2600  \n",
       "T_volume                   0.3623               0.3148  \n",
       "XOM_close                 85.7000              85.7100  \n",
       "XOM_volume                 0.2153               0.1854  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e1jUr6C00U6"
   },
   "source": [
    "we need to deal with the nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Sx5WeRMzUEhe",
    "outputId": "b80f56d4-c5d3-49af-ef86-a8d3a46ac5da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe309263190>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df.isnull().sum().plot.bar(title=\"amount of Nans at each column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "-sUqhkXPVfbA",
    "outputId": "999d03d9-6e35-47e5-c3a1-5c5d908356c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "sns.heatmap(merged_df.isnull(), cbar=False, ax = ax)\n",
    "ax.set_title('Nans distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to drop the nans from the spy data, because this is the data we try to predict.  \n",
    "In addition, we can't realy drop the rows where the other stocks have nans -  \n",
    "because this would mean giving up on a lots of data.  \n",
    "We chose to fill the missing values of the other stocks with the latest available value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYxFkIvBTbQZ"
   },
   "outputs": [],
   "source": [
    "# drop all rows with Nan in the SPY data\n",
    "merged_df.dropna(subset=[STOCK+'_close', STOCK+'_volume'], inplace=True)\n",
    "\n",
    "# add flag column for each stock, to signal for Nan value\n",
    "for stock in stocks:\n",
    "    add_col = merged_df[[stock+'_close']].isnull().astype(int)\n",
    "    merged_df[stock+\"_isnull\"] = add_col\n",
    "\n",
    "# fill existing Nans with last available value\n",
    "merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# add target\n",
    "merged_df['target'] = merged_df[STOCK+'_close'].shift(-10) / merged_df[STOCK+'_close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(merged_df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "merged_df = merged_df[mask]\n",
    "\n",
    "# drop any remaining Nans\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "# drop test set\n",
    "merged_df = merged_df[merged_df.index < SPLIT_DATE2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making sure no Nans left in the datafame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NdJy4IfpbxcK",
    "outputId": "827e45a0-8c6d-4c4f-a3c6-cc31cc7f1cba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "om06MUC9cdjV",
    "outputId": "3bdda2b9-b9cf-4a06-e6d2-99ab3741740c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_close</th>\n",
       "      <th>SPY_volume</th>\n",
       "      <th>AAPL_close</th>\n",
       "      <th>AAPL_volume</th>\n",
       "      <th>BAC_close</th>\n",
       "      <th>BAC_volume</th>\n",
       "      <th>GE_close</th>\n",
       "      <th>GE_volume</th>\n",
       "      <th>GOOGL_close</th>\n",
       "      <th>GOOGL_volume</th>\n",
       "      <th>JNJ_close</th>\n",
       "      <th>JNJ_volume</th>\n",
       "      <th>KO_close</th>\n",
       "      <th>KO_volume</th>\n",
       "      <th>MSFT_close</th>\n",
       "      <th>MSFT_volume</th>\n",
       "      <th>PG_close</th>\n",
       "      <th>PG_volume</th>\n",
       "      <th>T_close</th>\n",
       "      <th>T_volume</th>\n",
       "      <th>XOM_close</th>\n",
       "      <th>XOM_volume</th>\n",
       "      <th>AAPL_isnull</th>\n",
       "      <th>BAC_isnull</th>\n",
       "      <th>GE_isnull</th>\n",
       "      <th>GOOGL_isnull</th>\n",
       "      <th>JNJ_isnull</th>\n",
       "      <th>KO_isnull</th>\n",
       "      <th>MSFT_isnull</th>\n",
       "      <th>PG_isnull</th>\n",
       "      <th>T_isnull</th>\n",
       "      <th>XOM_isnull</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:30:00</th>\n",
       "      <td>234.61</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>135.120</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>24.47</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>30.41</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>841.285</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>117.645</td>\n",
       "      <td>0.2867</td>\n",
       "      <td>40.78</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>65.08</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>90.94</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>41.21</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>82.97</td>\n",
       "      <td>0.2457</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:31:00</th>\n",
       "      <td>234.68</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>135.151</td>\n",
       "      <td>0.4686</td>\n",
       "      <td>24.47</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>30.41</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>841.670</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>117.590</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>40.79</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>65.10</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>90.94</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>82.93</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:32:00</th>\n",
       "      <td>234.71</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>135.130</td>\n",
       "      <td>0.4705</td>\n",
       "      <td>24.47</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>30.42</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>841.870</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>117.590</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>40.81</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>65.10</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>90.99</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>82.92</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:33:00</th>\n",
       "      <td>234.74</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>135.143</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>24.47</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>30.44</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>842.010</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>117.640</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>40.83</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>65.09</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>90.99</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>41.27</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>82.94</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-16 17:34:00</th>\n",
       "      <td>234.89</td>\n",
       "      <td>0.4003</td>\n",
       "      <td>135.337</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>24.49</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>30.45</td>\n",
       "      <td>0.0587</td>\n",
       "      <td>842.270</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>117.640</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>40.84</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>65.15</td>\n",
       "      <td>0.1658</td>\n",
       "      <td>90.99</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>41.29</td>\n",
       "      <td>0.0719</td>\n",
       "      <td>83.02</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     SPY_close  SPY_volume  ...  XOM_isnull    target\n",
       "datetime                                    ...                      \n",
       "2017-02-16 17:30:00     234.61      0.2130  ...           0  0.001364\n",
       "2017-02-16 17:31:00     234.68      0.1257  ...           0  0.001108\n",
       "2017-02-16 17:32:00     234.71      0.2515  ...           0  0.000639\n",
       "2017-02-16 17:33:00     234.74      0.1152  ...           0  0.000298\n",
       "2017-02-16 17:34:00     234.89      0.4003  ...           0 -0.000298\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate simple linear windows of size 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGwcxybtgvob"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(merged_df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(merged_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "# split to train-windows and validation-windows\n",
    "num_train_windows = len(merged_df[merged_df.index < SPLIT_DATE1])\n",
    "train_x = windows_x[:num_train_windows]\n",
    "train_y = windows_y[:num_train_windows]\n",
    "valid_x = windows_x[num_train_windows:]\n",
    "valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "# create data\n",
    "lgb_train = lgb.Dataset(train_x, train_y.flatten())\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkwiemlBd9SE"
   },
   "source": [
    "**modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvyFHh-pTMjL"
   },
   "outputs": [],
   "source": [
    "# create hyperparameters for grid search\n",
    "search_hps = {\n",
    "    'num_iterations': [500],\n",
    "    'early_stopping_round' : [50],\n",
    "    'max_depth': [-1,4,8,12],\n",
    "    'num_leaves': [15,63,255],\n",
    "    'bagging_fraction': [0.3,0.7],\n",
    "    'feature_fraction': [0.3,0.7],\n",
    "    'learning_rate': [0.003],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['mean_squared_error'],\n",
    "    'seed': [1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "all_search_hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in all_search_hps]\n",
    "\n",
    "len(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRj1b-pSgOBb"
   },
   "outputs": [],
   "source": [
    "# grid search\n",
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "    # train\n",
    "    model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "\n",
    "    # save\n",
    "    dump(model, f\"{save_dir}{STOCK}_all_stocks_one_lgbm{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ynojB82o2AZF",
    "outputId": "a27115b8-3950-49c8-b43f-482f11f043f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "      <th>lgbm_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.406809e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.271291e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526516</td>\n",
       "      <td>0.460120</td>\n",
       "      <td>0.481546</td>\n",
       "      <td>0.505528</td>\n",
       "      <td>0.324908</td>\n",
       "      <td>0.675092</td>\n",
       "      <td>0.496157</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.406809e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.271291e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526516</td>\n",
       "      <td>0.460120</td>\n",
       "      <td>0.481546</td>\n",
       "      <td>0.505528</td>\n",
       "      <td>0.324908</td>\n",
       "      <td>0.675092</td>\n",
       "      <td>0.496157</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.406878e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.208456e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525601</td>\n",
       "      <td>0.460858</td>\n",
       "      <td>0.481280</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.327849</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>0.495810</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.406878e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.208456e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525601</td>\n",
       "      <td>0.460858</td>\n",
       "      <td>0.481280</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.327849</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>0.495810</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.406878e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.208456e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525601</td>\n",
       "      <td>0.460858</td>\n",
       "      <td>0.481280</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.327849</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>0.495810</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.406878e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.208456e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525601</td>\n",
       "      <td>0.460858</td>\n",
       "      <td>0.481280</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.327849</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>0.495810</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8.408080e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.024311e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526095</td>\n",
       "      <td>0.461038</td>\n",
       "      <td>0.479556</td>\n",
       "      <td>0.507301</td>\n",
       "      <td>0.268472</td>\n",
       "      <td>0.731528</td>\n",
       "      <td>0.492051</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.408080e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.024311e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526095</td>\n",
       "      <td>0.461038</td>\n",
       "      <td>0.479556</td>\n",
       "      <td>0.507301</td>\n",
       "      <td>0.268472</td>\n",
       "      <td>0.731528</td>\n",
       "      <td>0.492051</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.406725e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.012036e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522230</td>\n",
       "      <td>0.464588</td>\n",
       "      <td>0.478911</td>\n",
       "      <td>0.508070</td>\n",
       "      <td>0.302045</td>\n",
       "      <td>0.697955</td>\n",
       "      <td>0.491995</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.406725e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.012036e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522230</td>\n",
       "      <td>0.464588</td>\n",
       "      <td>0.478911</td>\n",
       "      <td>0.508070</td>\n",
       "      <td>0.302045</td>\n",
       "      <td>0.697955</td>\n",
       "      <td>0.491995</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.406733e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.539416e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.464018</td>\n",
       "      <td>0.479104</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.299575</td>\n",
       "      <td>0.700425</td>\n",
       "      <td>0.492078</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.406733e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.539416e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.464018</td>\n",
       "      <td>0.479104</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.299575</td>\n",
       "      <td>0.700425</td>\n",
       "      <td>0.492078</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.406733e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.539416e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.464018</td>\n",
       "      <td>0.479104</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.299575</td>\n",
       "      <td>0.700425</td>\n",
       "      <td>0.492078</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.406733e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.539416e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.464018</td>\n",
       "      <td>0.479104</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.299575</td>\n",
       "      <td>0.700425</td>\n",
       "      <td>0.492078</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.408197e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.285706e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526919</td>\n",
       "      <td>0.460138</td>\n",
       "      <td>0.479016</td>\n",
       "      <td>0.507877</td>\n",
       "      <td>0.234747</td>\n",
       "      <td>0.765253</td>\n",
       "      <td>0.490261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.408197e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>9.285706e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526919</td>\n",
       "      <td>0.460138</td>\n",
       "      <td>0.479016</td>\n",
       "      <td>0.507877</td>\n",
       "      <td>0.234747</td>\n",
       "      <td>0.765253</td>\n",
       "      <td>0.490261</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.407795e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>8.822203e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527981</td>\n",
       "      <td>0.459121</td>\n",
       "      <td>0.479845</td>\n",
       "      <td>0.507028</td>\n",
       "      <td>0.253850</td>\n",
       "      <td>0.746150</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.407795e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>8.822203e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527981</td>\n",
       "      <td>0.459121</td>\n",
       "      <td>0.479845</td>\n",
       "      <td>0.507028</td>\n",
       "      <td>0.253850</td>\n",
       "      <td>0.746150</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.408113e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.994499e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526764</td>\n",
       "      <td>0.460625</td>\n",
       "      <td>0.478224</td>\n",
       "      <td>0.508588</td>\n",
       "      <td>0.206820</td>\n",
       "      <td>0.793180</td>\n",
       "      <td>0.488263</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8.408113e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.994499e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526764</td>\n",
       "      <td>0.460625</td>\n",
       "      <td>0.478224</td>\n",
       "      <td>0.508588</td>\n",
       "      <td>0.206820</td>\n",
       "      <td>0.793180</td>\n",
       "      <td>0.488263</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.408095e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.947023e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526810</td>\n",
       "      <td>0.460573</td>\n",
       "      <td>0.478236</td>\n",
       "      <td>0.508578</td>\n",
       "      <td>0.206723</td>\n",
       "      <td>0.793277</td>\n",
       "      <td>0.488277</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.408095e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.947023e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526810</td>\n",
       "      <td>0.460573</td>\n",
       "      <td>0.478236</td>\n",
       "      <td>0.508578</td>\n",
       "      <td>0.206723</td>\n",
       "      <td>0.793277</td>\n",
       "      <td>0.488277</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.407956e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.339868e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527235</td>\n",
       "      <td>0.460190</td>\n",
       "      <td>0.478479</td>\n",
       "      <td>0.508320</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>0.787076</td>\n",
       "      <td>0.488860</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.407956e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>7.339868e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527235</td>\n",
       "      <td>0.460190</td>\n",
       "      <td>0.478479</td>\n",
       "      <td>0.508320</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>0.787076</td>\n",
       "      <td>0.488860</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8.407936e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.398199e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.531110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.477683</td>\n",
       "      <td>0.509309</td>\n",
       "      <td>0.139355</td>\n",
       "      <td>0.860645</td>\n",
       "      <td>0.485128</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8.407936e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>6.398199e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.531110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.477683</td>\n",
       "      <td>0.509309</td>\n",
       "      <td>0.139355</td>\n",
       "      <td>0.860645</td>\n",
       "      <td>0.485128</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.407929e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.980248e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522055</td>\n",
       "      <td>0.464013</td>\n",
       "      <td>0.477498</td>\n",
       "      <td>0.509671</td>\n",
       "      <td>0.216073</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.487126</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.407929e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>5.980248e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522055</td>\n",
       "      <td>0.464013</td>\n",
       "      <td>0.477498</td>\n",
       "      <td>0.509671</td>\n",
       "      <td>0.216073</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.487126</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8.407872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>4.698849e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521789</td>\n",
       "      <td>0.465247</td>\n",
       "      <td>0.477976</td>\n",
       "      <td>0.508917</td>\n",
       "      <td>0.266460</td>\n",
       "      <td>0.733540</td>\n",
       "      <td>0.489651</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8.407872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>4.698849e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521789</td>\n",
       "      <td>0.465247</td>\n",
       "      <td>0.477976</td>\n",
       "      <td>0.508917</td>\n",
       "      <td>0.266460</td>\n",
       "      <td>0.733540</td>\n",
       "      <td>0.489651</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.408380e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.552571e-06</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>0.465096</td>\n",
       "      <td>0.475599</td>\n",
       "      <td>0.511087</td>\n",
       "      <td>0.096584</td>\n",
       "      <td>0.903416</td>\n",
       "      <td>0.480286</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.408380e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.552571e-06</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>0.465096</td>\n",
       "      <td>0.475599</td>\n",
       "      <td>0.511087</td>\n",
       "      <td>0.096584</td>\n",
       "      <td>0.903416</td>\n",
       "      <td>0.480286</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.408062e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.720411e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.536657</td>\n",
       "      <td>0.450785</td>\n",
       "      <td>0.476518</td>\n",
       "      <td>0.510372</td>\n",
       "      <td>0.075123</td>\n",
       "      <td>0.924877</td>\n",
       "      <td>0.481035</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.408062e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.720411e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.536657</td>\n",
       "      <td>0.450785</td>\n",
       "      <td>0.476518</td>\n",
       "      <td>0.510372</td>\n",
       "      <td>0.075123</td>\n",
       "      <td>0.924877</td>\n",
       "      <td>0.481035</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.407998e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.086200e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.534558</td>\n",
       "      <td>0.454653</td>\n",
       "      <td>0.476372</td>\n",
       "      <td>0.510355</td>\n",
       "      <td>0.082295</td>\n",
       "      <td>0.917705</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.407998e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2.086200e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.534558</td>\n",
       "      <td>0.454653</td>\n",
       "      <td>0.476372</td>\n",
       "      <td>0.510355</td>\n",
       "      <td>0.082295</td>\n",
       "      <td>0.917705</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.408148e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.872468e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.537066</td>\n",
       "      <td>0.450513</td>\n",
       "      <td>0.476407</td>\n",
       "      <td>0.510476</td>\n",
       "      <td>0.070364</td>\n",
       "      <td>0.929636</td>\n",
       "      <td>0.480675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.408148e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.872468e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.537066</td>\n",
       "      <td>0.450513</td>\n",
       "      <td>0.476407</td>\n",
       "      <td>0.510476</td>\n",
       "      <td>0.070364</td>\n",
       "      <td>0.929636</td>\n",
       "      <td>0.480675</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8.408368e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.652382e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.527910</td>\n",
       "      <td>0.459104</td>\n",
       "      <td>0.476171</td>\n",
       "      <td>0.510752</td>\n",
       "      <td>0.092950</td>\n",
       "      <td>0.907050</td>\n",
       "      <td>0.480980</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8.408368e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.652382e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.527910</td>\n",
       "      <td>0.459104</td>\n",
       "      <td>0.476171</td>\n",
       "      <td>0.510752</td>\n",
       "      <td>0.092950</td>\n",
       "      <td>0.907050</td>\n",
       "      <td>0.480980</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8.408150e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.529741e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.536412</td>\n",
       "      <td>0.451155</td>\n",
       "      <td>0.476356</td>\n",
       "      <td>0.510527</td>\n",
       "      <td>0.070295</td>\n",
       "      <td>0.929705</td>\n",
       "      <td>0.480578</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8.408150e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.529741e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.536412</td>\n",
       "      <td>0.451155</td>\n",
       "      <td>0.476356</td>\n",
       "      <td>0.510527</td>\n",
       "      <td>0.070295</td>\n",
       "      <td>0.929705</td>\n",
       "      <td>0.480578</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.408364e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>8.322401e-07</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.534702</td>\n",
       "      <td>0.455755</td>\n",
       "      <td>0.475533</td>\n",
       "      <td>0.511221</td>\n",
       "      <td>0.047973</td>\n",
       "      <td>0.952027</td>\n",
       "      <td>0.478372</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.408364e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>8.322401e-07</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.534702</td>\n",
       "      <td>0.455755</td>\n",
       "      <td>0.475533</td>\n",
       "      <td>0.511221</td>\n",
       "      <td>0.047973</td>\n",
       "      <td>0.952027</td>\n",
       "      <td>0.478372</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.408417e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>4.320351e-08</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.529893</td>\n",
       "      <td>0.456311</td>\n",
       "      <td>0.475094</td>\n",
       "      <td>0.511857</td>\n",
       "      <td>0.027150</td>\n",
       "      <td>0.972850</td>\n",
       "      <td>0.476582</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8.408417e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>4.320351e-08</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.529893</td>\n",
       "      <td>0.456311</td>\n",
       "      <td>0.475094</td>\n",
       "      <td>0.511857</td>\n",
       "      <td>0.027150</td>\n",
       "      <td>0.972850</td>\n",
       "      <td>0.476582</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8.408374e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.052397e-06</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.558743</td>\n",
       "      <td>0.430328</td>\n",
       "      <td>0.475039</td>\n",
       "      <td>0.511871</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.989845</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8.408374e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-1.052397e-06</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.558743</td>\n",
       "      <td>0.430328</td>\n",
       "      <td>0.475039</td>\n",
       "      <td>0.511871</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.989845</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse  true_pnl       avg_pnl  ...   max_win  max_loss  lgbm_num\n",
       "12  8.406809e-07  0.000617  1.271291e-05  ...  0.014526 -0.013981        12\n",
       "14  8.406809e-07  0.000617  1.271291e-05  ...  0.014526 -0.013981        14\n",
       "22  8.406878e-07  0.000617  1.208456e-05  ...  0.014526 -0.013981        22\n",
       "16  8.406878e-07  0.000617  1.208456e-05  ...  0.014526 -0.013981        16\n",
       "20  8.406878e-07  0.000617  1.208456e-05  ...  0.014526 -0.013981        20\n",
       "18  8.406878e-07  0.000617  1.208456e-05  ...  0.014526 -0.013981        18\n",
       "34  8.408080e-07  0.000617  1.024311e-05  ...  0.014526 -0.013981        34\n",
       "32  8.408080e-07  0.000617  1.024311e-05  ...  0.014526 -0.013981        32\n",
       "13  8.406725e-07  0.000617  1.012036e-05  ...  0.014526 -0.013981        13\n",
       "15  8.406725e-07  0.000617  1.012036e-05  ...  0.014526 -0.013981        15\n",
       "21  8.406733e-07  0.000617  9.539416e-06  ...  0.014526 -0.013981        21\n",
       "19  8.406733e-07  0.000617  9.539416e-06  ...  0.014526 -0.013981        19\n",
       "17  8.406733e-07  0.000617  9.539416e-06  ...  0.014526 -0.013981        17\n",
       "23  8.406733e-07  0.000617  9.539416e-06  ...  0.014526 -0.013981        23\n",
       "30  8.408197e-07  0.000617  9.285706e-06  ...  0.014526 -0.013981        30\n",
       "28  8.408197e-07  0.000617  9.285706e-06  ...  0.014526 -0.013981        28\n",
       "11  8.407795e-07  0.000617  8.822203e-06  ...  0.014526 -0.013981        11\n",
       "9   8.407795e-07  0.000617  8.822203e-06  ...  0.014526 -0.013981         9\n",
       "36  8.408113e-07  0.000617  7.994499e-06  ...  0.014526 -0.013981        36\n",
       "38  8.408113e-07  0.000617  7.994499e-06  ...  0.014526 -0.013981        38\n",
       "2   8.408095e-07  0.000617  7.947023e-06  ...  0.014526 -0.013981         2\n",
       "0   8.408095e-07  0.000617  7.947023e-06  ...  0.014526 -0.013981         0\n",
       "26  8.407956e-07  0.000617  7.339868e-06  ...  0.014526 -0.013981        26\n",
       "24  8.407956e-07  0.000617  7.339868e-06  ...  0.014526 -0.013981        24\n",
       "43  8.407936e-07  0.000617  6.398199e-06  ...  0.014526 -0.013981        43\n",
       "41  8.407936e-07  0.000617  6.398199e-06  ...  0.014526 -0.013981        41\n",
       "29  8.407929e-07  0.000617  5.980248e-06  ...  0.014526 -0.013981        29\n",
       "31  8.407929e-07  0.000617  5.980248e-06  ...  0.014526 -0.013981        31\n",
       "47  8.407872e-07  0.000617  4.698849e-06  ...  0.014526 -0.013981        47\n",
       "45  8.407872e-07  0.000617  4.698849e-06  ...  0.014526 -0.013981        45\n",
       "10  8.408380e-07  0.000617  3.552571e-06  ...  0.014526 -0.013981        10\n",
       "8   8.408380e-07  0.000617  3.552571e-06  ...  0.014526 -0.013981         8\n",
       "27  8.408062e-07  0.000617  2.720411e-06  ...  0.014526 -0.013981        27\n",
       "25  8.408062e-07  0.000617  2.720411e-06  ...  0.014526 -0.013981        25\n",
       "7   8.407998e-07  0.000617  2.086200e-06  ...  0.014526 -0.013981         7\n",
       "5   8.407998e-07  0.000617  2.086200e-06  ...  0.014526 -0.013981         5\n",
       "3   8.408148e-07  0.000617  1.872468e-06  ...  0.014526 -0.013981         3\n",
       "1   8.408148e-07  0.000617  1.872468e-06  ...  0.014526 -0.013981         1\n",
       "35  8.408368e-07  0.000617  1.652382e-06  ...  0.013981 -0.014526        35\n",
       "33  8.408368e-07  0.000617  1.652382e-06  ...  0.013981 -0.014526        33\n",
       "37  8.408150e-07  0.000617  1.529741e-06  ...  0.014526 -0.013981        37\n",
       "39  8.408150e-07  0.000617  1.529741e-06  ...  0.014526 -0.013981        39\n",
       "6   8.408364e-07  0.000617  8.322401e-07  ...  0.014526 -0.013493         6\n",
       "4   8.408364e-07  0.000617  8.322401e-07  ...  0.014526 -0.013493         4\n",
       "44  8.408417e-07  0.000617  4.320351e-08  ...  0.014526 -0.013493        44\n",
       "46  8.408417e-07  0.000617  4.320351e-08  ...  0.014526 -0.013493        46\n",
       "40  8.408374e-07  0.000617 -1.052397e-06  ...  0.014526 -0.013493        40\n",
       "42  8.408374e-07  0.000617 -1.052397e-06  ...  0.014526 -0.013493        42\n",
       "\n",
       "[48 rows x 14 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "summaries = []\n",
    "\n",
    "for i in range(len(hps)):\n",
    "    \n",
    "    # evaluate\n",
    "    model = load(f\"{save_dir}{STOCK}_all_stocks_one_lgbm{i}\")\n",
    "    prediction = model.predict(valid_x)\n",
    "    summary = evaluate_model(valid_y.flatten(), prediction, f\"lgbm_all_{i}\")\n",
    "    summary['lgbm_num'] = i\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vutfqu3C2VpB"
   },
   "source": [
    "Relatively good results,  \n",
    "though the down/up ratios are not so balance,  \n",
    "and the `avg_pnl` is not the highest we've seen so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80SsHUpgPSR_"
   },
   "source": [
    "### 4.2) model for each stock - LGBM\n",
    "\n",
    "Here, for each stock, we try to build LightGBM model that predict the the ***`SPY`*** 10 minute return,\n",
    "and then, from each stock, we take the LightGBM model built on it, and feed its predictions into one LightGBM model.  \n",
    "\n",
    "Below is a full diagram of the overall model:\n",
    "<br/>\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=1a3eUMiEWPgoWnBRgtZII-Ps5AYVM_lmP)\n",
    "\n",
    "\n",
    "Hopefully, each stock's model will specialize at predicting 'SPY' future returns using only its own stock's data,  \n",
    "while all models' predictions are fed into one model, which will make his predictions based on theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be3Ho5pQUxh5"
   },
   "source": [
    "Same preprocess as the one before, in section 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVMx1OL0iSo-"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for stock in [STOCK] + stocks:\n",
    "    df = read_data(data_dir, stock)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df[['close', 'volume']]\n",
    "    df.columns = [stock+\"_\"+col for col in df.columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZroXYe6UUuhZ"
   },
   "outputs": [],
   "source": [
    "# drop all rows with Nan in the SPY data\n",
    "merged_df.dropna(subset=[STOCK+'_close', STOCK+'_volume'], inplace=True)\n",
    "\n",
    "# add flag column for each stock, to signal for Nan value\n",
    "for stock in stocks:\n",
    "    add_col = merged_df[[stock+'_close']].isnull().astype(int)\n",
    "    merged_df[stock+\"_isnull\"] = add_col\n",
    "\n",
    "# fill existing Nans with last available value\n",
    "merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# add target\n",
    "merged_df['target'] = merged_df[STOCK+'_close'].shift(-10) / merged_df[STOCK+'_close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(merged_df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "merged_df = merged_df[mask]\n",
    "\n",
    "# drop any remaining Nans\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "# drop test set\n",
    "merged_df = merged_df[merged_df.index < SPLIT_DATE2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKTKl-VGQOFU"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "window_size = 10\n",
    "\n",
    "windows_y = df_windows(merged_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "train_preds = {}\n",
    "valid_preds = {}\n",
    "for stock in [STOCK] + stocks:\n",
    "\n",
    "    print('-'*10 + stock + '-'*10)\n",
    "    \n",
    "    # windows generation\n",
    "    cols = [col for col in merged_df.columns if stock == col.split(\"_\")[0]]\n",
    "    windows_x = df_windows(merged_df[cols], window_size)\n",
    "    windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "    windows_x = np.array([win.flatten() for win in windows_x])\n",
    "\n",
    "    # split to train-windows and validation-windows\n",
    "    num_train_windows = len(merged_df[merged_df.index < SPLIT_DATE1])\n",
    "    train_x = windows_x[:num_train_windows]\n",
    "    train_y = windows_y[:num_train_windows]\n",
    "    valid_x = windows_x[num_train_windows:]\n",
    "    valid_y = windows_y[num_train_windows:]\n",
    "\n",
    "    # create data\n",
    "    lgb_train = lgb.Dataset(train_x, train_y.flatten())\n",
    "    lgb_valid = lgb.Dataset(valid_x, valid_y.flatten())\n",
    "\n",
    "    # grid search\n",
    "    best_model = None\n",
    "    best_avg_pnl = float(\"-inf\")\n",
    "    for i, hp in enumerate(hps):\n",
    "        print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "        # train\n",
    "        model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "\n",
    "        # pick best model for current stock\n",
    "        prediction = model.predict(valid_x)\n",
    "        avg_pnl = evaluate_model(valid_y.flatten(), prediction)['avg_pnl']\n",
    "        if avg_pnl > best_avg_pnl:\n",
    "            best_model = model\n",
    "            best_avg_pnl = avg_pnl\n",
    "        \n",
    "    print(f\"best avg pnl = {best_avg_pnl}\")\n",
    "    \n",
    "    # save best model\n",
    "    dump(best_model, f\"{save_dir}lgbm_prediction_as_input_{stock}.joblib\")\n",
    "\n",
    "    # predict validation, save result for later\n",
    "    train_preds[stock] = best_model.predict(train_x)\n",
    "    valid_preds[stock] = best_model.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igfSwG7h4MgO"
   },
   "outputs": [],
   "source": [
    "main_model_train_x = np.stack([preds for stock, preds in train_preds.items()], axis=1)\n",
    "main_model_valid_x = np.stack([preds for stock, preds in valid_preds.items()], axis=1)\n",
    "main_model_train_y = train_y\n",
    "main_model_valid_y = valid_y\n",
    "\n",
    "lgb_train = lgb.Dataset(main_model_train_x, main_model_train_y.flatten())\n",
    "lgb_valid = lgb.Dataset(main_model_valid_x, main_model_valid_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aAHdD1l4NnI"
   },
   "outputs": [],
   "source": [
    "search_hps = {\n",
    "    'num_iterations': [500],\n",
    "    'early_stopping_round' : [50],\n",
    "    'max_depth': [-1,4,8,12],\n",
    "    'num_leaves': [15,63,255],\n",
    "    'bagging_fraction': [0.3,0.7],\n",
    "    'feature_fraction': [0.3,0.7],\n",
    "    'learning_rate': [0.003],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['mean_squared_error'],\n",
    "    'seed': [1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "all_search_hps = list(itertools.product(*search_hps.values()))\n",
    "hps = [dict(zip(search_hps.keys(), hp)) for hp in all_search_hps]\n",
    "\n",
    "len(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7Azpsn34i9H"
   },
   "outputs": [],
   "source": [
    "for i, hp in enumerate(hps):\n",
    "    print(f\"{i}/{len(hps)}\")\n",
    "\n",
    "    # train\n",
    "    model = lgb.train(hp, lgb_train, valid_sets=lgb_valid, verbose_eval=False)\n",
    "\n",
    "    # save\n",
    "    dump(model, f\"{save_dir}{STOCK}_main_pred_input_lgbm_model_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SQ2f6-o8_QFM",
    "outputId": "3dc2d64f-cb77-42e4-9fdf-5a73afb84a74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "      <th>lgbm_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.406895e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.366111</td>\n",
       "      <td>0.633889</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.406895e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.366111</td>\n",
       "      <td>0.633889</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8.406872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.406872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.406872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.406872e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8.407297e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520145</td>\n",
       "      <td>0.467472</td>\n",
       "      <td>0.477903</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.318165</td>\n",
       "      <td>0.681835</td>\n",
       "      <td>0.491343</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8.407297e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520145</td>\n",
       "      <td>0.467472</td>\n",
       "      <td>0.477903</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.318165</td>\n",
       "      <td>0.681835</td>\n",
       "      <td>0.491343</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8.406798e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520907</td>\n",
       "      <td>0.466265</td>\n",
       "      <td>0.479490</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.370939</td>\n",
       "      <td>0.629061</td>\n",
       "      <td>0.494853</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.406798e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520907</td>\n",
       "      <td>0.466265</td>\n",
       "      <td>0.479490</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.370939</td>\n",
       "      <td>0.629061</td>\n",
       "      <td>0.494853</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8.407000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521189</td>\n",
       "      <td>0.466374</td>\n",
       "      <td>0.478217</td>\n",
       "      <td>0.508435</td>\n",
       "      <td>0.306748</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.491399</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8.407000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521189</td>\n",
       "      <td>0.466374</td>\n",
       "      <td>0.478217</td>\n",
       "      <td>0.508435</td>\n",
       "      <td>0.306748</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.491399</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.406712e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520358</td>\n",
       "      <td>0.466995</td>\n",
       "      <td>0.479141</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.375142</td>\n",
       "      <td>0.624858</td>\n",
       "      <td>0.494603</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.406712e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520358</td>\n",
       "      <td>0.466995</td>\n",
       "      <td>0.479141</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.375142</td>\n",
       "      <td>0.624858</td>\n",
       "      <td>0.494603</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.406666e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521275</td>\n",
       "      <td>0.466116</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>0.507970</td>\n",
       "      <td>0.328973</td>\n",
       "      <td>0.671027</td>\n",
       "      <td>0.492731</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.406666e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521275</td>\n",
       "      <td>0.466116</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>0.507970</td>\n",
       "      <td>0.328973</td>\n",
       "      <td>0.671027</td>\n",
       "      <td>0.492731</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.407206e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521068</td>\n",
       "      <td>0.466196</td>\n",
       "      <td>0.478497</td>\n",
       "      <td>0.508280</td>\n",
       "      <td>0.318054</td>\n",
       "      <td>0.681946</td>\n",
       "      <td>0.492037</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.407206e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521068</td>\n",
       "      <td>0.466196</td>\n",
       "      <td>0.478497</td>\n",
       "      <td>0.508280</td>\n",
       "      <td>0.318054</td>\n",
       "      <td>0.681946</td>\n",
       "      <td>0.492037</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.407386e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520372</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.477625</td>\n",
       "      <td>0.509073</td>\n",
       "      <td>0.289767</td>\n",
       "      <td>0.710233</td>\n",
       "      <td>0.490011</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.407386e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520372</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.477625</td>\n",
       "      <td>0.509073</td>\n",
       "      <td>0.289767</td>\n",
       "      <td>0.710233</td>\n",
       "      <td>0.490011</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.407386e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520372</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.477625</td>\n",
       "      <td>0.509073</td>\n",
       "      <td>0.289767</td>\n",
       "      <td>0.710233</td>\n",
       "      <td>0.490011</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.407386e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520372</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.477625</td>\n",
       "      <td>0.509073</td>\n",
       "      <td>0.289767</td>\n",
       "      <td>0.710233</td>\n",
       "      <td>0.490011</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.407390e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520345</td>\n",
       "      <td>0.467161</td>\n",
       "      <td>0.477614</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.289809</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.489998</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.407390e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520345</td>\n",
       "      <td>0.467161</td>\n",
       "      <td>0.477614</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.289809</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.489998</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.406880e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520494</td>\n",
       "      <td>0.466930</td>\n",
       "      <td>0.478924</td>\n",
       "      <td>0.507728</td>\n",
       "      <td>0.361824</td>\n",
       "      <td>0.638176</td>\n",
       "      <td>0.493965</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8.406880e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520494</td>\n",
       "      <td>0.466930</td>\n",
       "      <td>0.478924</td>\n",
       "      <td>0.507728</td>\n",
       "      <td>0.361824</td>\n",
       "      <td>0.638176</td>\n",
       "      <td>0.493965</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.406921e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519828</td>\n",
       "      <td>0.467265</td>\n",
       "      <td>0.478898</td>\n",
       "      <td>0.507938</td>\n",
       "      <td>0.370828</td>\n",
       "      <td>0.629172</td>\n",
       "      <td>0.494076</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.406921e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519828</td>\n",
       "      <td>0.467265</td>\n",
       "      <td>0.478898</td>\n",
       "      <td>0.507938</td>\n",
       "      <td>0.370828</td>\n",
       "      <td>0.629172</td>\n",
       "      <td>0.494076</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.406628e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>0.467522</td>\n",
       "      <td>0.478712</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.368844</td>\n",
       "      <td>0.631156</td>\n",
       "      <td>0.493799</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.406628e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>0.467522</td>\n",
       "      <td>0.478712</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.368844</td>\n",
       "      <td>0.631156</td>\n",
       "      <td>0.493799</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.407485e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520511</td>\n",
       "      <td>0.467060</td>\n",
       "      <td>0.476931</td>\n",
       "      <td>0.509801</td>\n",
       "      <td>0.237743</td>\n",
       "      <td>0.762257</td>\n",
       "      <td>0.487292</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.407485e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520511</td>\n",
       "      <td>0.467060</td>\n",
       "      <td>0.476931</td>\n",
       "      <td>0.509801</td>\n",
       "      <td>0.237743</td>\n",
       "      <td>0.762257</td>\n",
       "      <td>0.487292</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8.406613e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519309</td>\n",
       "      <td>0.467644</td>\n",
       "      <td>0.478680</td>\n",
       "      <td>0.508239</td>\n",
       "      <td>0.371091</td>\n",
       "      <td>0.628909</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8.406613e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519309</td>\n",
       "      <td>0.467644</td>\n",
       "      <td>0.478680</td>\n",
       "      <td>0.508239</td>\n",
       "      <td>0.371091</td>\n",
       "      <td>0.628909</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8.407406e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520545</td>\n",
       "      <td>0.467081</td>\n",
       "      <td>0.476881</td>\n",
       "      <td>0.509838</td>\n",
       "      <td>0.234316</td>\n",
       "      <td>0.765684</td>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8.407406e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520545</td>\n",
       "      <td>0.467081</td>\n",
       "      <td>0.476881</td>\n",
       "      <td>0.509838</td>\n",
       "      <td>0.234316</td>\n",
       "      <td>0.765684</td>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.407710e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.520048</td>\n",
       "      <td>0.467842</td>\n",
       "      <td>0.476336</td>\n",
       "      <td>0.510346</td>\n",
       "      <td>0.206210</td>\n",
       "      <td>0.793790</td>\n",
       "      <td>0.485350</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.407710e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.520048</td>\n",
       "      <td>0.467842</td>\n",
       "      <td>0.476336</td>\n",
       "      <td>0.510346</td>\n",
       "      <td>0.206210</td>\n",
       "      <td>0.793790</td>\n",
       "      <td>0.485350</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8.407692e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.519155</td>\n",
       "      <td>0.468668</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>0.210760</td>\n",
       "      <td>0.789240</td>\n",
       "      <td>0.485225</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.407692e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.519155</td>\n",
       "      <td>0.468668</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>0.210760</td>\n",
       "      <td>0.789240</td>\n",
       "      <td>0.485225</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8.407692e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.519155</td>\n",
       "      <td>0.468668</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>0.210760</td>\n",
       "      <td>0.789240</td>\n",
       "      <td>0.485225</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.407692e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.519155</td>\n",
       "      <td>0.468668</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>0.210760</td>\n",
       "      <td>0.789240</td>\n",
       "      <td>0.485225</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.408007e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.408029e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.408007e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.408029e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.408029e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.408029e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.519449</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.849852</td>\n",
       "      <td>0.482187</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse  true_pnl   avg_pnl  ...   max_win  max_loss  lgbm_num\n",
       "24  8.406895e-07  0.000617  0.000016  ...  0.012759 -0.014526        24\n",
       "26  8.406895e-07  0.000617  0.000016  ...  0.012759 -0.014526        26\n",
       "38  8.406872e-07  0.000617  0.000016  ...  0.012759 -0.014526        38\n",
       "36  8.406872e-07  0.000617  0.000016  ...  0.012759 -0.014526        36\n",
       "0   8.406872e-07  0.000617  0.000016  ...  0.012759 -0.014526         0\n",
       "2   8.406872e-07  0.000617  0.000016  ...  0.012759 -0.014526         2\n",
       "43  8.407297e-07  0.000617  0.000015  ...  0.012759 -0.014526        43\n",
       "41  8.407297e-07  0.000617  0.000015  ...  0.012759 -0.014526        41\n",
       "46  8.406798e-07  0.000617  0.000015  ...  0.012759 -0.014526        46\n",
       "44  8.406798e-07  0.000617  0.000015  ...  0.012759 -0.014526        44\n",
       "47  8.407000e-07  0.000617  0.000015  ...  0.012759 -0.014526        47\n",
       "45  8.407000e-07  0.000617  0.000015  ...  0.012759 -0.014526        45\n",
       "8   8.406712e-07  0.000617  0.000015  ...  0.012759 -0.014526         8\n",
       "10  8.406712e-07  0.000617  0.000015  ...  0.012759 -0.014526        10\n",
       "9   8.406666e-07  0.000617  0.000015  ...  0.012759 -0.014526         9\n",
       "11  8.406666e-07  0.000617  0.000015  ...  0.012759 -0.014526        11\n",
       "5   8.407206e-07  0.000617  0.000015  ...  0.012759 -0.014526         5\n",
       "7   8.407206e-07  0.000617  0.000015  ...  0.012759 -0.014526         7\n",
       "20  8.407386e-07  0.000617  0.000014  ...  0.012759 -0.014526        20\n",
       "22  8.407386e-07  0.000617  0.000014  ...  0.012759 -0.014526        22\n",
       "16  8.407386e-07  0.000617  0.000014  ...  0.012759 -0.014526        16\n",
       "18  8.407386e-07  0.000617  0.000014  ...  0.012759 -0.014526        18\n",
       "12  8.407390e-07  0.000617  0.000014  ...  0.012759 -0.014526        12\n",
       "14  8.407390e-07  0.000617  0.000014  ...  0.012759 -0.014526        14\n",
       "32  8.406880e-07  0.000617  0.000014  ...  0.012759 -0.014526        32\n",
       "34  8.406880e-07  0.000617  0.000014  ...  0.012759 -0.014526        34\n",
       "28  8.406921e-07  0.000617  0.000013  ...  0.012759 -0.014526        28\n",
       "30  8.406921e-07  0.000617  0.000013  ...  0.012759 -0.014526        30\n",
       "4   8.406628e-07  0.000617  0.000012  ...  0.012759 -0.014526         4\n",
       "6   8.406628e-07  0.000617  0.000012  ...  0.012759 -0.014526         6\n",
       "29  8.407485e-07  0.000617  0.000012  ...  0.013981 -0.014526        29\n",
       "31  8.407485e-07  0.000617  0.000012  ...  0.013981 -0.014526        31\n",
       "42  8.406613e-07  0.000617  0.000012  ...  0.012759 -0.014526        42\n",
       "40  8.406613e-07  0.000617  0.000012  ...  0.012759 -0.014526        40\n",
       "33  8.407406e-07  0.000617  0.000012  ...  0.013981 -0.014526        33\n",
       "35  8.407406e-07  0.000617  0.000012  ...  0.013981 -0.014526        35\n",
       "25  8.407710e-07  0.000617  0.000011  ...  0.014526 -0.013493        25\n",
       "27  8.407710e-07  0.000617  0.000011  ...  0.014526 -0.013493        27\n",
       "39  8.407692e-07  0.000617  0.000010  ...  0.014526 -0.013493        39\n",
       "1   8.407692e-07  0.000617  0.000010  ...  0.014526 -0.013493         1\n",
       "37  8.407692e-07  0.000617  0.000010  ...  0.014526 -0.013493        37\n",
       "3   8.407692e-07  0.000617  0.000010  ...  0.014526 -0.013493         3\n",
       "15  8.408007e-07  0.000617  0.000007  ...  0.014526 -0.013493        15\n",
       "23  8.408029e-07  0.000617  0.000007  ...  0.014526 -0.013493        23\n",
       "13  8.408007e-07  0.000617  0.000007  ...  0.014526 -0.013493        13\n",
       "21  8.408029e-07  0.000617  0.000007  ...  0.014526 -0.013493        21\n",
       "19  8.408029e-07  0.000617  0.000007  ...  0.014526 -0.013493        19\n",
       "17  8.408029e-07  0.000617  0.000007  ...  0.014526 -0.013493        17\n",
       "\n",
       "[48 rows x 14 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "for i, hp in enumerate(hps):\n",
    "    \n",
    "    # evaluate\n",
    "    model = load(f\"{save_dir}{STOCK}_main_pred_input_lgbm_model_{i}.joblib\")\n",
    "    prediction = model.predict(main_model_valid_x)\n",
    "    summary = evaluate_model(main_model_valid_y.flatten(), prediction, f\"stocks_lgbm_predict_spy_{i}\")\n",
    "    summary['lgbm_num'] = i\n",
    "    summaries.append(summary)\n",
    "\n",
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(MODEL_RESULTS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good `avg_pnl`,  \n",
    "though the up and down are still biased towards the downwards side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fl7Qe9oLEje0"
   },
   "source": [
    "# Comparing the results\n",
    "\n",
    "During the evaluation of the models we saved the results into a shared csv file.  \n",
    "We will load it so we can compare the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(MODEL_RESULTS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that managed to get the best loss (`mse`) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>indicatores_rf_25</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.473976</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.982370</td>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>indicatores_rf_27</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.505114</td>\n",
       "      <td>0.473976</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.982370</td>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>indicatores_rf_20</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.480590</td>\n",
       "      <td>0.513975</td>\n",
       "      <td>0.473808</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>0.982134</td>\n",
       "      <td>0.473929</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>indicatores_rf_18</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.480590</td>\n",
       "      <td>0.513975</td>\n",
       "      <td>0.473808</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>0.982134</td>\n",
       "      <td>0.473929</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>indicatores_rf_24</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.484668</td>\n",
       "      <td>0.507666</td>\n",
       "      <td>0.473410</td>\n",
       "      <td>0.513342</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model       mse  true_pnl   avg_pnl  median_pnl   true_up  \\\n",
       "67  indicatores_rf_25  0.000002  0.000617 -0.000006   -0.000033  0.489378   \n",
       "69  indicatores_rf_27  0.000002  0.000617 -0.000006   -0.000033  0.489378   \n",
       "62  indicatores_rf_20  0.000002  0.000617 -0.000007   -0.000033  0.480590   \n",
       "59  indicatores_rf_18  0.000002  0.000617 -0.000007   -0.000033  0.480590   \n",
       "66  indicatores_rf_24  0.000002  0.000617 -0.000009   -0.000033  0.484668   \n",
       "\n",
       "    false_up  true_down  false_down  up_ratio  down_ratio  accuracy   max_win  \\\n",
       "67  0.505114   0.473976    0.512821  0.017630    0.982370  0.474248  0.014526   \n",
       "69  0.505114   0.473976    0.512821  0.017630    0.982370  0.474248  0.014526   \n",
       "62  0.513975   0.473808    0.512987  0.017866    0.982134  0.473929  0.014526   \n",
       "59  0.513975   0.473808    0.512987  0.017866    0.982134  0.473929  0.014526   \n",
       "66  0.507666   0.473410    0.513342  0.032570    0.967430  0.473776  0.014526   \n",
       "\n",
       "    max_loss  \n",
       "67 -0.013493  \n",
       "69 -0.013493  \n",
       "62 -0.013493  \n",
       "59 -0.013493  \n",
       "66 -0.013493  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=\"mse\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the model that got the best `avg_pnl` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>rf_liniar_21</td>\n",
       "      <td>8.420000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520935</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.505489</td>\n",
       "      <td>0.443994</td>\n",
       "      <td>0.556006</td>\n",
       "      <td>0.498918</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>rf_liniar_22</td>\n",
       "      <td>8.420000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520935</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.505489</td>\n",
       "      <td>0.443994</td>\n",
       "      <td>0.556006</td>\n",
       "      <td>0.498918</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>stocks_lgbm_predict_spy_38</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>stocks_lgbm_predict_spy_24</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.366111</td>\n",
       "      <td>0.633889</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>stocks_lgbm_predict_spy_26</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.366111</td>\n",
       "      <td>0.633889</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model           mse  true_pnl   avg_pnl  median_pnl  \\\n",
       "268                rf_liniar_21  8.420000e-07  0.000617  0.000017         0.0   \n",
       "269                rf_liniar_22  8.420000e-07  0.000617  0.000017         0.0   \n",
       "460  stocks_lgbm_predict_spy_38  8.410000e-07  0.000617  0.000016         0.0   \n",
       "446  stocks_lgbm_predict_spy_24  8.410000e-07  0.000617  0.000016         0.0   \n",
       "448  stocks_lgbm_predict_spy_26  8.410000e-07  0.000617  0.000016         0.0   \n",
       "\n",
       "      true_up  false_up  true_down  false_down  up_ratio  down_ratio  \\\n",
       "268  0.520935  0.466129   0.481336    0.505489  0.443994    0.556006   \n",
       "269  0.520935  0.466129   0.481336    0.505489  0.443994    0.556006   \n",
       "460  0.519968  0.466984   0.478988    0.507932  0.366832    0.633168   \n",
       "446  0.519856  0.467185   0.478858    0.508010  0.366111    0.633889   \n",
       "448  0.519856  0.467185   0.478858    0.508010  0.366111    0.633889   \n",
       "\n",
       "     accuracy   max_win  max_loss  \n",
       "268  0.498918  0.014526 -0.013493  \n",
       "269  0.498918  0.014526 -0.013493  \n",
       "460  0.494021  0.012759 -0.014526  \n",
       "446  0.493868  0.012759 -0.014526  \n",
       "448  0.493868  0.012759 -0.014526  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=\"avg_pnl\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We optimized our model to get the minimum `mse` - but this didnt mean the best return.  \n",
    "For further exploration, we need to find a better loss function, that will be more correlated with the `pnl`. \n",
    "\n",
    "Lets check which model has the best accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>rf_log_returns_13</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>0.459787</td>\n",
       "      <td>0.483340</td>\n",
       "      <td>0.503897</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>rf_log_returns_12</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>0.459787</td>\n",
       "      <td>0.483340</td>\n",
       "      <td>0.503897</td>\n",
       "      <td>0.371729</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>rf_no_OHL_13</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524944</td>\n",
       "      <td>0.461295</td>\n",
       "      <td>0.483087</td>\n",
       "      <td>0.504288</td>\n",
       "      <td>0.390153</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.499417</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>rf_no_OHL_12</td>\n",
       "      <td>8.410000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524944</td>\n",
       "      <td>0.461295</td>\n",
       "      <td>0.483087</td>\n",
       "      <td>0.504288</td>\n",
       "      <td>0.390153</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.499417</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>-0.013981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>rf_exponential_11</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517201</td>\n",
       "      <td>0.469285</td>\n",
       "      <td>0.480110</td>\n",
       "      <td>0.507267</td>\n",
       "      <td>0.519176</td>\n",
       "      <td>0.480824</td>\n",
       "      <td>0.499367</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>-0.014526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model           mse  true_pnl   avg_pnl  median_pnl  \\\n",
       "283  rf_log_returns_13  8.410000e-07  0.000617  0.000015         0.0   \n",
       "282  rf_log_returns_12  8.410000e-07  0.000617  0.000015         0.0   \n",
       "307       rf_no_OHL_13  8.410000e-07  0.000617  0.000015         0.0   \n",
       "306       rf_no_OHL_12  8.410000e-07  0.000617  0.000015         0.0   \n",
       "233  rf_exponential_11  8.600000e-07  0.000617  0.000007         0.0   \n",
       "\n",
       "      true_up  false_up  true_down  false_down  up_ratio  down_ratio  \\\n",
       "283  0.526628  0.459787   0.483340    0.503897  0.371729    0.628271   \n",
       "282  0.526628  0.459787   0.483340    0.503897  0.371729    0.628271   \n",
       "307  0.524944  0.461295   0.483087    0.504288  0.390153    0.609847   \n",
       "306  0.524944  0.461295   0.483087    0.504288  0.390153    0.609847   \n",
       "233  0.517201  0.469285   0.480110    0.507267  0.519176    0.480824   \n",
       "\n",
       "     accuracy   max_win  max_loss  \n",
       "283  0.499431  0.014526 -0.013981  \n",
       "282  0.499431  0.014526 -0.013981  \n",
       "307  0.499417  0.014526 -0.013981  \n",
       "306  0.499417  0.014526 -0.013981  \n",
       "233  0.499367  0.013493 -0.014526  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=\"accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy is also an important matric, since a good `avg_pnl` could be caused (potentially) by little number of good trades,  \n",
    "but we want our overall strategy to be accurate and hit the right side if the trade most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how our model did on the test set\n",
    "\n",
    "Now we will take the top performing models, and test them against the test set.  \n",
    "These are the models we will choose: \n",
    "- rf_liniar_21 (top winnig models in terms of `pnl`)\n",
    "- rf_log_returns_13 (top winning model in terms of `accuracy`)\n",
    "- stocks_lgbm_predict_spy_38 (second best in terms of `pnl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rf_liniar_21 model VS test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = read_data(data_dir, STOCK)\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# add target\n",
    "df['target'] = df['close'].shift(-10) / df['close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "df = df[mask]\n",
    "\n",
    "# drop Nans + test data\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 30s, sys: 7.49 s, total: 5min 37s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "window_size = 10\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "num_test_windows = len(df[df.index >= SPLIT_DATE2])\n",
    "test_x = windows_x[num_test_windows:]\n",
    "test_y = windows_y[num_test_windows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "    \n",
    "rf = load(f\"{save_dir}{STOCK}_linear_window_rf_21.joblib\")\n",
    "\n",
    "prediction = rf.predict(test_x)\n",
    "summary = evaluate_model(test_y.flatten(), prediction, f\"rf_liniar_21\")\n",
    "summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rf_log_returns_13 model VS test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "adj_df = df.drop(columns=['open', 'high', 'low'])\n",
    "\n",
    "# apply log\n",
    "adj_df['target'] = np.log(adj_df['target'] + 1)\n",
    "\n",
    "# windows generation\n",
    "windows_x = df_windows(adj_df.drop(columns=['target']), window_size)\n",
    "windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "windows_x = np.array([win.flatten() for win in windows_x])\n",
    "windows_y = df_windows(adj_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "\n",
    "num_test_windows = len(adj_df[adj_df.index >= SPLIT_DATE2])\n",
    "test_x = windows_x[num_test_windows:]\n",
    "test_y = windows_y[num_test_windows:]\n",
    "\n",
    "# apply exponent to the validation target - bring it back to original targets\n",
    "test_y = np.exp(test_y) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = load(f\"{save_dir}{STOCK}_linear_log_window_rf_13.joblib\")\n",
    "\n",
    "# evaluate rfs\n",
    "prediction = rf.predict(test_x)\n",
    "\n",
    "# apply exponent to the prediction - bring it back to original targets\n",
    "prediction = np.exp(prediction) - 1\n",
    "    \n",
    "summary = evaluate_model(test_y.flatten(), prediction, f\"rf_log_returns_13\")\n",
    "summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stocks_lgbm_predict_spy_38 model VS test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for stock in [STOCK] + stocks:\n",
    "    df = read_data(data_dir, stock)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df[['close', 'volume']]\n",
    "    df.columns = [stock+\"_\"+col for col in df.columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with Nan in the SPY data\n",
    "merged_df.dropna(subset=[STOCK+'_close', STOCK+'_volume'], inplace=True)\n",
    "\n",
    "# add flag column for each stock, to signal for Nan value\n",
    "for stock in stocks:\n",
    "    add_col = merged_df[[stock+'_close']].isnull().astype(int)\n",
    "    merged_df[stock+\"_isnull\"] = add_col\n",
    "\n",
    "# fill existing Nans with last available value\n",
    "merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# add target\n",
    "merged_df['target'] = merged_df[STOCK+'_close'].shift(-10) / merged_df[STOCK+'_close'] - 1\n",
    "\n",
    "# drop last 10 minutes of every day\n",
    "datetime = pd.Series(merged_df.index)\n",
    "mask = list(datetime.dt.day == datetime.shift(-10).dt.day)\n",
    "merged_df = merged_df[mask]\n",
    "\n",
    "# drop any remaining Nans\n",
    "merged_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "window_size = 10\n",
    "\n",
    "windows_y = df_windows(merged_df['target'].T, window_size)[:,-1]\n",
    "windows_y = np.array(np.split(windows_y, windows_y.shape[0]))\n",
    "\n",
    "test_preds = {}\n",
    "\n",
    "for stock in [STOCK] + stocks:\n",
    "\n",
    "    print('-'*10 + stock + '-'*10)\n",
    "    \n",
    "    # windows generation\n",
    "    cols = [col for col in merged_df.columns if stock == col.split(\"_\")[0]]\n",
    "    windows_x = df_windows(merged_df[cols], window_size)\n",
    "    windows_x = np.apply_along_axis(minmax_scale, 1, windows_x)\n",
    "    windows_x = np.array([win.flatten() for win in windows_x])\n",
    "    \n",
    "    num_test_windows = len(merged_df[merged_df.index >= SPLIT_DATE2])\n",
    "    test_x = windows_x[num_test_windows:]\n",
    "    test_y = windows_y[num_test_windows:]\n",
    "\n",
    "    # create data\n",
    "    lgb_test = lgb.Dataset(test_x, test_y.flatten())\n",
    "\n",
    "    # save best model\n",
    "    best_model = load(f\"{save_dir}lgbm_prediction_as_input_{stock}.joblib\")\n",
    "\n",
    "    # predict validation, save result for later\n",
    "    test_preds[stock] = best_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model_test_x = np.stack([preds for stock, preds in test_preds.items()], axis=1)\n",
    "main_model_test_y = test_y\n",
    "\n",
    "lgb_test = lgb.Dataset(main_model_test_x, main_model_test_y.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# evaluate final model\n",
    "model = load(f\"{save_dir}{STOCK}_main_pred_input_lgbm_model_38.joblib\")\n",
    "prediction = model.predict(main_model_test_x)\n",
    "summary = evaluate_model(main_model_test_y.flatten(), prediction, f\"stocks_lgbm_predict_spy_38\")\n",
    "summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>true_pnl</th>\n",
       "      <th>avg_pnl</th>\n",
       "      <th>median_pnl</th>\n",
       "      <th>true_up</th>\n",
       "      <th>false_up</th>\n",
       "      <th>true_down</th>\n",
       "      <th>false_down</th>\n",
       "      <th>up_ratio</th>\n",
       "      <th>down_ratio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>max_win</th>\n",
       "      <th>max_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_liniar_21</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.619872</td>\n",
       "      <td>0.367221</td>\n",
       "      <td>0.567103</td>\n",
       "      <td>0.419416</td>\n",
       "      <td>0.451192</td>\n",
       "      <td>0.548808</td>\n",
       "      <td>0.590912</td>\n",
       "      <td>0.038988</td>\n",
       "      <td>-0.035628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stocks_lgbm_predict_spy_38</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.580763</td>\n",
       "      <td>0.405847</td>\n",
       "      <td>0.518202</td>\n",
       "      <td>0.468674</td>\n",
       "      <td>0.367497</td>\n",
       "      <td>0.632503</td>\n",
       "      <td>0.541193</td>\n",
       "      <td>0.035560</td>\n",
       "      <td>-0.038988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_log_returns_13</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>0.495619</td>\n",
       "      <td>0.490952</td>\n",
       "      <td>0.377690</td>\n",
       "      <td>0.622310</td>\n",
       "      <td>0.512765</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>-0.038988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model       mse  true_pnl   avg_pnl  median_pnl  \\\n",
       "0                rf_liniar_21  0.000002  0.000855  0.000273    0.000149   \n",
       "2  stocks_lgbm_predict_spy_38  0.000002  0.000855  0.000168    0.000073   \n",
       "1           rf_log_returns_13  0.000002  0.000855  0.000076    0.000034   \n",
       "\n",
       "    true_up  false_up  true_down  false_down  up_ratio  down_ratio  accuracy  \\\n",
       "0  0.619872  0.367221   0.567103    0.419416  0.451192    0.548808  0.590912   \n",
       "2  0.580763  0.405847   0.518202    0.468674  0.367497    0.632503  0.541193   \n",
       "1  0.541016  0.446105   0.495619    0.490952  0.377690    0.622310  0.512765   \n",
       "\n",
       "    max_win  max_loss  \n",
       "0  0.038988 -0.035628  \n",
       "2  0.035560 -0.038988  \n",
       "1  0.028812 -0.038988  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(summaries)\n",
    "summary.sort_values(by=['avg_pnl'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing results in the test set! \n",
    "We didnt expect such relatively good results on the test set,  \n",
    "since we didnt get such good results on the validation set.\n",
    "But turns out that in the test set, our models did pretty well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHpR77qTQgFH"
   },
   "source": [
    "# Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we have learnt\n",
    "\n",
    "- This is a very hard (and big) problem.  \n",
    "    We had a lot of ideas, and every day led to new discoveries.  \n",
    "    We had to focus and only work on certain aspects of the problem,  \n",
    "    and give up the parts we didn't have time to explore.\n",
    "- We are disappointed with neural networks. \n",
    "    We expected neural networks to be our best performing model,  \n",
    "    but turns out they performed way worse than tree-based models.  \n",
    "    They take a lot of time to converge, and require a lot of CPU time.  \n",
    "    It is possible that with more computing power and more time,  \n",
    "    they could have done better - however, those were not within our hands.\n",
    "- Trees are a very strong model.  \n",
    "    They run relatively fast, and its very easy to tweak their hyper-parameters. \n",
    "- Our computing power is limited. \n",
    "    As we said, a big limit for us was memory and computing power.  \n",
    "    We had a big dataset of over 250 stocks, but we could use only a fraction of it,  \n",
    "    since our computers just crashed.  \n",
    "    Also our computing power is limited - and we had to wait a lot of time to run models, such as neural networks.  \n",
    "    This led to a limit in our ability to tweak their parameters, and test different architectures. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we could have done better:\n",
    "\n",
    "- Try different loss functions:  \n",
    "    We have seen that low loss != high pnl.  \n",
    "    Another thing to try is to pick an evaluation function that will better represent \n",
    "    the matrices that we really cared about - `avg_pnl` and `accuracy`.  \n",
    "- Try classification models:\n",
    "    In retrospect, our main metric, `avg_pnl`, is affected by the direction of the prediction only,  \n",
    "    and not the magnitude of the prediction.  \n",
    "    Therefore, maybe using classification models, instead of regression ones, would have yield better results over this metric.\n",
    "- cross validation:\n",
    "As we said earlier in section \"split the data\", using cross validation would make our models assessment more accurate, for the price of longer training times (and general complexity).\n",
    "No surprise we got relatively big differences between the results of the validation set and the test set. \n",
    "Maybe if we used cross validation, there wouldn't be such big differences between the validation set and the test set. \n",
    "Luckily, this time, the test results were in our favor.\n",
    "- check other targets:\n",
    "    We only tried to predict the return, 10 minutes into the future.  \n",
    "    There are plenty of other targets we could had choose -  \n",
    "    Different time intervals, another metric such as `volume` or `volatility`,  \n",
    "    or maybe the price itself (even though the return is interchangeable with it).  \n",
    "    Different targets could have led us to different, and maybe better, overall results. \n",
    "- Try a different approach (Q Learning):\n",
    "    Even if our models did amazing in predicting the prices -  \n",
    "    There are still other decisions we need to make.  \n",
    "    What trades to take and what not to take?  \n",
    "    Is the trade still profitable under the trading fees? \n",
    "    How much money should we trade in a single trade? And more. \n",
    "    We could have used Q learning, to model the market as a whole -  \n",
    "    Including fees, taxes, and even other traders.  \n",
    "    Q learning models should know how to take all of these  \n",
    "    Into account, and come up with a good policy of trading stocks.  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CkHeQIxq4O-o"
   ],
   "name": "end_project_v9 - add indicators hyper parameters.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
